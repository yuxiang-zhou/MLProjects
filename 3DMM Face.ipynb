{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/phoebe/yz4009/src/miniconda3/envs/gitdev/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/vol/phoebe/yz4009/src/miniconda3/envs/gitdev/lib/python3.5/site-packages/scipy/sparse/compressed.py:774: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import deepmachine.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection/detect_face.py:182: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection/detect_face.py:184: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "import menpo.io as mio\n",
    "import numpy as np\n",
    "import menpo.io as mio\n",
    "import menpo3d.io as m3io\n",
    "import menpo3d\n",
    "import scipy.io as sio\n",
    "import sys, traceback\n",
    "\n",
    "from menpo.feature import no_op, fast_dsift\n",
    "from menpo.landmark import face_ibug_68_to_face_ibug_49, face_ibug_68_to_face_ibug_68\n",
    "from menpo3d.camera import PerspectiveCamera\n",
    "from menpo.transform import AlignmentAffine\n",
    "# from menpo3d.result import _affine_2d_to_3d\n",
    "\n",
    "from menpo3d.rasterize import rasterize_mesh\n",
    "from menpo.transform import rotate_ccw_about_centre\n",
    "from menpo3d.morphablemodel import ColouredMorphableModel\n",
    "from menpo.model import PCAModel\n",
    "from menpo.shape import PointDirectedGraph, PointCloud, ColouredTriMesh, TriMesh\n",
    "from pathlib import Path\n",
    "from menpo.visualize import print_progress\n",
    "from menpo3d.unwrap import optimal_cylindrical_unwrap\n",
    "from menpo.transform import Translation\n",
    "from menpo3d.rasterize import rasterize_barycentric_coordinate_images\n",
    "from menpo.transform import image_coords_to_tcoords\n",
    "from menpo.shape import TexturedTriMesh\n",
    "from menpo.image import Image\n",
    "\n",
    "from menpofit.aam import load_balanced_frontal_face_fitter\n",
    "from menpo3d.morphablemodel.fitter import LucasKanadeMMFitter\n",
    "from menpo3d.camera import PerspectiveCamera\n",
    "\n",
    "from menpowidgets import visualize_appearance_model, visualize_shape_model_2d\n",
    "\n",
    "sys.path.append('/homes/yz4009/wd/gitdev/face2d3d/')\n",
    "from itw3dmm.data import load_tassos_lsfm_combined_model\n",
    "from itw3dmm import mappings\n",
    "from shading import lambertian_shading\n",
    "\n",
    "\n",
    "sys.path.append('/homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection')\n",
    "import detect_face\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    device_count = {'GPU': 0}\n",
    ")\n",
    "\n",
    "with tf.Graph().as_default() as detect_g:\n",
    "\n",
    "    with tf.Session(graph=detect_g, config=config).as_default() as sess:\n",
    "        with tf.variable_scope('pnet'):\n",
    "            data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "            pnet = detect_face.PNet({'data':data})\n",
    "            pnet.load('/homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection/cas1.npy', sess)\n",
    "        with tf.variable_scope('rnet'):\n",
    "            data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "            rnet = detect_face.RNet({'data':data})\n",
    "            rnet.load('/homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection/cas2.npy', sess)\n",
    "        with tf.variable_scope('onet'):\n",
    "            data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "            onet = detect_face.ONet({'data':data})\n",
    "            onet.load('/homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/detection/cas3.npy', sess)\n",
    "\n",
    "        pnet_fun = lambda img : sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0':img})\n",
    "        rnet_fun = lambda img : sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0':img})\n",
    "        onet_fun = lambda img : sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0':img})\n",
    "\n",
    "minsize = 120 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "def detect(img):\n",
    "\n",
    "    bounding_boxes, points = detect_face.detect_face(img, minsize, pnet_fun, rnet_fun, onet_fun, threshold, factor)\n",
    "    bounding_boxes = [PointCloud(b[:4].reshape(-1,2)[:,::-1]).bounding_box() for b in bounding_boxes]\n",
    "\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/phoebe/yz4009/src/miniconda3/envs/gitdev/lib/python3.5/site-packages/menpo/shape/pointcloud.py:261: MenpoDeprecationWarning: The .lms property is deprecated. LandmarkGroups are now shapes themselves - so you can use them directly anywhere you previously used .lms.Simply remove \".lms\" from your code and things will work as expected (and this warning will go away)\n",
      "  MenpoDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# shape model\n",
    "__model_path = Path('/vol/atlas/homes/aroussos/results/fit3Dto2D/model/ver2016-12-12_LSFMfrmt_maxNpcInf/all_all_all.mat')\n",
    "\n",
    "shape_model_dict = load_tassos_lsfm_combined_model(__model_path)\n",
    "shape_model = shape_model_dict['shape_model']\n",
    "\n",
    "# template landmarks\n",
    "landmarks = m3io.import_landmark_file('/homes/yz4009/wd/gitdev/face2d3d/ibug68.ljson').lms.from_vector(shape_model.mean().lms.points[mappings.fw_index_for_lms()])\n",
    "\n",
    "# texture_model\n",
    "texture_model = mappings.load_itwmm_texture_fast_dsift_fw()\n",
    "\n",
    "# morphable model\n",
    "diagonal = 185\n",
    "mm = ColouredMorphableModel(shape_model, texture_model, (landmarks), \n",
    "                            holistic_features=fast_dsift, diagonal=diagonal)\n",
    "# fitter\n",
    "fitter = LucasKanadeMMFitter(mm, n_shape=200, n_texture=200,\n",
    "                             n_samples=8000, n_scales=1, \n",
    "                             camera_cls=PerspectiveCamera)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/homes/yz4009/wd/gitdev/DeepLearning/Applications/FaceHGNet/')\n",
    "\n",
    "import networks\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "with tf.Graph().as_default() as lms_graph:\n",
    "    images_input = tf.placeholder(tf.float32, shape=(None, None, None, 3), name='input_images')\n",
    "    net_model = networks.DNFaceMultiView('')\n",
    "    with tf.variable_scope('net'):\n",
    "        with slim.arg_scope([slim.batch_norm, slim.layers.dropout], is_training=False):\n",
    "            lms_heatmap_prediction,states = net_model._build_network(images_input)\n",
    "\n",
    "def get_landmarks(pimg, bb=None):\n",
    "#     \n",
    "    if bb is None:\n",
    "        bboxes = detect(pimg.pixels_with_channels_at_back()*255)\n",
    "        if len(bboxes) > 0:\n",
    "            index = np.argmin([np.linalg.norm(b.centre_of_bounds() - pimg.centre()) for b in bboxes])\n",
    "            bb = bboxes[index]\n",
    "        else:\n",
    "            bb = PointCloud(image.bounds()).bounding_box()\n",
    "\n",
    "    centre = bb.centre_of_bounds()\n",
    "    \n",
    "    bmin,bmax = bb.bounds()\n",
    "    diagnal = np.linalg.norm(bmax - bmin)\n",
    "    scale = diagnal/250\n",
    "    # crop 384\n",
    "    cimg, trans, c_scale = utils.crop_image(pimg, centre, scale, [384,384])\n",
    "\n",
    "    rimg = cimg\n",
    "    # crop to 256 * 256\n",
    "    offset = (rimg.shape[0] - 256) // 2\n",
    "\n",
    "    image = rimg.crop((offset,offset),(offset+256,offset+256))\n",
    "\n",
    "    input_pixels =image.pixels_with_channels_at_back()\n",
    "\n",
    "    model_path = '/vol/atlas/homes/jiankang_share/Grigorios/MultiviewHG/ckpt/menpo_3D_68/model.ckpt-223664'\n",
    "    with tf.Session(graph=lms_graph) as sess:\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        saver.restore(sess, model_path)\n",
    "        lms_hm_prediction = sess.run(lms_heatmap_prediction, feed_dict={images_input:input_pixels[None,...]})\n",
    "\n",
    "    lms_hm_prediction_filter = np.stack(list(map(\n",
    "        lambda x: scipy.ndimage.filters.gaussian_filter(*x),\n",
    "        zip(lms_hm_prediction[0].transpose(2,0,1), [3] * 68))))\n",
    "\n",
    "    hs = np.argmax(np.max(lms_hm_prediction_filter, 2), 1)\n",
    "    ws = np.argmax(np.max(lms_hm_prediction_filter, 1), 1)\n",
    "    pts_predictions = np.stack([hs,ws]).T\n",
    "\n",
    "    orig_pts = trans.apply(PointCloud((pts_predictions + np.array([offset,offset]))))\n",
    "    \n",
    "    return orig_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(image, export=False):\n",
    "\n",
    "    if len(image.pixels.shape) == 2:\n",
    "        image.pixels = np.stack([image.pixels,image.pixels,image.pixels])\n",
    "\n",
    "    if image.pixels.shape[0] == 1:\n",
    "        image.pixels = np.concatenate([image.pixels,image.pixels,image.pixels], axis=0)\n",
    "\n",
    "    \n",
    "    initial_shape = get_landmarks(image)\n",
    "    result = fitter.fit_from_shape(image, initial_shape, max_iters=20,\n",
    "                                   camera_update=True,\n",
    "                                   focal_length_update=False,\n",
    "                                   reconstruction_weight=1,\n",
    "                                   shape_prior_weight=.4e8,\n",
    "                                   texture_prior_weight=1.,\n",
    "                                   landmarks_prior_weight=1e5,\n",
    "                                   return_costs=True, init_shape_params_from_lms=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mesh = ColouredTriMesh(result.final_mesh.points, result.final_mesh.trilist)\n",
    "\n",
    "\n",
    "    def transform(mesh):\n",
    "        return result._affine_transforms[-1].apply(result.camera_transforms[-1].apply(mesh))\n",
    "\n",
    "\n",
    "    mesh_in_img = transform(lambertian_shading(mesh))\n",
    "    expr_dir = image.path.parent\n",
    "    p = image.path.stem\n",
    "    raster = rasterize_mesh(mesh_in_img, image.shape)\n",
    "\n",
    "    uv_shape = (600, 1000)\n",
    "    template = shape_model.mean()\n",
    "    unwrapped_template = optimal_cylindrical_unwrap(template).apply(template)\n",
    "\n",
    "    minimum = unwrapped_template.bounds(boundary=0)[0]\n",
    "    unwrapped_template = Translation(-minimum).apply(unwrapped_template)\n",
    "    unwrapped_template.points = unwrapped_template.points[:, [1, 0]]\n",
    "    unwrapped_template.points[:, 0] = unwrapped_template.points[:, 0].max() - unwrapped_template.points[:, 0]\n",
    "    unwrapped_template.points *= np.array([.40, .31])\n",
    "    unwrapped_template.points *= np.array([uv_shape])\n",
    "\n",
    "    bcoords_img, tri_index_img = rasterize_barycentric_coordinate_images(unwrapped_template, uv_shape)\n",
    "    TI = tri_index_img.as_vector()\n",
    "    BC = bcoords_img.as_vector(keep_channels=True).T\n",
    "\n",
    "    def masked_texture(mesh_in_image, background):\n",
    "\n",
    "        sample_points_3d = mesh_in_image.project_barycentric_coordinates(BC, TI)\n",
    "\n",
    "        texture = bcoords_img.from_vector(background.sample(sample_points_3d.points[:, :2]))\n",
    "\n",
    "        return texture\n",
    "\n",
    "\n",
    "    uv = masked_texture(mesh_in_img, image)\n",
    "\n",
    "    t = TexturedTriMesh(result.final_mesh.points, image_coords_to_tcoords(uv.shape).apply(unwrapped_template).points , uv, mesh_in_img.trilist)\n",
    "\n",
    "    if export:\n",
    "        m3io.export_textured_mesh(t, str(expr_dir / Path(p).with_suffix('.mesh.obj')), overwrite=True)\n",
    "        mio.export_image(raster, str(expr_dir / Path(p).with_suffix('.render.jpg')), overwrite=True)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<menpo.visualize.viewmatplotlib.MatplotlibImageViewer2d at 0x7f9941588dd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = mio.import_image('/homes/yz4009/Downloads/2793330.jpg')\n",
    "\n",
    "image.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /vol/atlas/homes/jiankang_share/Grigorios/MultiviewHG/ckpt/menpo_3D_68/model.ckpt-223664\n",
      "20/20                                                                           "
     ]
    }
   ],
   "source": [
    "fitted_image = fit(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_image.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gitdev)",
   "language": "python",
   "name": "gitdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
