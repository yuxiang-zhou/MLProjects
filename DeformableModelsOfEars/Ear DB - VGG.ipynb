{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 #include \"theano_mod_helper.h\"\n",
      "6 \n",
      "7 #include <numpy/arrayobject.h>\n",
      "8 #include <iostream>\n",
      "9 \n",
      "10 #include \"cuda_ndarray.cuh\"\n",
      "11 \n",
      "12 #ifndef CNMEM_DLLEXPORT\n",
      "13 #define CNMEM_DLLEXPORT\n",
      "14 #endif\n",
      "15 \n",
      "16 #include \"cnmem.h\"\n",
      "17 #include \"cnmem.cpp\"\n",
      "18 \n",
      "19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "20 #define COMPUTE_GPU_MEM_USED 0\n",
      "21 \n",
      "22 //If true, we fill with NAN allocated device memory.\n",
      "23 #define ALLOC_MEMSET 0\n",
      "24 \n",
      "25 //If true, we print out when we free a device pointer, uninitialize a\n",
      "26 //CudaNdarray, or allocate a device pointer\n",
      "27 #define PRINT_FREE_MALLOC 0\n",
      "28 \n",
      "29 //If true, we do error checking at the start of functions, to make sure there\n",
      "30 //is not a pre-existing error when the function is called.\n",
      "31 //You probably need to set the environment variable\n",
      "32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "33 //preprocessor macro in cuda_ndarray.cuh\n",
      "34 //if you want this to work.\n",
      "35 #define PRECHECK_ERROR 0\n",
      "36 \n",
      "37 cublasHandle_t handle = NULL;\n",
      "38 int* err_var = NULL;\n",
      "39 \n",
      "40 /////////////////////////\n",
      "41 // Alloc and Free\n",
      "42 /////////////////////////\n",
      "43 \n",
      "44 static int g_gpu_context_active = 0;\n",
      "45 \n",
      "46 \n",
      "47 PyObject *\n",
      "48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "50 \n",
      "51 \n",
      "52 /**\n",
      "53  *\n",
      "54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "56  *\n",
      "57  */\n",
      "58 int _outstanding_mallocs[] = {0,0};\n",
      "59 \n",
      "60 #if COMPUTE_GPU_MEM_USED\n",
      "61 size_t _allocated_size = 0;\n",
      "62 size_t _max_allocated_size = 0;\n",
      "63 \n",
      "64 const int TABLE_SIZE = 10000;\n",
      "65 struct table_struct{\n",
      "66     void* ptr;\n",
      "67     size_t size;\n",
      "68 };\n",
      "69 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "70 #endif\n",
      "71 \n",
      "72 void * device_malloc(size_t size)\n",
      "73 {\n",
      "74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "75 }\n",
      "76 \n",
      "77 ///@TODO: thejaswi: link this option to a theano config variable?\n",
      "78 static bool g_use_cnmem = false;\n",
      "79 static const int g_max_devices = 8;\n",
      "80 int initCnmem(int card_number_provided, int card_nb, size_t mem) {\n",
      "81     static bool cnmemInitialized = false;\n",
      "82     if(cnmemInitialized) {\n",
      "83         return 0;\n",
      "84     }\n",
      "85     // On stderr to be at the same place as \"Using gpu device...\"\n",
      "86     int numDevices = 0;\n",
      "87     cnmemDevice_t devices[g_max_devices];\n",
      "88     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {\n",
      "89         PyErr_Format(PyExc_RuntimeError,\n",
      "90                      \"initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\\n\",\n",
      "91                      cudaGetErrorString(cudaGetLastError()));\n",
      "92         return -1;\n",
      "93     }\n",
      "94     if(card_number_provided){\n",
      "95         numDevices = 1;\n",
      "96         int i = 0;\n",
      "97         devices[i].device = card_nb;\n",
      "98         devices[i].size = mem;\n",
      "99         ///@TODO: thejaswi: add support for multiple streams\n",
      "100         devices[i].numStreams = 0;\n",
      "101         devices[i].streams = NULL;\n",
      "102         devices[i].streamSizes = NULL;\n",
      "103     }else{\n",
      "104         for(int i=0;i<numDevices;++i) {\n",
      "105             devices[i].device = i;\n",
      "106             devices[i].size = mem;\n",
      "107             ///@TODO: thejaswi: add support for multiple streams\n",
      "108             devices[i].numStreams = 0;\n",
      "109             devices[i].streams = NULL;\n",
      "110         }\n",
      "111     }\n",
      "112 \n",
      "113     ///@TODO: thejaswi: passing custom cnmem flags?\n",
      "114     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);\n",
      "115     if(status != CNMEM_STATUS_SUCCESS) {\n",
      "116         PyErr_Format(PyExc_RuntimeError,\n",
      "117                      \"initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\\n\",\n",
      "118                      cnmemGetErrorString(status), numDevices);\n",
      "119         return -1;\n",
      "120     }\n",
      "121     cnmemInitialized = true;\n",
      "122     return 0;\n",
      "123 }\n",
      "124 \n",
      "125 void * device_malloc(size_t size, int verbose)\n",
      "126 {\n",
      "127     #if PRECHECK_ERROR\n",
      "128         cudaThreadSynchronize();\n",
      "129         cudaError_t prevError = cudaGetLastError();\n",
      "130         if (cudaSuccess != prevError)\n",
      "131         {\n",
      "132             fprintf(stderr,\n",
      "133                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "134                     cudaGetErrorString(prevError)\n",
      "135                     );\n",
      "136         }\n",
      "137     #endif\n",
      "138     void * rval=NULL;\n",
      "139     ///@TODO: thejaswi: support for multiple-streams?\n",
      "140     if(g_use_cnmem) {\n",
      "141         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;\n",
      "142         status = cnmemMalloc(&rval, size, NULL);\n",
      "143         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "144             PyErr_Format(PyExc_MemoryError,\n",
      "145                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "146                          (unsigned long long)size, cnmemGetErrorString(status));\n",
      "147             return NULL;\n",
      "148         }\n",
      "149     }\n",
      "150     else {\n",
      "151         cudaError_t err = cudaMalloc(&rval, size);\n",
      "152         if (cudaSuccess != err)\n",
      "153         {\n",
      "154             // Clear the error flag, cudaMalloc doesn't do it.\n",
      "155             // Currently this returns the same thing as err, but if in future\n",
      "156             // it returns something else I still don't see why we should ignore\n",
      "157             // it.  All we want to do here is reset the flag.\n",
      "158             cudaGetLastError();\n",
      "159             if (verbose)\n",
      "160             {\n",
      "161                 size_t free = 0, total = 0;\n",
      "162                 cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "163                 if (err2 != cudaSuccess){\n",
      "164                     cudaGetLastError();\n",
      "165                     fprintf(stderr,\n",
      "166                             \"Error when trying to find the memory information\"\n",
      "167                             \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "168                 }\n",
      "169                 #if COMPUTE_GPU_MEM_USED\n",
      "170                     fprintf(stderr,\n",
      "171                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "172                             \" new total bytes allocated: %llu.\"\n",
      "173                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "174                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)_allocated_size,\n",
      "175                             (unsigned long long)free, (unsigned long long)total);\n",
      "176                 #else\n",
      "177                     fprintf(stderr,\n",
      "178                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "179                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "180                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "181                 #endif\n",
      "182             }\n",
      "183             PyErr_Format(PyExc_MemoryError,\n",
      "184                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "185                          (unsigned long long)size, cudaGetErrorString(err));\n",
      "186             return NULL;\n",
      "187         }\n",
      "188     }\n",
      "189     if (rval != NULL){\n",
      "190         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "191         // Could this be what happen if size is 0?\n",
      "192         _outstanding_mallocs[0] += 1;\n",
      "193 \n",
      "194 #if COMPUTE_GPU_MEM_USED\n",
      "195         _allocated_size += size;\n",
      "196         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "197         int i = 0;\n",
      "198         for(;i<TABLE_SIZE;i++){\n",
      "199             if(NULL==_alloc_size_table[i].ptr){\n",
      "200                 _alloc_size_table[i].ptr=rval;\n",
      "201                 _alloc_size_table[i].size=size;\n",
      "202                 break;\n",
      "203             }\n",
      "204         }\n",
      "205         if (i == TABLE_SIZE){\n",
      "206             fprintf(stderr,\n",
      "207                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "208                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "209         }\n",
      "210 #endif\n",
      "211     }\n",
      "212     //fprintf(stderr,\n",
      "213     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "214     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "215 \n",
      "216     if(ALLOC_MEMSET){\n",
      "217         //We init them to nan to make sure we catch more debug case.\n",
      "218         cudaMemset(rval, 0xFF, size);\n",
      "219         //printf(\"MEMSET\\n\");\n",
      "220     }\n",
      "221     #if PRINT_FREE_MALLOC\n",
      "222         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "223     #endif\n",
      "224     return rval;\n",
      "225 }\n",
      "226 \n",
      "227 int device_free(void *ptr)\n",
      "228 {\n",
      "229     #if PRECHECK_ERROR\n",
      "230         cudaThreadSynchronize();\n",
      "231         cudaError_t prevError = cudaGetLastError();\n",
      "232         if (cudaSuccess != prevError)\n",
      "233         {\n",
      "234             fprintf(stderr,\n",
      "235                     \"Error existed before calling device_free. %s\\n\",\n",
      "236                     cudaGetErrorString(prevError)\n",
      "237                     );\n",
      "238         }\n",
      "239     #endif\n",
      "240     #if PRINT_FREE_MALLOC\n",
      "241         size_t free = 0, total = 0;\n",
      "242         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "243         if (err2 != cudaSuccess){\n",
      "244             cudaGetLastError();\n",
      "245             fprintf(stderr,\n",
      "246                     \"Error when tring to find the memory information\"\n",
      "247                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "248         }\n",
      "249         #if COMPUTE_GPU_MEM_USED\n",
      "250         {\n",
      "251             int i = 0;\n",
      "252             for(;i<TABLE_SIZE;i++)\n",
      "253                 if(_alloc_size_table[i].ptr==ptr){\n",
      "254                     break;\n",
      "255                 }\n",
      "256             assert(i<TABLE_SIZE);\n",
      "257             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "258                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "259                     ptr, _alloc_size_table[i].size, free, total);\n",
      "260         }\n",
      "261         #else\n",
      "262             fprintf(stderr, \"device_free %p.\"\n",
      "263                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "264                     ptr, free, total);\n",
      "265         #endif\n",
      "266     #endif\n",
      "267 \n",
      "268     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "269     if(!g_gpu_context_active) {\n",
      "270         return 0;\n",
      "271     }\n",
      "272 \n",
      "273     ///@TODO: thejaswi: multi-stream support\n",
      "274     if(g_use_cnmem) {\n",
      "275         cnmemStatus_t status = cnmemFree(ptr, NULL);\n",
      "276         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "277             fprintf(stderr, \"device_free: cnmemFree call failed! Reason=%s\\n\",\n",
      "278                     cnmemGetErrorString(status));\n",
      "279         }\n",
      "280     }\n",
      "281     else {\n",
      "282         // We need sync as the Theano's GC could remove intermediate variable that\n",
      "283         // are still needed as the gpu kernel are running or in the queue.\n",
      "284         CNDA_BEGIN_ALLOW_THREADS\n",
      "285         cudaThreadSynchronize();\n",
      "286         CNDA_END_ALLOW_THREADS\n",
      "287 \n",
      "288         cudaError_t err =  cudaFree(ptr);\n",
      "289         if (cudaSuccess != err)\n",
      "290         {\n",
      "291             // Clear the error flag, cudaFree doesn't do it.\n",
      "292             // Currently this returns the same thing as err, but if in future\n",
      "293             // it returns something else I still don't see why we should ignore\n",
      "294             // it.  All we want to do here is reset the flag.\n",
      "295             cudaGetLastError();\n",
      "296             size_t free = 0, total = 0;\n",
      "297             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "298             if (err2 != cudaSuccess){\n",
      "299                 cudaGetLastError();\n",
      "300                 fprintf(stderr,\n",
      "301                         \"Error when tring to find the memory information\"\n",
      "302                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "303             }\n",
      "304             #if COMPUTE_GPU_MEM_USED\n",
      "305             {\n",
      "306                 int i = 0;\n",
      "307                 for(;i<TABLE_SIZE;i++)\n",
      "308                     if(_alloc_size_table[i].ptr==ptr){\n",
      "309                         break;\n",
      "310                     }\n",
      "311                 assert(i<TABLE_SIZE);\n",
      "312                 fprintf(stderr,\n",
      "313                         \"Error freeing device pointer %p (%s) of size %llu. %llu byte already allocated.\"\n",
      "314                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "315                         ptr, cudaGetErrorString(err),\n",
      "316                         (unsigned long long)_alloc_size_table[i].size, (unsigned long long)_allocated_size, (unsigned long long)free, (unsigned long long)total);\n",
      "317             }\n",
      "318             #else\n",
      "319                 fprintf(stderr,\n",
      "320                         \"Error freeing device pointer %p (%s).\"\n",
      "321                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "322                         ptr,\n",
      "323                         cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "324             #endif\n",
      "325             if (NULL != PyErr_Occurred()){\n",
      "326                 fprintf(stderr,\n",
      "327                         \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "328                         \" Python error set. This happen during the clean up when there is a\"\n",
      "329                         \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "330                         \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "331                 return -1;\n",
      "332             }\n",
      "333             PyErr_Format(PyExc_MemoryError,\n",
      "334                     \"error freeing device pointer %p (%s)\",\n",
      "335                     ptr,\n",
      "336                     cudaGetErrorString(err));\n",
      "337             return -1;\n",
      "338         }\n",
      "339     }\n",
      "340     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "341     #if COMPUTE_GPU_MEM_USED\n",
      "342         int i=0;\n",
      "343         size_t total_freed = 0;\n",
      "344         for(;i<TABLE_SIZE;i++)\n",
      "345             if(_alloc_size_table[i].ptr==ptr){\n",
      "346                 _allocated_size -= _alloc_size_table[i].size;\n",
      "347                 total_freed += _alloc_size_table[i].size;\n",
      "348                 _alloc_size_table[i].ptr=0;\n",
      "349                 _alloc_size_table[i].size=0;\n",
      "350 \n",
      "351                 break;\n",
      "352             }\n",
      "353         //if(i==TABLE_SIZE)\n",
      "354         //    printf(\"Unallocated unknow size!\\n\");\n",
      "355         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "356     #endif\n",
      "357     return 0;\n",
      "358 }\n",
      "359 \n",
      "360 static PyObject *\n",
      "361 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "362 {\n",
      "363     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "364 }\n",
      "365 \n",
      "366 \n",
      "367 static void *work_mem = NULL;\n",
      "368 static size_t work_size = 0;\n",
      "369 \n",
      "370 /*\n",
      "371  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "372  * request a single chunk of memory at a time since it is reused.\n",
      "373  */\n",
      "374 void *get_work_mem(size_t sz) {\n",
      "375     if (sz <= work_size)\n",
      "376         return work_mem;\n",
      "377     device_free(work_mem);\n",
      "378     work_mem = device_malloc(sz);\n",
      "379     work_size = sz;\n",
      "380     if (work_mem == NULL)\n",
      "381         work_size = 0;\n",
      "382     return work_mem;\n",
      "383 }\n",
      "384 \n",
      "385 /////////////////////////\n",
      "386 // Static helper methods\n",
      "387 /////////////////////////\n",
      "388 \n",
      "389 static void\n",
      "390 CudaNdarray_null_init(CudaNdarray*self)\n",
      "391 {\n",
      "392     self->base = NULL;\n",
      "393     self->nd = -1;\n",
      "394     self->host_structure = NULL;\n",
      "395     self->data_allocated = 0;\n",
      "396     self->dev_structure_fresh = 1;\n",
      "397     self->dev_structure = NULL;\n",
      "398     self->devdata = NULL;\n",
      "399 }\n",
      "400 \n",
      "401 static int\n",
      "402 CudaNdarray_uninit(CudaNdarray*self)\n",
      "403 {\n",
      "404     #if PRINT_FREE_MALLOC\n",
      "405         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "406     #endif\n",
      "407     int rval = 0;\n",
      "408     if (self->data_allocated) {\n",
      "409         assert(self->devdata);\n",
      "410         if (device_free(self->devdata))\n",
      "411         {\n",
      "412             fprintf(stderr,\n",
      "413                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "414                     self, self->devdata);\n",
      "415             rval = -1;\n",
      "416         }\n",
      "417         self->devdata = NULL;\n",
      "418         self->data_allocated = 0;\n",
      "419     }\n",
      "420     if (self->dev_structure)\n",
      "421     {\n",
      "422         if (device_free(self->dev_structure))\n",
      "423         {\n",
      "424             fprintf(stderr,\n",
      "425                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "426                     self->dev_structure, self);\n",
      "427             rval = -1;\n",
      "428         }\n",
      "429         self->dev_structure = NULL;\n",
      "430     }\n",
      "431     if (self->host_structure)\n",
      "432     {\n",
      "433         free(self->host_structure);\n",
      "434         self->host_structure = NULL;\n",
      "435     }\n",
      "436     self->nd = -1;\n",
      "437     Py_XDECREF(self->base);\n",
      "438     self->base = NULL;\n",
      "439     return rval;\n",
      "440 }\n",
      "441 \n",
      "442 \n",
      "443 //make the rightmost coords change fastest\n",
      "444 //TODO: why does a downward for-loop not work????\n",
      "445 //TODO: use the log2_dims and driver code to remove / and %\n",
      "446 //TODO: skip the last division (when d == 0)\n",
      "447 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "448 __global__ void name (unsigned int numEls,  \\\n",
      "449         unsigned int nd, \\\n",
      "450         const int * dim,  \\\n",
      "451         const float * a_data, const int * a_str, \\\n",
      "452         float * z_data, const int * z_str) \\\n",
      "453 { \\\n",
      "454     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "455     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "456  \\\n",
      "457     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "458     { \\\n",
      "459         unsigned int ii = i; \\\n",
      "460         const float * a_i = a_data; \\\n",
      "461         float * z_i = z_data; \\\n",
      "462         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "463         { \\\n",
      "464             unsigned int d = nd - _d-1;  \\\n",
      "465             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "466             ii = ii / dim[d]; \\\n",
      "467             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "468             z_i += i_d * z_str[d]; \\\n",
      "469         } \\\n",
      "470         z_i[0] = F(a_i[0]); \\\n",
      "471     } \\\n",
      "472 }\n",
      "473 \n",
      "474 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "475 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "476 \n",
      "477 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "478 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "479 \n",
      "480 /////////////////////////////\n",
      "481 // Satisfying reqs to be Type\n",
      "482 /////////////////////////////\n",
      "483 \n",
      "484 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "485 static void\n",
      "486 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "487 {\n",
      "488     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "489     if(Py_REFCNT(self) > 1)\n",
      "490       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "491     CudaNdarray_uninit(self);\n",
      "492     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "493     --_outstanding_mallocs[1];\n",
      "494     if (0)\n",
      "495     {\n",
      "496         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "497                 _outstanding_mallocs[0],\n",
      "498                 _outstanding_mallocs[1]);\n",
      "499     }\n",
      "500 }\n",
      "501 \n",
      "502 static PyObject *\n",
      "503 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "504 {\n",
      "505     CudaNdarray *self;\n",
      "506 \n",
      "507     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "508     if (self != NULL)\n",
      "509     {\n",
      "510         CudaNdarray_null_init(self);\n",
      "511         ++_outstanding_mallocs[1];\n",
      "512     }\n",
      "513     return (PyObject *)self;\n",
      "514 }\n",
      "515 static int\n",
      "516 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "517 {\n",
      "518     PyObject *arr=NULL;\n",
      "519 \n",
      "520     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "521         return -1;\n",
      "522     if (! PyArray_Check(arr))\n",
      "523     {\n",
      "524         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "525         return -1;\n",
      "526     }\n",
      "527     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "528     return rval;\n",
      "529 }\n",
      "530 static PyMemberDef CudaNdarray_members[] =\n",
      "531 {\n",
      "532     /*\n",
      "533     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "534      \"first name\"},\n",
      "535     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "536      \"last name\"},\n",
      "537     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "538      \"noddy number\"},\n",
      "539      */\n",
      "540     {NULL}  /* Sentinel */\n",
      "541 };\n",
      "542 \n",
      "543 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "544 {\n",
      "545     PyObject * dtype = NULL;\n",
      "546     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "547         return NULL;\n",
      "548     if (dtype) {\n",
      "549         PyArray_Descr* dtype2;\n",
      "550         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "551         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "552         {\n",
      "553             PyObject * str = PyObject_Repr(dtype);\n",
      "554             PyErr_Format(PyExc_TypeError,\n",
      "555                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "556                          PyString_AsString(str)\n",
      "557                          );\n",
      "558             Py_CLEAR(str);\n",
      "559             return NULL;\n",
      "560         }\n",
      "561         int typeNum = dtype2->type_num;\n",
      "562         Py_DECREF(dtype2);\n",
      "563         if (typeNum != NPY_FLOAT32)\n",
      "564         {\n",
      "565             PyObject * str = PyObject_Repr(dtype);\n",
      "566             PyErr_Format(PyExc_TypeError,\n",
      "567                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "568                          typeNum\n",
      "569                          );\n",
      "570             Py_CLEAR(str);\n",
      "571             return NULL;\n",
      "572         }\n",
      "573     }\n",
      "574 \n",
      "575     int verbose = 0;\n",
      "576     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "577         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "578         assert (npydims);\n",
      "579         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "580         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "581         free(npydims);\n",
      "582         if (!rval){\n",
      "583             return NULL;\n",
      "584         }\n",
      "585         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "586         return rval;\n",
      "587     }\n",
      "588     if ((self->nd < 0) || (self->devdata == 0))\n",
      "589     {\n",
      "590         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "591         return NULL;\n",
      "592     }\n",
      "593     CudaNdarray * contiguous_self = NULL;\n",
      "594     if (CudaNdarray_is_c_contiguous(self))\n",
      "595     {\n",
      "596         contiguous_self = self;\n",
      "597         Py_INCREF(contiguous_self);\n",
      "598         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "599     }\n",
      "600     else\n",
      "601     {\n",
      "602         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "603         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "604     }\n",
      "605     if (!contiguous_self)\n",
      "606     {\n",
      "607         return NULL;\n",
      "608     }\n",
      "609 \n",
      "610     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "611     assert (npydims);\n",
      "612     for (int i = 0; i < self->nd; ++i)\n",
      "613         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "614     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "615                                                                npydims,\n",
      "616                                                                REAL_TYPENUM);\n",
      "617     free(npydims);\n",
      "618     if (!rval)\n",
      "619     {\n",
      "620         Py_DECREF(contiguous_self);\n",
      "621         return NULL;\n",
      "622     }\n",
      "623 \n",
      "624     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "625 \n",
      "626     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "627     void *rval_data = PyArray_DATA(rval);\n",
      "628     cudaError_t err;\n",
      "629     CNDA_BEGIN_ALLOW_THREADS;\n",
      "630 \n",
      "631     err = cudaMemcpy(rval_data, contiguous_self->devdata,\n",
      "632                      rval_size * sizeof(real),\n",
      "633                      cudaMemcpyDeviceToHost\n",
      "634                      );\n",
      "635     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "636     CNDA_END_ALLOW_THREADS;\n",
      "637 \n",
      "638     if (cudaSuccess != err)\n",
      "639     {\n",
      "640         PyErr_Format(PyExc_RuntimeError, \"error (%s)copying data to host\",\n",
      "641                      cudaGetErrorString(err));\n",
      "642         Py_DECREF(rval);\n",
      "643         rval = NULL;\n",
      "644     }\n",
      "645 \n",
      "646     Py_DECREF(contiguous_self);\n",
      "647     return (PyObject *)rval;\n",
      "648 }\n",
      "649 \n",
      "650 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "651 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "652 // but this naming is very weird, makes it look like a macro\n",
      "653 // we should figure out the correct convention and change to that\n",
      "654 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "655 {\n",
      "656 \n",
      "657     size_t total_elements = 1;\n",
      "658 \n",
      "659     for(size_t i=0;i<n;i++){\n",
      "660         // Detect overflow on unsigned integer\n",
      "661         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "662             PyErr_Format(PyExc_RuntimeError,\n",
      "663                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "664                          (unsigned long long)total_elements,\n",
      "665                          (unsigned long long)dims[i]);\n",
      "666             return NULL;\n",
      "667         }\n",
      "668         total_elements*=dims[i];\n",
      "669     }\n",
      "670 \n",
      "671     // total_elements now contains the size of the array, in reals\n",
      "672     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "673         PyErr_Format(PyExc_RuntimeError,\n",
      "674                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "675                      (unsigned long long)total_elements);\n",
      "676         return NULL;\n",
      "677     }\n",
      "678     size_t total_size = total_elements * sizeof(real);\n",
      "679 \n",
      "680     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "681     if (!rval)\n",
      "682     {\n",
      "683         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "684         return NULL;\n",
      "685     }\n",
      "686 \n",
      "687     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "688     {\n",
      "689         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "690         Py_DECREF(rval);\n",
      "691         return NULL;\n",
      "692     }\n",
      "693 \n",
      "694     // Fill with zeros\n",
      "695     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "696     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "697     {\n",
      "698         PyErr_Format(PyExc_MemoryError,\n",
      "699                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "700                      (unsigned long long)total_size);\n",
      "701         Py_DECREF(rval);\n",
      "702         return NULL;\n",
      "703     }\n",
      "704 \n",
      "705     if (cnda_copy_structure_to_device(rval))\n",
      "706     {\n",
      "707         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "708         Py_DECREF(rval);\n",
      "709         return NULL;\n",
      "710     }\n",
      "711     return (PyObject*) rval;\n",
      "712 }\n",
      "713 \n",
      "714 // declared as a static method (hence 1st parameter is not used)\n",
      "715 // Based on _Copy and _dimshuffle\n",
      "716 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "717 {\n",
      "718     if(!shape)\n",
      "719     {\n",
      "720         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "721         return NULL;\n",
      "722     }\n",
      "723     if(!PySequence_Check(shape))\n",
      "724     {\n",
      "725         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "726         return NULL;\n",
      "727     }\n",
      "728 \n",
      "729     int shplen = PySequence_Length(shape);\n",
      "730 \n",
      "731     if (shplen == 0)\n",
      "732     {\n",
      "733         return CudaNdarray_ZEROS(0, NULL);\n",
      "734     }\n",
      "735 \n",
      "736     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "737 \n",
      "738     if (!newdims)\n",
      "739     {\n",
      "740         PyErr_SetString(PyExc_MemoryError,\n",
      "741             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "742         return NULL;\n",
      "743     }\n",
      "744 \n",
      "745     // start from the end to compute strides\n",
      "746     for (int i = shplen-1; i >= 0; --i)\n",
      "747     {\n",
      "748         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "749         if(shp_el_obj == NULL)\n",
      "750         {\n",
      "751             // shouldn't happen since we checked length before...\n",
      "752             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "753             free(newdims);\n",
      "754             return NULL;\n",
      "755         }\n",
      "756 \n",
      "757         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "758         Py_DECREF(shp_el_obj);\n",
      "759 \n",
      "760         if (shp_el < 0)\n",
      "761         {\n",
      "762             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "763             free(newdims);\n",
      "764             return NULL;\n",
      "765         }\n",
      "766 \n",
      "767         newdims[i] = shp_el;\n",
      "768     }\n",
      "769 \n",
      "770     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "771 \n",
      "772     free(newdims);\n",
      "773 \n",
      "774     return (PyObject*)rval;\n",
      "775 }\n",
      "776 \n",
      "777 \n",
      "778 \n",
      "779 \n",
      "780 \n",
      "781 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "782 {\n",
      "783     PyObject * rval = CudaNdarray_New();\n",
      "784     if ((!rval) || (-1 == self->nd))\n",
      "785     {\n",
      "786         return rval;\n",
      "787     }\n",
      "788     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "789     {\n",
      "790         Py_DECREF(rval);\n",
      "791         return NULL;\n",
      "792     }\n",
      "793     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "794     {\n",
      "795         Py_DECREF(rval);\n",
      "796         return NULL;\n",
      "797     }\n",
      "798     return rval;\n",
      "799 }\n",
      "800 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "801 {\n",
      "802     assert(PyDict_Check(memo));\n",
      "803     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "804     assert(selfkey);\n",
      "805     if (PyDict_Contains(memo, selfkey))\n",
      "806     {\n",
      "807         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "808         Py_DECREF(selfkey);\n",
      "809         Py_XINCREF(rval);\n",
      "810         return rval;\n",
      "811     }\n",
      "812     else\n",
      "813     {\n",
      "814         PyObject * rval = CudaNdarray_Copy(self);\n",
      "815         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "816         if (NULL == rval)\n",
      "817         {\n",
      "818             Py_DECREF(selfkey);\n",
      "819             return NULL;\n",
      "820         }\n",
      "821         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "822         {\n",
      "823             Py_DECREF(rval);\n",
      "824             Py_DECREF(selfkey);\n",
      "825             return NULL;\n",
      "826         }\n",
      "827         Py_DECREF(selfkey);\n",
      "828         return rval;\n",
      "829     }\n",
      "830 }\n",
      "831 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "832 {\n",
      "833     if (!PySequence_Check(py_reduce_mask))\n",
      "834     {\n",
      "835         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "836         return NULL;\n",
      "837     }\n",
      "838     int len = PySequence_Length(py_reduce_mask);\n",
      "839     if (len != self->nd)\n",
      "840     {\n",
      "841         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "842         return NULL;\n",
      "843     }\n",
      "844     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "845     if (!self_sum)\n",
      "846     {\n",
      "847         return NULL;\n",
      "848     }\n",
      "849     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "850     //      and use it if it is big enough.\n",
      "851     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "852     int * sum_dims = dimshuffle_pattern + len;\n",
      "853     int n_remaining_dims = 0;\n",
      "854     if (!dimshuffle_pattern)\n",
      "855     {\n",
      "856         Py_DECREF(self_sum);\n",
      "857         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "858         return NULL;\n",
      "859     }\n",
      "860     for (int i = 0; i < len; ++i)\n",
      "861     {\n",
      "862         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "863         int o_i_int = PyInt_AsLong(o_i);\n",
      "864         Py_XDECREF(o_i);\n",
      "865         if (PyErr_Occurred())\n",
      "866         {\n",
      "867             Py_DECREF(self_sum);\n",
      "868             free(dimshuffle_pattern);\n",
      "869             return NULL;\n",
      "870         }\n",
      "871         if (o_i_int) // this is a dimension over which we are reducing\n",
      "872         {\n",
      "873             sum_dims[i] = 1;\n",
      "874         }\n",
      "875         else\n",
      "876         {\n",
      "877             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "878             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "879         }\n",
      "880     }\n",
      "881     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "882             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "883             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "884     {\n",
      "885         Py_DECREF(self_sum);\n",
      "886         free(dimshuffle_pattern);\n",
      "887         return NULL;\n",
      "888     }\n",
      "889     free(dimshuffle_pattern);\n",
      "890     return (PyObject*)self_sum;\n",
      "891 }\n",
      "892 \n",
      "893 // Reshape self to the new shape gived by the tuple shape.\n",
      "894 //\n",
      "895 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "896 // TODO: make it return a view when the strides allow it even if it is not\n",
      "897 //       c contiguous\n",
      "898 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "899 {\n",
      "900     if(!CudaNdarray_is_c_contiguous(self))\n",
      "901     {\n",
      "902         // allocate new space\n",
      "903         //TODO: test to see if we can re-use old one and take a new param to\n",
      "904         //  use this\n",
      "905         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "906         if (!rval)\n",
      "907         {\n",
      "908             return NULL;\n",
      "909         }\n",
      "910 \n",
      "911         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "912         Py_XDECREF(rval);\n",
      "913         return (PyObject*)ret;\n",
      "914     }\n",
      "915 \n",
      "916     // check shape tuple\n",
      "917     unsigned int rval_nd;\n",
      "918     unsigned int * rval_dims;\n",
      "919     size_t rval_size = 1;\n",
      "920 \n",
      "921     if (PyTuple_Check(shape)){\n",
      "922         // copy shape to integer array\n",
      "923         rval_nd = PyTuple_Size(shape);\n",
      "924     }else if (PyInt_Check(shape)){\n",
      "925         rval_nd = 1;\n",
      "926     }else{\n",
      "927         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "928         return NULL;\n",
      "929     }\n",
      "930     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "931 \n",
      "932     if(PyTuple_Check(shape)){\n",
      "933         for (int i = 0; i < rval_nd; ++i)\n",
      "934         {\n",
      "935             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "936             if (PyErr_Occurred()) //error in AsLong\n",
      "937             {\n",
      "938                 free(rval_dims);\n",
      "939                 return NULL;\n",
      "940             }\n",
      "941             if(rval_dims[i]<0){\n",
      "942                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >=0)\",rval_dims[i]);\n",
      "943                 free(rval_dims);\n",
      "944                 return NULL;\n",
      "945             }\n",
      "946             rval_size = rval_size * rval_dims[i];\n",
      "947         }\n",
      "948     }else{\n",
      "949         rval_size = PyInt_AsLong(shape);\n",
      "950         rval_dims[0] = rval_size;\n",
      "951     }\n",
      "952     // calculate new size, assert same as old size\n",
      "953     if (rval_size != CudaNdarray_SIZE(self))\n",
      "954     {\n",
      "955         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %lld to %lld\", CudaNdarray_SIZE(self), rval_size);\n",
      "956         free(rval_dims);\n",
      "957         return NULL;\n",
      "958     }\n",
      "959     if (rval_size==0)\n",
      "960     {\n",
      "961         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "962         free(rval_dims);\n",
      "963         return rval;\n",
      "964     }\n",
      "965 \n",
      "966     //return a view, not a copy\n",
      "967     //we can do this as we checked self is c_contiguous\n",
      "968     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "969 \n",
      "970     if (!rval || 0 != rval->data_allocated\n",
      "971         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "972     {\n",
      "973         Py_XDECREF(rval);\n",
      "974         free(rval_dims);\n",
      "975         return NULL;\n",
      "976     }\n",
      "977     //set dim and stride\n",
      "978     int size = 1;\n",
      "979     for (int i = rval_nd-1; i >= 0; --i)\n",
      "980     {\n",
      "981         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "982         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "983         size = size * rval_dims[i];\n",
      "984     }\n",
      "985     free(rval_dims);\n",
      "986     return (PyObject*)rval;\n",
      "987 }\n",
      "988 \n",
      "989 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "990 {\n",
      "991     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "992     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "993     {\n",
      "994         Py_XDECREF(rval);\n",
      "995         rval = NULL;\n",
      "996     }\n",
      "997     else\n",
      "998     {\n",
      "999         for (int i = 0; i < self->nd; ++i)\n",
      "1000         {\n",
      "1001             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "1002             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "1003         }\n",
      "1004     }\n",
      "1005     return (PyObject*)rval;\n",
      "1006 }\n",
      "1007 \n",
      "1008 /*\n",
      "1009  * d0,... are the output dims\n",
      "1010  * indices are a list of index to operate on\n",
      "1011  *         They are int32 viewed as float32.\n",
      "1012  * a is the output\n",
      "1013  * b is the input\n",
      "1014  * dB0, the source leading dimensions size\n",
      "1015  */\n",
      "1016 template <int operator_num>\n",
      "1017 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "1018                          const npy_int64* indices,\n",
      "1019                          float* a,\n",
      "1020                          const int sA0, const int sA1, const int sA2,\n",
      "1021                          const float* b, const int dB0,\n",
      "1022                          const int sB0, const int sB1, const int sB2,\n",
      "1023                          int* err){\n",
      "1024     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1025         npy_int64 idx = indices[i0];\n",
      "1026         if (idx<0)\n",
      "1027             idx += dB0; // To allow negative indexing.\n",
      "1028         if ((idx < 0) || (idx >= dB0)){\n",
      "1029             // Any value other the 0 probably work. But to be more safe, I want\n",
      "1030             // to change all bits to prevent problem with concurrent write that\n",
      "1031             // could cross cache line. But this should not happen with the\n",
      "1032             // current code and driver.\n",
      "1033             *err = 0xFFFF;\n",
      "1034             continue;\n",
      "1035         }\n",
      "1036         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "1037             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "1038                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "1039                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "1040                 a[a_idx] = b[b_idx];\n",
      "1041             }\n",
      "1042         }\n",
      "1043     }\n",
      "1044 }\n",
      "1045 \n",
      "1046 // We try to be similar to the PyArray_TakeFrom function\n",
      "1047 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "1048 //TODO: support other clip mode then raise(clip, wrap)\n",
      "1049 //self is the input that we copy data from.\n",
      "1050 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "1051 //    that is in fact a view to int64 indices\n",
      "1052 PyObject*\n",
      "1053 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "1054     int verbose = 0;\n",
      "1055     PyObject * indices_obj = NULL;\n",
      "1056     //int axis; Default None, that mean the flattened array.\n",
      "1057     PyObject * axis_obj = Py_None;\n",
      "1058     PyObject * out_obj = Py_None;\n",
      "1059     PyObject * clipmode_obj = NULL;\n",
      "1060     int max_threads = 1; // max threads per blocks\n",
      "1061 \n",
      "1062     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1063                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1064         return NULL;\n",
      "1065 \n",
      "1066     //Check argument indices\n",
      "1067     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1068     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1069     //TODO: Support ndarray of other dtype then int32\n",
      "1070     //TODO: support list of indices that are not c_contiguous\n",
      "1071     CudaNdarray * indices = NULL;\n",
      "1072     if (CudaNdarray_Check(indices_obj)) {\n",
      "1073         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1074         indices = (CudaNdarray*) indices_obj;\n",
      "1075         Py_INCREF(indices);\n",
      "1076     } else if (PyArray_Check(indices_obj)) {\n",
      "1077         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1078         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {\n",
      "1079             PyErr_SetString(PyExc_TypeError,\n",
      "1080                             \"CudaNdarray_TakeFrom: need a ndarray for indices\"\n",
      "1081                             \" with dtype int64\");\n",
      "1082             return NULL;\n",
      "1083         }\n",
      "1084         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1085             PyErr_SetString(PyExc_TypeError,\n",
      "1086                             \"CudaNdarray_TakeFrom: need a CudaNdarray of\"\n",
      "1087                             \" indices with only 1 dimensions\");\n",
      "1088             return NULL;\n",
      "1089         }\n",
      "1090         // We need indices_obj to be contiguous, in order to take a view\n",
      "1091         // with a different dtype.\n",
      "1092         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {\n",
      "1093             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);\n",
      "1094             if (!indices_obj_contig)\n",
      "1095                 return NULL;\n",
      "1096             indices_obj = indices_obj_contig;\n",
      "1097         } else {\n",
      "1098             // Keep the refcount consistent\n",
      "1099             Py_INCREF(indices_obj);\n",
      "1100         }\n",
      "1101         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1102         PyObject * indices_float32 = NULL;\n",
      "1103         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1104                                                   float32_descr, NULL);\n",
      "1105         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1106         if (!indices_float32) {\n",
      "1107             Py_DECREF(indices_obj);\n",
      "1108             return NULL;\n",
      "1109         }\n",
      "1110 \n",
      "1111         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1112         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1113         if (! indices){\n",
      "1114             Py_DECREF(indices_obj);\n",
      "1115             Py_DECREF(indices_float32);\n",
      "1116             return NULL;\n",
      "1117         }\n",
      "1118         if (CudaNdarray_CopyFromArray(indices,\n",
      "1119                                       (PyArrayObject *)indices_float32)){\n",
      "1120             Py_DECREF(indices_obj);\n",
      "1121             Py_DECREF(indices_float32);\n",
      "1122             return NULL;\n",
      "1123         }\n",
      "1124         Py_DECREF(indices_obj);\n",
      "1125         Py_DECREF(indices_float32);\n",
      "1126     } else {\n",
      "1127         PyObject* py_s = PyObject_Str(indices_obj);\n",
      "1128         const char* s = PyString_AsString(py_s);\n",
      "1129         Py_DECREF(py_s);\n",
      "1130         PyErr_Format(PyExc_TypeError,\n",
      "1131                      \"CudaNdarray_TakeFrom: need an ndarray of int64 or a\"\n",
      "1132                      \" CudaNdarray(float32) that is a view from int64 data\"\n",
      "1133                      \" for indices. Got %s\", s);\n",
      "1134         return NULL;\n",
      "1135     }\n",
      "1136 \n",
      "1137     if (verbose) {\n",
      "1138         printf(\"indices used on the gpu\\n\");\n",
      "1139         fprint_CudaNdarray(stdout, indices);\n",
      "1140         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1141         PyObject_Print(used_indices, stdout, 0);\n",
      "1142         Py_DECREF(used_indices);\n",
      "1143     }\n",
      "1144     if (verbose) printf(\"after print of object\\n\");\n",
      "1145     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1146         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1147                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1148         Py_DECREF(indices);\n",
      "1149         return NULL;\n",
      "1150     }\n",
      "1151     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1152 \n",
      "1153     //Check argument axis\n",
      "1154     //TODO: implement the default and other axis\n",
      "1155     long axis = PyInt_AsLong(axis_obj);\n",
      "1156 \n",
      "1157     if (axis != 0) {\n",
      "1158         PyErr_Format(PyExc_NotImplementedError,\n",
      "1159                      \"CudaNdarray_TakeFrom: only axis=0 is currently supported.\"\n",
      "1160                      \" Got %ld.\", axis);\n",
      "1161         Py_DECREF(indices);\n",
      "1162         return NULL;\n",
      "1163     }\n",
      "1164 \n",
      "1165     //Check argument out_obj\n",
      "1166     CudaNdarray * out = NULL;\n",
      "1167     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1168         out = (CudaNdarray*) out_obj;\n",
      "1169     if (out && (out->nd != self->nd ||\n",
      "1170                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1171         out = NULL;\n",
      "1172     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1173     dims[0] = nb_indices;\n",
      "1174 \n",
      "1175     for (int i=1 ; i<self->nd ; i++) {\n",
      "1176         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1177         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1178             out = NULL;\n",
      "1179         }\n",
      "1180     }\n",
      "1181     if (!out) {\n",
      "1182         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1183         if (!out){\n",
      "1184             Py_DECREF(indices);\n",
      "1185             free(dims);\n",
      "1186             return NULL;\n",
      "1187         }\n",
      "1188         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1189             Py_DECREF(out);\n",
      "1190             Py_DECREF(indices);\n",
      "1191             free(dims);\n",
      "1192             return NULL;\n",
      "1193         }\n",
      "1194     }else {\n",
      "1195         Py_INCREF(out);\n",
      "1196     }\n",
      "1197 \n",
      "1198     //Check argument clipmode\n",
      "1199     if (clipmode_obj) {\n",
      "1200         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1201         if (! clipmode){\n",
      "1202             Py_DECREF(indices);\n",
      "1203             Py_DECREF(out);\n",
      "1204             free(dims);\n",
      "1205             return NULL;\n",
      "1206         }\n",
      "1207         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1208             PyErr_Format(PyExc_NotImplementedError,\n",
      "1209                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1210                          clipmode);\n",
      "1211             Py_DECREF(indices);\n",
      "1212             Py_DECREF(out);\n",
      "1213             free(dims);\n",
      "1214             return NULL;\n",
      "1215         }\n",
      "1216     }\n",
      "1217     void (*k3)(const int, const int, const int,\n",
      "1218                const npy_int64*,\n",
      "1219                float*, const int, const int, const int,\n",
      "1220                const float*, const int,\n",
      "1221                const int, const int, const int,\n",
      "1222                int*);\n",
      "1223     k3 = k_take_3<CPY>;\n",
      "1224 \n",
      "1225     // Create the memory place that will store the error information.\n",
      "1226     if(init_err_var() != 0) return NULL;\n",
      "1227 \n",
      "1228     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1229     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1230         // We take 0 elements, so no need for the rest of the code.\n",
      "1231         // This speed up that case AND fix crash otherwise.\n",
      "1232         free(dims);\n",
      "1233         Py_DECREF(indices);\n",
      "1234         return (PyObject *)out;\n",
      "1235     }\n",
      "1236 \n",
      "1237     switch (self->nd) {\n",
      "1238         case 1:\n",
      "1239             {\n",
      "1240                 dim3 n_threads(1, 1, 1);\n",
      "1241                 if (verbose)\n",
      "1242                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1243                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1244                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1245                            cudaGetLastError(), self->nd,\n",
      "1246                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1247                 k3<<<n_blocks, n_threads>>>(\n",
      "1248                         dims[0],\n",
      "1249                         1,\n",
      "1250                         1,\n",
      "1251                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1252                         CudaNdarray_DEV_DATA(out),\n",
      "1253                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1254                         1,\n",
      "1255                         1,\n",
      "1256                         CudaNdarray_DEV_DATA(self),\n",
      "1257                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1258                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1259                         1,\n",
      "1260                         1,\n",
      "1261                         err_var);\n",
      "1262             }\n",
      "1263             break;\n",
      "1264         case 2:\n",
      "1265             {\n",
      "1266                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1267 \n",
      "1268                 if (verbose)\n",
      "1269                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1270                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1271                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1272                            cudaGetLastError(), self->nd,\n",
      "1273                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1274 \n",
      "1275                 k3<<<n_blocks, n_threads>>>(\n",
      "1276                         dims[0], //dimensions\n",
      "1277                         dims[1],\n",
      "1278                         1,\n",
      "1279                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1280                         CudaNdarray_DEV_DATA(out),\n",
      "1281                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1282                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1283                         1,\n",
      "1284                         CudaNdarray_DEV_DATA(self),\n",
      "1285                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1286                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1287                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1288                         1,\n",
      "1289                         err_var);\n",
      "1290             }\n",
      "1291             break;\n",
      "1292         case 3:\n",
      "1293             {\n",
      "1294                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1295                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1296                 dim3 n_threads(tx, ty, 1);\n",
      "1297                 if (verbose)\n",
      "1298                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1299                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1300                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1301                            cudaGetLastError(), self->nd,\n",
      "1302                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1303                 k3<<<n_blocks, n_threads>>>(\n",
      "1304                         dims[0], //dimensions\n",
      "1305                         dims[1],\n",
      "1306                         dims[2],\n",
      "1307                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1308                         CudaNdarray_DEV_DATA(out),\n",
      "1309                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1310                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1311                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1312                         CudaNdarray_DEV_DATA(self),\n",
      "1313                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1314                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1315                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1316                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1317                         err_var);\n",
      "1318             }\n",
      "1319             break;\n",
      "1320     default:\n",
      "1321         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1322                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1323                         \" dimensions are currently supported\");\n",
      "1324 \n",
      "1325     }\n",
      "1326     free(dims);\n",
      "1327     CNDA_THREAD_SYNC;\n",
      "1328     cudaError_t err = cudaGetLastError();\n",
      "1329     if (cudaSuccess != err) {\n",
      "1330         PyErr_Format(PyExc_RuntimeError,\n",
      "1331                      \"Cuda error: %s: %s.\\n\",\n",
      "1332                      \"CudaNdarray_TakeFrom\",\n",
      "1333                      cudaGetErrorString(err));\n",
      "1334         Py_DECREF(indices);\n",
      "1335         Py_DECREF(out);\n",
      "1336         return NULL;\n",
      "1337     }\n",
      "1338 \n",
      "1339     int index_err = check_err_var();\n",
      "1340     Py_DECREF(indices);\n",
      "1341     if (index_err != 0) {\n",
      "1342         Py_DECREF(out);\n",
      "1343         return NULL;\n",
      "1344     }\n",
      "1345 \n",
      "1346     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1347     return (PyObject *)out;\n",
      "1348 }\n",
      "1349 \n",
      "1350 \n",
      "1351 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1352 {\n",
      "1353     int pos, stride;\n",
      "1354     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1355         return NULL;\n",
      "1356     if ((pos < 0) || (pos >= self->nd))\n",
      "1357     {\n",
      "1358         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1359         return NULL;\n",
      "1360     }\n",
      "1361     CudaNdarray_set_stride(self, pos, stride);\n",
      "1362     if (cnda_copy_structure_to_device(self))\n",
      "1363     {\n",
      "1364         return NULL;\n",
      "1365     }\n",
      "1366     Py_INCREF(Py_None);\n",
      "1367     return Py_None;\n",
      "1368 }\n",
      "1369 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1370 {\n",
      "1371     int pos, dim;\n",
      "1372     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1373         return NULL;\n",
      "1374     if ((pos < 0) || (pos >= self->nd))\n",
      "1375     {\n",
      "1376         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1377         return NULL;\n",
      "1378     }\n",
      "1379     CudaNdarray_set_dim(self, pos, dim);\n",
      "1380     if (cnda_copy_structure_to_device(self))\n",
      "1381     {\n",
      "1382         return NULL;\n",
      "1383     }\n",
      "1384     Py_INCREF(Py_None);\n",
      "1385     return Py_None;\n",
      "1386 }\n",
      "1387 \n",
      "1388 static PyObject *\n",
      "1389 CudaNdarray_exp(CudaNdarray* self)\n",
      "1390 {\n",
      "1391     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1392     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1393     {\n",
      "1394         Py_XDECREF(rval);\n",
      "1395         return NULL;\n",
      "1396     }\n",
      "1397     unsigned int size = 1;\n",
      "1398     for (int i = 0; i < self->nd; i++)\n",
      "1399     {\n",
      "1400         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1401     }\n",
      "1402     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1403     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1404     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1405             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1406             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1407 \n",
      "1408     //TODO: don't do this right away, do it when we need the result\n",
      "1409     CNDA_THREAD_SYNC;\n",
      "1410     cudaError_t err = cudaGetLastError();\n",
      "1411     if( cudaSuccess != err)\n",
      "1412     {\n",
      "1413         Py_DECREF(rval);\n",
      "1414         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1415         return NULL;\n",
      "1416     }\n",
      "1417 \n",
      "1418     return (PyObject*)rval;\n",
      "1419 }\n",
      "1420 \n",
      "1421 static PyMethodDef CudaNdarray_methods[] =\n",
      "1422 {\n",
      "1423     {\"__array__\",\n",
      "1424         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1425         \"Copy from the device to a numpy ndarray\"},\n",
      "1426     {\"__copy__\",\n",
      "1427         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1428         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1429     {\"__deepcopy__\",\n",
      "1430         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1431         \"Create a copy of this object\"},\n",
      "1432     {\"zeros\",\n",
      "1433         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1434         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1435     {\"copy\",\n",
      "1436         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1437         \"Create a copy of this object\"},\n",
      "1438     {\"is_c_contiguous\",\n",
      "1439         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1440         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1441     {\"reduce_sum\",\n",
      "1442         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1443         \"Reduce over the given dimensions by summation\"},\n",
      "1444     {\"exp\",\n",
      "1445         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1446         \"Return the exponential of all elements\"},\n",
      "1447     {\"reshape\",\n",
      "1448         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1449         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1450             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1451     {\"view\",\n",
      "1452         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1453         \"Return an alias of this ndarray\"},\n",
      "1454     {\"_set_stride\",\n",
      "1455         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1456         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1457     {\"take\",\n",
      "1458         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1459         \"Equivalent of numpy.take\"},\n",
      "1460     {\"_set_shape_i\",\n",
      "1461         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1462         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1463     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1464 };\n",
      "1465 \n",
      "1466 \n",
      "1467 ////////////////////\n",
      "1468 // Number protocol\n",
      "1469 ////////////////////\n",
      "1470 \n",
      "1471 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1472     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1473     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1474 \n",
      "1475     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1476         dest[i] = a[i] + b[i];\n",
      "1477     }\n",
      "1478 }\n",
      "1479 \n",
      "1480 // Will be called by __add__ in Python\n",
      "1481 static PyObject *\n",
      "1482 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1483 {\n",
      "1484     if (! CudaNdarray_Check(py_self)) {\n",
      "1485         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1486         return NULL;\n",
      "1487     }\n",
      "1488     if (! CudaNdarray_Check(py_other)) {\n",
      "1489         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1490         return NULL;\n",
      "1491     }\n",
      "1492     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1493     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1494     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1495         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1496         return NULL;\n",
      "1497     }\n",
      "1498 \n",
      "1499     //standard elemwise size checks\n",
      "1500     if (self->nd != other->nd)\n",
      "1501     {\n",
      "1502         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1503         return NULL;\n",
      "1504     }\n",
      "1505     //standard elemwise dim checks\n",
      "1506     unsigned int size = 1;\n",
      "1507     for (int i = 0; i< self->nd; ++i)\n",
      "1508     {\n",
      "1509         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1510         {\n",
      "1511             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1512             return NULL;\n",
      "1513         }\n",
      "1514         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1515     }\n",
      "1516     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1517     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1518     {\n",
      "1519         Py_XDECREF(rval);\n",
      "1520         return NULL;\n",
      "1521     }\n",
      "1522 \n",
      "1523     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1524       return (PyObject *) rval;\n",
      "1525     }\n",
      "1526 \n",
      "1527     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1528     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1529     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1530             self->devdata, other->devdata, rval->devdata, size);\n",
      "1531     CNDA_THREAD_SYNC;\n",
      "1532     cudaError_t err = cudaGetLastError();\n",
      "1533     if( cudaSuccess != err)\n",
      "1534     {\n",
      "1535         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1536         Py_DECREF(rval);\n",
      "1537         return NULL;\n",
      "1538     }\n",
      "1539     return (PyObject *) rval;\n",
      "1540 }\n",
      "1541 \n",
      "1542 template <int operator_num>\n",
      "1543 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1544         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1545         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1546     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1547         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1548             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1549                 switch (operator_num)\n",
      "1550                 {\n",
      "1551                   case IADD:\n",
      "1552                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1553                     break;\n",
      "1554                   case IDIV:\n",
      "1555                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1556                     break;\n",
      "1557                   case CPY:\n",
      "1558                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1559                     break;\n",
      "1560                 }\n",
      "1561             }\n",
      "1562         }\n",
      "1563     }\n",
      "1564 }\n",
      "1565 \n",
      "1566 template <int operator_num>\n",
      "1567 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1568                          float* a, const int sA0, const int sA1,\n",
      "1569                          const int sA2, const int sA3,\n",
      "1570                          const float* b, const int sB0, const int sB1,\n",
      "1571                          const int sB2, const int sB3){\n",
      "1572     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1573         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1574             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1575                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1576                     switch (operator_num) {\n",
      "1577                         case IADD:\n",
      "1578                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1579                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1580                             break;\n",
      "1581                         case IDIV:\n",
      "1582                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1583                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1584                             break;\n",
      "1585                         case CPY:\n",
      "1586                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1587                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1588                             break;\n",
      "1589                     }\n",
      "1590                 }\n",
      "1591             }\n",
      "1592         }\n",
      "1593     }\n",
      "1594 }\n",
      "1595 \n",
      "1596 template <int operator_num>\n",
      "1597 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1598                           const int d2, const int d3,\n",
      "1599                           const int d4, const int d5,\n",
      "1600                           float* a, const int sA0, const int sA1,\n",
      "1601                           const int sA2, const int sA3,\n",
      "1602                           const int sA4, const int sA5,\n",
      "1603                           const float* b, const int sB0, const int sB1,\n",
      "1604                           const int sB2, const int sB3,\n",
      "1605                           const int sB4, const int sB5\n",
      "1606                           ){\n",
      "1607     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1608         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1609             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1610                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1611                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1612                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1613                             switch (operator_num) {\n",
      "1614                             case IADD:\n",
      "1615                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1616                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1617                                 break;\n",
      "1618                             case IDIV:\n",
      "1619                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1620                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1621                                 break;\n",
      "1622                             case CPY:\n",
      "1623                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1624                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1625                                 break;\n",
      "1626                             }\n",
      "1627                         }\n",
      "1628                     }\n",
      "1629                 }\n",
      "1630             }\n",
      "1631         }\n",
      "1632     }\n",
      "1633 }\n",
      "1634 \n",
      "1635 /*\n",
      "1636 CudaNdarray_inplace_elemwise\n",
      "1637 Compute elemwise, working inplace on A.\n",
      "1638 Currently implemented A / B, A + B and A = B\n",
      "1639 (the last is not tested and not used!)\n",
      "1640 \n",
      "1641 py_self - the CudaNdarray that we'll modify (A)\n",
      "1642 py_other - the other argument (B)\n",
      "1643 fct_nb - which operation to perform (operator_t)\n",
      "1644 \n",
      "1645 Returns 0 on success.\n",
      "1646 Returns -1 on failure, and sets Python exception.\n",
      "1647 \n",
      "1648 */\n",
      "1649 int\n",
      "1650 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1651 {\n",
      "1652     int verbose = 0;\n",
      "1653     void (*k3)(const int, const int, const int,\n",
      "1654                     float*, const int, const int, const int,\n",
      "1655                     const float*, const int, const int, const int);\n",
      "1656     void (*k4)(const int, const int, const int, const int,\n",
      "1657                     float*, const int, const int,\n",
      "1658                     const int, const int,\n",
      "1659                     const float*, const int, const int,\n",
      "1660                     const int, const int);\n",
      "1661     void (*k6)(const int, const int,\n",
      "1662                const int, const int,\n",
      "1663                const int, const int,\n",
      "1664                float*, const int, const int,\n",
      "1665                const int, const int,\n",
      "1666                const int, const int,\n",
      "1667                const float*, const int, const int,\n",
      "1668                const int, const int,\n",
      "1669                const int, const int);\n",
      "1670     switch (fct_nb)\n",
      "1671     {\n",
      "1672         case IADD:\n",
      "1673             k3 = k_ielem_3<IADD>;\n",
      "1674             k4 = k_ielem_4<IADD>;\n",
      "1675             k6 = k_ielem_6<IADD>;\n",
      "1676             break;\n",
      "1677         case IDIV:\n",
      "1678             k3 = k_ielem_3<IDIV>;\n",
      "1679             k4 = k_ielem_4<IDIV>;\n",
      "1680             k6 = k_ielem_6<IDIV>;\n",
      "1681             break;\n",
      "1682         case CPY:\n",
      "1683             k3 = k_ielem_3<CPY>;\n",
      "1684             k4 = k_ielem_4<CPY>;\n",
      "1685             k6 = k_ielem_6<CPY>;\n",
      "1686             break;\n",
      "1687         default:\n",
      "1688             assert (0);\n",
      "1689             PyErr_Format(\n",
      "1690                 PyExc_TypeError,\n",
      "1691                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1692                 (int)fct_nb);\n",
      "1693             return -1;\n",
      "1694     }\n",
      "1695     if (!CudaNdarray_Check(py_self)) {\n",
      "1696         PyErr_SetString(\n",
      "1697             PyExc_TypeError,\n",
      "1698             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1699         return -1;\n",
      "1700     }\n",
      "1701     CudaNdarray * new_other = NULL;\n",
      "1702     if (!CudaNdarray_Check(py_other)) {\n",
      "1703         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1704         if(!new_other)\n",
      "1705         {\n",
      "1706             return -1;\n",
      "1707         }\n",
      "1708         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1709         {\n",
      "1710             Py_XDECREF(new_other);\n",
      "1711             return -1;\n",
      "1712         }\n",
      "1713         py_other = (PyObject *) new_other;\n",
      "1714     }\n",
      "1715 \n",
      "1716     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1717     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1718 \n",
      "1719     if (verbose)\n",
      "1720     {\n",
      "1721         fprintf(stderr,\n",
      "1722             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1723             self->nd, other->nd);\n",
      "1724     }\n",
      "1725 \n",
      "1726     //standard elemwise nb dim checks\n",
      "1727     if (self->nd < other->nd)\n",
      "1728     {\n",
      "1729         PyErr_Format(\n",
      "1730             PyExc_TypeError,\n",
      "1731             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1732             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1733             self->nd, other->nd);\n",
      "1734         Py_XDECREF(new_other);\n",
      "1735         return -1;\n",
      "1736     }\n",
      "1737 \n",
      "1738     //broadcast to the same number of dimensions.\n",
      "1739     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1740     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1741     int added_dims = self->nd - other->nd;\n",
      "1742     // Add the added broadcasted dimensions\n",
      "1743     for (int i = 0; i< added_dims; ++i)\n",
      "1744     {\n",
      "1745         other_dims[i] = 1;\n",
      "1746         other_strides[i] = 0;\n",
      "1747     }\n",
      "1748     // Copy the existing dimensions\n",
      "1749     for (int i = 0; i< other->nd; ++i)\n",
      "1750     {\n",
      "1751         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1752         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1753     }\n",
      "1754 \n",
      "1755     //standard elemwise dim checks\n",
      "1756     unsigned int size = 1;\n",
      "1757     for (int i = 0; i< self->nd; ++i)\n",
      "1758     {\n",
      "1759         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1760             && (other_dims[i] != 1))\n",
      "1761         {\n",
      "1762             PyErr_SetString(\n",
      "1763                 PyExc_ValueError,\n",
      "1764                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1765             Py_XDECREF(new_other);\n",
      "1766             return -1;\n",
      "1767         }\n",
      "1768         // if we're broadcasting other, then make sure it has stride 0\n",
      "1769         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1770             || (other_strides[i] == 0));\n",
      "1771         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1772     }\n",
      "1773 \n",
      "1774     if (size==0)\n",
      "1775     {\n",
      "1776         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1777         if (!(other_size == 0 || other_size == 1))\n",
      "1778         {\n",
      "1779             PyErr_SetString(\n",
      "1780                 PyExc_ValueError,\n",
      "1781                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1782                 \" un-initialized array when the new value have more than\"\n",
      "1783                 \" 0 or 1 broadcastable dimensions\");\n",
      "1784             Py_XDECREF(new_other);\n",
      "1785             return 0;\n",
      "1786         }\n",
      "1787         Py_XDECREF(new_other);\n",
      "1788         return 0;\n",
      "1789     }\n",
      "1790 \n",
      "1791     switch(self->nd)\n",
      "1792     {\n",
      "1793         case 0:\n",
      "1794             {\n",
      "1795                 dim3 n_blocks(1, 1, 1);\n",
      "1796                 dim3 n_threads(1);\n",
      "1797                 k3<<<n_blocks, n_threads>>>(\n",
      "1798                         1, //d0\n",
      "1799                         1, //d1\n",
      "1800                         1, //d2\n",
      "1801                         CudaNdarray_DEV_DATA(self),\n",
      "1802                         1, //strides\n",
      "1803                         1,\n",
      "1804                         1,\n",
      "1805                         CudaNdarray_DEV_DATA(other),\n",
      "1806                         1, //strides\n",
      "1807                         1,\n",
      "1808                         1);\n",
      "1809                 CNDA_THREAD_SYNC;\n",
      "1810                 cudaError_t err = cudaGetLastError();\n",
      "1811                 if (cudaSuccess != err)\n",
      "1812                 {\n",
      "1813                     PyErr_Format(\n",
      "1814                         PyExc_RuntimeError,\n",
      "1815                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1816                         \"k3\",\n",
      "1817                         cudaGetErrorString(err));\n",
      "1818                     Py_XDECREF(new_other);\n",
      "1819                     return -1;\n",
      "1820                 }\n",
      "1821             }\n",
      "1822             break;\n",
      "1823         case 1:\n",
      "1824             {\n",
      "1825                 dim3 n_blocks(1, 1, 1);\n",
      "1826                 dim3 n_threads(\n",
      "1827                         std::min(\n",
      "1828                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1829                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1830                 k3<<<n_blocks, n_threads>>>(\n",
      "1831                         1, //dimensions\n",
      "1832                         1,\n",
      "1833                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1834                         CudaNdarray_DEV_DATA(self),\n",
      "1835                         1, //strides\n",
      "1836                         1,\n",
      "1837                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1838                         CudaNdarray_DEV_DATA(other),\n",
      "1839                         1, //strides\n",
      "1840                         1,\n",
      "1841                         other_strides[0]);\n",
      "1842                 CNDA_THREAD_SYNC;\n",
      "1843                 cudaError_t err = cudaGetLastError();\n",
      "1844                 if (cudaSuccess != err)\n",
      "1845                 {\n",
      "1846                     PyErr_Format(\n",
      "1847                         PyExc_RuntimeError,\n",
      "1848                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1849                         \"k3\",\n",
      "1850                         cudaGetErrorString(err));\n",
      "1851                     Py_XDECREF(new_other);\n",
      "1852                     return -1;\n",
      "1853                 }\n",
      "1854             }\n",
      "1855             break;\n",
      "1856         case 2:\n",
      "1857             {\n",
      "1858                 //TODO:  if both self and other are f-contiguous\n",
      "1859                 //       Then flip the block and thread dimensions\n",
      "1860                 //       to make contiguous reads & writes\n",
      "1861                 dim3 n_blocks(1,\n",
      "1862                         std::min(\n",
      "1863                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1864                             NUM_VECTOR_OP_BLOCKS));\n",
      "1865                 dim3 n_threads(\n",
      "1866                         std::min(\n",
      "1867                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1868                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1869                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1870                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1871                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1872                         CudaNdarray_DEV_DATA(self),\n",
      "1873                         1,\n",
      "1874                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1875                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1876                         CudaNdarray_DEV_DATA(other),\n",
      "1877                         1,\n",
      "1878                         other_strides[0],\n",
      "1879                         other_strides[1]);\n",
      "1880                 CNDA_THREAD_SYNC;\n",
      "1881                 cudaError_t err = cudaGetLastError();\n",
      "1882                 if (cudaSuccess != err)\n",
      "1883                 {\n",
      "1884                     PyErr_Format(\n",
      "1885                         PyExc_RuntimeError,\n",
      "1886                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1887                         \"k3\",\n",
      "1888                         cudaGetErrorString(err));\n",
      "1889                     Py_XDECREF(new_other);\n",
      "1890                     return -1;\n",
      "1891                 }\n",
      "1892             }\n",
      "1893             break;\n",
      "1894         case 3:\n",
      "1895             {\n",
      "1896                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1897                 //       has a contiguous dimension on the thread idx.\n",
      "1898                 dim3 n_blocks(\n",
      "1899                         std::min(\n",
      "1900                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1901                             NUM_VECTOR_OP_BLOCKS),\n",
      "1902                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1903                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1904                     n_blocks.y /= 2;\n",
      "1905                 dim3 n_threads(\n",
      "1906                         std::min(\n",
      "1907                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1908                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1909                 k3<<<n_blocks, n_threads>>>(\n",
      "1910                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1911                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1912                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1913                         CudaNdarray_DEV_DATA(self),\n",
      "1914                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1915                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1916                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1917                         CudaNdarray_DEV_DATA(other),\n",
      "1918                         other_strides[0],\n",
      "1919                         other_strides[1],\n",
      "1920                         other_strides[2]);\n",
      "1921                 CNDA_THREAD_SYNC;\n",
      "1922                 cudaError_t err = cudaGetLastError();\n",
      "1923                 if (cudaSuccess != err)\n",
      "1924                 {\n",
      "1925                     PyErr_Format(\n",
      "1926                         PyExc_RuntimeError,\n",
      "1927                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1928                         \"k3\",\n",
      "1929                         cudaGetErrorString(err));\n",
      "1930                     Py_XDECREF(new_other);\n",
      "1931                     return -1;\n",
      "1932                 }\n",
      "1933             }\n",
      "1934             break;\n",
      "1935         case 4:\n",
      "1936             {\n",
      "1937                 dim3 n_blocks(\n",
      "1938                         std::min(\n",
      "1939                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1940                             NUM_VECTOR_OP_BLOCKS),\n",
      "1941                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1942                         );\n",
      "1943                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1944                     n_blocks.y /= 2;\n",
      "1945                 dim3 n_threads(\n",
      "1946                         std::min(\n",
      "1947                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1948                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1949                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1950                             );\n",
      "1951                 k4<<<n_blocks, n_threads>>>(\n",
      "1952                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1953                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1954                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1955                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1956                         CudaNdarray_DEV_DATA(self),\n",
      "1957                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1958                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1959                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1960                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1961                         CudaNdarray_DEV_DATA(other),\n",
      "1962                         other_strides[0],\n",
      "1963                         other_strides[1],\n",
      "1964                         other_strides[2],\n",
      "1965                         other_strides[3]);\n",
      "1966                 CNDA_THREAD_SYNC;\n",
      "1967                 cudaError_t err = cudaGetLastError();\n",
      "1968                 if (cudaSuccess != err)\n",
      "1969                 {\n",
      "1970                     PyErr_Format(\n",
      "1971                         PyExc_RuntimeError,\n",
      "1972                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1973                         \"k4\",\n",
      "1974                         cudaGetErrorString(err));\n",
      "1975                     Py_XDECREF(new_other);\n",
      "1976                     return -1;\n",
      "1977                 }\n",
      "1978             }\n",
      "1979             break;\n",
      "1980         case 5:\n",
      "1981             {\n",
      "1982                 dim3 n_blocks(\n",
      "1983                         std::min(\n",
      "1984                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1985                             NUM_VECTOR_OP_BLOCKS),\n",
      "1986                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1987                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1988                     n_blocks.y /= 2;\n",
      "1989                 dim3 n_threads(\n",
      "1990                         std::min(\n",
      "1991                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1992                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1993                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1994                     );\n",
      "1995                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1996                 {\n",
      "1997                      k4<<<n_blocks, n_threads>>>(\n",
      "1998                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1999                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "2000                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2001                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "2002                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2003                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2004                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2005                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2006                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2007                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "2008                             other_strides[1],\n",
      "2009                             other_strides[2],\n",
      "2010                             other_strides[3],\n",
      "2011                             other_strides[4]);\n",
      "2012                     CNDA_THREAD_SYNC;\n",
      "2013                     cudaError_t err = cudaGetLastError();\n",
      "2014                     if( cudaSuccess != err)\n",
      "2015                     {\n",
      "2016                         PyErr_Format(\n",
      "2017                             PyExc_RuntimeError,\n",
      "2018                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2019                             \"k5 with loop over k4\",\n",
      "2020                             cudaGetErrorString(err),\n",
      "2021                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2022                         Py_XDECREF(new_other);\n",
      "2023                         return -1;\n",
      "2024                     }\n",
      "2025                 }\n",
      "2026             }\n",
      "2027             break;\n",
      "2028         case 6:\n",
      "2029             {\n",
      "2030                 dim3 n_blocks(\n",
      "2031                         std::min(\n",
      "2032                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2033                             NUM_VECTOR_OP_BLOCKS),\n",
      "2034                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2035                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2036                         );\n",
      "2037                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2038                     n_blocks.y /= 2;\n",
      "2039                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2040                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2041                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2042                 //    n_blocks.z /= 2;\n",
      "2043                 n_blocks.z = 1;\n",
      "2044                 dim3 n_threads(\n",
      "2045                         std::min(\n",
      "2046                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2047                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2048                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2049                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2050                             );\n",
      "2051                 k6<<<n_blocks, n_threads>>>(\n",
      "2052                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2053                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2054                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2055                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2056                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2057                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2058                         CudaNdarray_DEV_DATA(self),\n",
      "2059                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2060                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2061                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2062                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2063                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2064                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2065                         CudaNdarray_DEV_DATA(other),\n",
      "2066                         other_strides[0],\n",
      "2067                         other_strides[1],\n",
      "2068                         other_strides[2],\n",
      "2069                         other_strides[3],\n",
      "2070                         other_strides[4],\n",
      "2071                         other_strides[5]);\n",
      "2072                 CNDA_THREAD_SYNC;\n",
      "2073                 cudaError_t err = cudaGetLastError();\n",
      "2074                 if (cudaSuccess != err)\n",
      "2075                 {\n",
      "2076                     PyErr_Format(\n",
      "2077                         PyExc_RuntimeError,\n",
      "2078                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2079                         \"k6\",\n",
      "2080                         cudaGetErrorString(err),\n",
      "2081                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2082                         (long) n_threads.x);\n",
      "2083                     Py_XDECREF(new_other);\n",
      "2084                     return -1;\n",
      "2085                 }\n",
      "2086             }\n",
      "2087             break;\n",
      "2088         default:\n",
      "2089         {\n",
      "2090             PyErr_Format(\n",
      "2091                 PyExc_NotImplementedError,\n",
      "2092                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2093                 self->nd);\n",
      "2094             Py_XDECREF(new_other);\n",
      "2095             return -1;\n",
      "2096         }\n",
      "2097     }\n",
      "2098     if (verbose)\n",
      "2099         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2100     Py_XDECREF(new_other);\n",
      "2101     return 0;\n",
      "2102 }\n",
      "2103 \n",
      "2104 /*\n",
      "2105  * We need this inplace Add to support IncSubTensor\n",
      "2106  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2107  */\n",
      "2108 // Will be called by __iadd__ in Python\n",
      "2109 PyObject *\n",
      "2110 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2111 {\n",
      "2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2113     {\n",
      "2114         return NULL;\n",
      "2115     }\n",
      "2116     Py_INCREF(py_self);\n",
      "2117     return py_self;\n",
      "2118 }\n",
      "2119 \n",
      "2120 /*\n",
      "2121  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2122  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2123  */\n",
      "2124 // Will be called by __idiv__ in Python\n",
      "2125 static PyObject *\n",
      "2126 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2127 {\n",
      "2128     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2129     {\n",
      "2130         return NULL;\n",
      "2131     }\n",
      "2132     Py_INCREF(py_self);\n",
      "2133     return py_self;\n",
      "2134 }\n",
      "2135 \n",
      "2136 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2137 #if PY_MAJOR_VERSION == 3\n",
      "2138 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2139 {\n",
      "2140     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2141     0,  //binaryfunc nb_subtract;\n",
      "2142     0,  //binaryfunc nb_multiply;\n",
      "2143     0,  //binaryfunc nb_remainder;\n",
      "2144     0,  //binaryfunc nb_divmod;\n",
      "2145     0,  //ternaryfunc nb_power;\n",
      "2146     0,  //unaryfunc nb_negative;\n",
      "2147     0,  //unaryfunc nb_positive;\n",
      "2148     0,  //unaryfunc nb_absolute;\n",
      "2149     0,  //inquiry nb_bool;\n",
      "2150     0,  //unaryfunc nb_invert;\n",
      "2151     0,  //binaryfunc nb_lshift;\n",
      "2152     0,  //binaryfunc nb_rshift;\n",
      "2153     0,  //binaryfunc nb_and;\n",
      "2154     0,  //binaryfunc nb_xor;\n",
      "2155     0,  //binaryfunc nb_or;\n",
      "2156     0,  //unaryfunc nb_int;\n",
      "2157     0,  //void *nb_reserved;\n",
      "2158     0,  //unaryfunc nb_float;\n",
      "2159 \n",
      "2160     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2161     0,  //binaryfunc nb_inplace_subtract;\n",
      "2162     0,  //binaryfunc nb_inplace_multiply;\n",
      "2163     0,  //binaryfunc nb_inplace_remainder;\n",
      "2164     0,  //ternaryfunc nb_inplace_power;\n",
      "2165     0,  //binaryfunc nb_inplace_lshift;\n",
      "2166     0,  //binaryfunc nb_inplace_rshift;\n",
      "2167     0,  //binaryfunc nb_inplace_and;\n",
      "2168     0,  //binaryfunc nb_inplace_xor;\n",
      "2169     0,  //binaryfunc nb_inplace_or;\n",
      "2170 \n",
      "2171     0,  //binaryfunc nb_floor_divide;\n",
      "2172     0,  //binaryfunc nb_true_divide;\n",
      "2173     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2174     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2175 \n",
      "2176     0,  //unaryfunc nb_index\n",
      "2177 };\n",
      "2178 #else\n",
      "2179 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2180 {\n",
      "2181     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2182     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2183     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2184     0,  //binaryfunc nb_divide;        __div__\n",
      "2185     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2186     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2187     0,  //ternaryfunc nb_power;        __pow__\n",
      "2188     0,  //unaryfunc nb_negative;       __neg__\n",
      "2189     0,  //unaryfunc nb_positive;       __pos__\n",
      "2190     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2191     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2192     0,  //unaryfunc nb_invert;         __invert__\n",
      "2193     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2194     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2195     0,  //binaryfunc nb_and;           __and__\n",
      "2196     0,  //binaryfunc nb_xor;           __xor__\n",
      "2197     0,  //binaryfunc nb_or;            __or__\n",
      "2198     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2199     0,  //unaryfunc nb_int;            __int__\n",
      "2200     0,  //unaryfunc nb_long;           __long__\n",
      "2201     0,  //unaryfunc nb_float;          __float__\n",
      "2202     0,  //unaryfunc nb_oct;            __oct__\n",
      "2203     0,  //unaryfunc nb_hex;            __hex__\n",
      "2204 \n",
      "2205     /* Added in release 2.0 */\n",
      "2206     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2207     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2208     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2209     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2210     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2211     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2212     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2213     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2214     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2215     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2216     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2217 \n",
      "2218     /* Added in release 2.2 */\n",
      "2219     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2220     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2221     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2222     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2223 \n",
      "2224 #if PY_MINOR_VERSION > 4\n",
      "2225     /* Added in release 2.5 */\n",
      "2226     0  //unaryfunc nb_index;  __index__\n",
      "2227 #endif\n",
      "2228 };\n",
      "2229 #endif\n",
      "2230 \n",
      "2231 \n",
      "2232 /////////////////////\n",
      "2233 // Mapping protocol\n",
      "2234 /////////////////////\n",
      "2235 \n",
      "2236 // Will by called by __len__ in Python\n",
      "2237 static Py_ssize_t\n",
      "2238 CudaNdarray_len(PyObject * py_self)\n",
      "2239 {\n",
      "2240     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2241     if (self->nd <= 0)\n",
      "2242     {\n",
      "2243         return (Py_ssize_t) 0;\n",
      "2244     }\n",
      "2245     else\n",
      "2246     {\n",
      "2247         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2248     }\n",
      "2249 }\n",
      "2250 \n",
      "2251 // Will by called by __getitem__ in Python\n",
      "2252 PyObject *\n",
      "2253 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2254 {\n",
      "2255     int verbose = 0;\n",
      "2256     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2257     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2258     PyObject * py_rval = NULL;\n",
      "2259     CudaNdarray * rval = NULL;\n",
      "2260     PyObject * intobj = NULL;\n",
      "2261 \n",
      "2262     //PyObject_Print(key, stderr, 0);\n",
      "2263 \n",
      "2264     if (key == Py_Ellipsis)\n",
      "2265     {\n",
      "2266         Py_INCREF(py_self);\n",
      "2267         return py_self;\n",
      "2268     }\n",
      "2269     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2270     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2271     {\n",
      "2272         int d_idx = PyInt_AsLong(intobj);\n",
      "2273         Py_DECREF(intobj); intobj=NULL;\n",
      "2274         //int d_idx = PyInt_AsLong(key);\n",
      "2275         if (self->nd == 0)\n",
      "2276         {\n",
      "2277             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2278             return NULL;\n",
      "2279         }\n",
      "2280         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2281         int offset = 0;\n",
      "2282 \n",
      "2283         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2284         {\n",
      "2285             //normal indexing\n",
      "2286             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2287         }\n",
      "2288         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2289         {\n",
      "2290             //end-based indexing\n",
      "2291             // d_idx is negative\n",
      "2292             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2293         }\n",
      "2294         else\n",
      "2295         {\n",
      "2296             PyErr_Format(PyExc_IndexError,\n",
      "2297                          \"index out of bounds. Asked %d, but size of %d\",\n",
      "2298                          d_idx, d_dim);\n",
      "2299             return NULL;\n",
      "2300         }\n",
      "2301 \n",
      "2302         //allocate our subtensor view\n",
      "2303         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2304         rval = (CudaNdarray*) py_rval;\n",
      "2305         if (!rval) return NULL;\n",
      "2306         assert (0 == rval->data_allocated);\n",
      "2307 \n",
      "2308         //initialize the view's data pointer to our own.\n",
      "2309         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2310         {\n",
      "2311             Py_DECREF(rval);\n",
      "2312             return NULL;\n",
      "2313         }\n",
      "2314         for (int d = 1; d < self->nd; ++d)\n",
      "2315         {\n",
      "2316             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2317             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2318         }\n",
      "2319     }\n",
      "2320     else\n",
      "2321     {\n",
      "2322         PyErr_Clear();\n",
      "2323     }\n",
      "2324     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2325     {\n",
      "2326         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2327         if (self->nd == 0)\n",
      "2328         {\n",
      "2329             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2330             return NULL;\n",
      "2331         }\n",
      "2332 \n",
      "2333         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2334         Py_ssize_t start, stop, step, slen;\n",
      "2335         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2336         {\n",
      "2337             if (verbose)\n",
      "2338                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2339             return NULL;\n",
      "2340         }\n",
      "2341         if (verbose)\n",
      "2342         {\n",
      "2343             std::cerr << \"start \" << start << \"\\n\";\n",
      "2344             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2345             std::cerr << \"step \" << step << \"\\n\";\n",
      "2346             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2347         }\n",
      "2348 \n",
      "2349         //allocate our subtensor view\n",
      "2350         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2351         rval = (CudaNdarray*) py_rval;\n",
      "2352         if (!rval) return NULL;\n",
      "2353         assert (0 == rval->data_allocated);\n",
      "2354 \n",
      "2355 \n",
      "2356         //initialize the view's data pointer to our own.\n",
      "2357         if (CudaNdarray_set_device_data(rval,\n",
      "2358                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2359                     self))\n",
      "2360         {\n",
      "2361             Py_DECREF(rval);\n",
      "2362             return NULL;\n",
      "2363         }\n",
      "2364         //initialize dimension 0 of rval\n",
      "2365         CudaNdarray_set_stride(rval, 0,\n",
      "2366                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2367         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2368         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2369         // initialize dimensions > 0 of rval\n",
      "2370         for (int d = 1; d < self->nd; ++d)\n",
      "2371         {\n",
      "2372             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2373             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2374         }\n",
      "2375     }\n",
      "2376     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2377     {\n",
      "2378         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2379         //elements of the tuple can be either integers or slices\n",
      "2380         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2381 \n",
      "2382         if (PyTuple_Size(key) > self->nd)\n",
      "2383         {\n",
      "2384             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2385             return NULL;\n",
      "2386         }\n",
      "2387 \n",
      "2388         //calculate the number of dimensions in the return value\n",
      "2389         int rval_nd = self->nd;\n",
      "2390         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2391         {\n",
      "2392             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2393             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2394             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2395         }\n",
      "2396 \n",
      "2397         //allocate our subtensor view\n",
      "2398         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2399         rval = (CudaNdarray*) py_rval;\n",
      "2400         if (!rval) return NULL;\n",
      "2401         assert (0 == rval->data_allocated);\n",
      "2402 \n",
      "2403         //initialize the view's data pointer to our own.\n",
      "2404         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2405         {\n",
      "2406             Py_DECREF(rval);\n",
      "2407             return NULL;\n",
      "2408         }\n",
      "2409 \n",
      "2410         // rval_d will refer to the current dimension in the rval.\n",
      "2411         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2412         // keys\n",
      "2413         int rval_d = 0;\n",
      "2414 \n",
      "2415         for (int d = 0; d < self->nd; ++d)\n",
      "2416         {\n",
      "2417             // keys can be shorter than self->nd.\n",
      "2418             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2419             if (d >=PyTuple_Size(key))\n",
      "2420             {\n",
      "2421                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2422                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2423                 ++rval_d;\n",
      "2424             }\n",
      "2425             else\n",
      "2426             {\n",
      "2427                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2428 \n",
      "2429                 if (PySlice_Check(key_d))\n",
      "2430                 {\n",
      "2431                     Py_ssize_t start, stop, step, slen;\n",
      "2432                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2433                     {\n",
      "2434                         Py_DECREF(rval);\n",
      "2435                         return NULL;\n",
      "2436                     }\n",
      "2437                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2438                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2439                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2440                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2441                     if (0)\n",
      "2442                     {\n",
      "2443                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2444                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2445                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2446                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2447                     }\n",
      "2448                     ++rval_d;\n",
      "2449                 }\n",
      "2450                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2451                 {\n",
      "2452                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2453                     int d_idx = PyInt_AsLong(intobj);\n",
      "2454                     Py_DECREF(intobj);\n",
      "2455                     intobj = NULL;\n",
      "2456                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2457 \n",
      "2458                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2459                     {\n",
      "2460                         //normal indexing\n",
      "2461                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2462                     }\n",
      "2463                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2464                     {\n",
      "2465                         //end-based indexing\n",
      "2466                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2467                     }\n",
      "2468                     else\n",
      "2469                     {\n",
      "2470                         PyErr_Format(PyExc_IndexError,\n",
      "2471                                      \"index out of bounds. Asked %d for dimensions %d, but size of %d\",\n",
      "2472                                      d_idx, d, d_dim);\n",
      "2473                         Py_DECREF(rval);\n",
      "2474                         return NULL;\n",
      "2475                     }\n",
      "2476                 }\n",
      "2477                 else\n",
      "2478                 {\n",
      "2479                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2480                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2481                     Py_DECREF(rval);\n",
      "2482                     return NULL;\n",
      "2483                 }\n",
      "2484             }\n",
      "2485         }\n",
      "2486     }\n",
      "2487     if (py_rval)\n",
      "2488     {\n",
      "2489         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2490         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2491     }\n",
      "2492     else\n",
      "2493     {\n",
      "2494         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2495         return NULL;\n",
      "2496     }\n",
      "2497     return py_rval;\n",
      "2498 }\n",
      "2499 \n",
      "2500 // Will by called by __setitem__ in Python\n",
      "2501 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2502 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2503 // Can only be assigned from a CudaNdarray on the right side\n",
      "2504 // Or a ndarray\n",
      "2505 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2506 static int\n",
      "2507 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2508 {\n",
      "2509     int verbose = 0;\n",
      "2510     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2511     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2512     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2513     CudaNdarray* new_value = NULL;\n",
      "2514 \n",
      "2515     if(!rval){\n",
      "2516         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2517         Py_XDECREF(rval);\n",
      "2518         return -1;\n",
      "2519     }\n",
      "2520 \n",
      "2521     if(rval != (CudaNdarray*)o &&\n",
      "2522                 (rval->data_allocated ||\n",
      "2523                  // The new array should have a base\n",
      "2524                  !(((CudaNdarray*)rval)->base) ||\n",
      "2525                  // If the original array has no base, the base of the new\n",
      "2526                  // array should be the original one\n",
      "2527                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2528                  // Else, the two arrays should have the same base\n",
      "2529                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2530     {\n",
      "2531         // This case shouldn't happen, based on what I see in Subscript\n",
      "2532         // but just in case it happens sometime in the future\n",
      "2533 \n",
      "2534         PyErr_Format(PyExc_RuntimeError,\n",
      "2535                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2536                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2537                      \" o.base=%p o=%p\",\n",
      "2538                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2539         Py_DECREF(rval);\n",
      "2540         return -1;\n",
      "2541     }\n",
      "2542 \n",
      "2543     PyObject * intobj = NULL;\n",
      "2544     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2545         if (verbose)\n",
      "2546             fprintf(stderr,\n",
      "2547                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2548                     \" value is a ndarray\\n\");\n",
      "2549         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2550         if(!new_value)\n",
      "2551         {\n",
      "2552             return -1;\n",
      "2553         }\n",
      "2554         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2555         {\n",
      "2556             Py_XDECREF(new_value);\n",
      "2557             Py_XDECREF(rval);\n",
      "2558             return -1;\n",
      "2559         }\n",
      "2560         value = (PyObject *) new_value;\n",
      "2561     }\n",
      "2562     else if ((intobj=PyNumber_Int(value)))\n",
      "2563     {\n",
      "2564         if (verbose)\n",
      "2565             fprintf(stderr,\n",
      "2566                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2567         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2568             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2569                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2570                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2571             Py_XDECREF(rval);\n",
      "2572             return -1;\n",
      "2573         }\n",
      "2574 \n",
      "2575         long val = PyInt_AsLong(intobj);\n",
      "2576         Py_DECREF(intobj); intobj=NULL;\n",
      "2577         if (val == 0)\n",
      "2578         {\n",
      "2579             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2580                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2581             Py_XDECREF(rval);\n",
      "2582             if (err)\n",
      "2583             {\n",
      "2584                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2585                 // Currently this returns the same thing as err, but if in future\n",
      "2586                 // it returns something else I still don't see why we should ignore\n",
      "2587                 // it.  All we want to do here is reset the flag.\n",
      "2588                 cudaGetLastError();\n",
      "2589                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2590                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2591                 return -1;\n",
      "2592             }\n",
      "2593             return 0;\n",
      "2594         } else {\n",
      "2595             Py_XDECREF(rval);\n",
      "2596             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2597                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2598                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2599                 return -1;\n",
      "2600         }\n",
      "2601     }\n",
      "2602 \n",
      "2603     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2604 \n",
      "2605     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2606     {\n",
      "2607         PyErr_SetString(PyExc_TypeError,\n",
      "2608           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2609           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2610         Py_XDECREF(new_value);\n",
      "2611         return -1;\n",
      "2612     }\n",
      "2613 \n",
      "2614     if (verbose)\n",
      "2615         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2616 \n",
      "2617     if (cnda_copy_structure_to_device(rval))\n",
      "2618     {\n",
      "2619         PyErr_SetString(PyExc_RuntimeError,\n",
      "2620                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2621         Py_DECREF(rval);\n",
      "2622         Py_XDECREF(new_value);\n",
      "2623 \n",
      "2624         if (verbose)\n",
      "2625             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2626         return -1;\n",
      "2627     }\n",
      "2628 \n",
      "2629     PyObject *baseSavedForComparison = rval->base;\n",
      "2630 \n",
      "2631     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2632     {\n",
      "2633         Py_DECREF((PyObject*)rval);\n",
      "2634         Py_XDECREF(new_value);\n",
      "2635 \n",
      "2636         if (verbose)\n",
      "2637             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2638         return -1;\n",
      "2639     }\n",
      "2640 \n",
      "2641     assert (rval->base == baseSavedForComparison);\n",
      "2642     assert (rval->dev_structure_fresh);\n",
      "2643 \n",
      "2644     // Clean up locally-created references\n",
      "2645     Py_DECREF(rval);\n",
      "2646     Py_XDECREF(new_value);\n",
      "2647 \n",
      "2648     return 0;\n",
      "2649 }\n",
      "2650 \n",
      "2651 \n",
      "2652 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2653     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2654     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2655     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2656 };\n",
      "2657 \n",
      "2658 ////////////////////\n",
      "2659 //\n",
      "2660 ////////////////////\n",
      "2661 \n",
      "2662 static PyObject *\n",
      "2663 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2664 {\n",
      "2665     if (self->nd < 0)\n",
      "2666     {\n",
      "2667         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2668         return NULL;\n",
      "2669     }\n",
      "2670     PyObject * rval = PyTuple_New(self->nd);\n",
      "2671     for (int i = 0; i < self->nd; ++i)\n",
      "2672     {\n",
      "2673         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2674         {\n",
      "2675             Py_XDECREF(rval);\n",
      "2676             return NULL;\n",
      "2677         }\n",
      "2678 \n",
      "2679     }\n",
      "2680     return rval;\n",
      "2681 }\n",
      "2682 \n",
      "2683 static int\n",
      "2684 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2685 {\n",
      "2686     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2687     return -1;\n",
      "2688 }\n",
      "2689 \n",
      "2690 static PyObject *\n",
      "2691 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2692 {\n",
      "2693     if (self->nd < 0)\n",
      "2694     {\n",
      "2695         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2696         return NULL;\n",
      "2697     }\n",
      "2698     PyObject * rval = PyTuple_New(self->nd);\n",
      "2699     for (int i = 0; i < self->nd; ++i)\n",
      "2700     {\n",
      "2701         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2702         {\n",
      "2703             Py_XDECREF(rval);\n",
      "2704             return NULL;\n",
      "2705         }\n",
      "2706 \n",
      "2707     }\n",
      "2708     return rval;\n",
      "2709 }\n",
      "2710 \n",
      "2711 static int\n",
      "2712 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2713 {\n",
      "2714     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2715     if (PyTuple_Check(value)){\n",
      "2716         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2717             PyErr_SetString(PyExc_ValueError,\n",
      "2718                             \"The new strides tuple must have the same length\"\n",
      "2719                             \" as the number of dimensions\");\n",
      "2720             return -1;\n",
      "2721         }\n",
      "2722     }else if (PyList_Check(value)){\n",
      "2723         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2724             PyErr_SetString(PyExc_ValueError,\n",
      "2725                             \"The new strides list must have the same length\"\n",
      "2726                             \" as the number of dimensions\");\n",
      "2727             return -1;\n",
      "2728         }\n",
      "2729     }else{\n",
      "2730         PyErr_SetString(PyExc_ValueError,\n",
      "2731                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2732         return -1;\n",
      "2733     }\n",
      "2734     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2735     if (PyTuple_Check(value)){\n",
      "2736         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2737             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2738             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2739         }\n",
      "2740     }else if (PyList_Check(value)){\n",
      "2741         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2742             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2743             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2744         }\n",
      "2745     }\n",
      "2746     /*\n",
      "2747     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2748     // to do it.\n",
      "2749     npy_intp dims[PyTuple_Size(value)];\n",
      "2750     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2751         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2752     }\n",
      "2753     if (!PyArray_CheckStrides(4,\n",
      "2754                               CudaNdarray_NDIM(self),\n",
      "2755                               0, 0,\n",
      "2756                               dims,\n",
      "2757                               newstrides_bytes)){\n",
      "2758         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2759         return -1;\n",
      "2760         }\n",
      "2761     */\n",
      "2762     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2763         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2764     }\n",
      "2765     return 0;\n",
      "2766 }\n",
      "2767 \n",
      "2768 static PyObject *\n",
      "2769 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2770 {\n",
      "2771     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2772     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2773     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2774 }\n",
      "2775 \n",
      "2776 static int\n",
      "2777 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2778 {\n",
      "2779     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2780     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2781     if (PyErr_Occurred())\n",
      "2782     {\n",
      "2783         return -1;\n",
      "2784     }\n",
      "2785     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2786 }\n",
      "2787 \n",
      "2788 static PyObject *\n",
      "2789 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2790 {\n",
      "2791     return PyString_FromString(\"float32\");\n",
      "2792 }\n",
      "2793 \n",
      "2794 static PyObject *\n",
      "2795 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2796 {\n",
      "2797     return PyInt_FromLong(self->nd);\n",
      "2798 }\n",
      "2799 \n",
      "2800 static PyObject *\n",
      "2801 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2802 {\n",
      "2803     PyObject * base = self->base;\n",
      "2804     if (!base)\n",
      "2805     {\n",
      "2806         // We cannot return a NULL pointer, use None instead\n",
      "2807         base = Py_None;\n",
      "2808     }\n",
      "2809     Py_INCREF(base);\n",
      "2810     return base;\n",
      "2811 }\n",
      "2812 \n",
      "2813 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2814 {\n",
      "2815   PyObject * k = PyString_FromString(key);\n",
      "2816   PyObject * v = PyInt_FromLong(val);\n",
      "2817   PyDict_SetItem(dict, k, v);\n",
      "2818   Py_DECREF(k);\n",
      "2819   Py_DECREF(v);\n",
      "2820 }\n",
      "2821 \n",
      "2822 PyObject *\n",
      "2823 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2824 {\n",
      "2825   int dev_id = -1;\n",
      "2826   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2827     return NULL;\n",
      "2828   cudaDeviceProp deviceProp;\n",
      "2829   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2830 \n",
      "2831   PyObject * dict = PyDict_New();\n",
      "2832   PyObject * str= PyString_FromString(\"name\");\n",
      "2833   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2834   PyDict_SetItem(dict, str, i);\n",
      "2835   Py_DECREF(str);\n",
      "2836   Py_DECREF(i);\n",
      "2837 \n",
      "2838   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2839   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2840 #if CUDART_VERSION >= 2020\n",
      "2841   int driverVersion = 0, runtimeVersion = 0;\n",
      "2842   cudaDriverGetVersion(&driverVersion);\n",
      "2843   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2844   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2845   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2846 #endif\n",
      "2847 #if CUDART_VERSION >= 2000\n",
      "2848 \n",
      "2849   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2850   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2851   int sm_cores = -1;\n",
      "2852   if(deviceProp.major==1)\n",
      "2853     sm_cores = 32;\n",
      "2854   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2855     sm_cores = 32;\n",
      "2856   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2857     sm_cores = 48;\n",
      "2858   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2859 #endif\n",
      "2860   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2861   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2862   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2863   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2864   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2865   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2866   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2867   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2868   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2869   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2870   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2871   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2872   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2873   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2874 #if CUDART_VERSION >= 2000\n",
      "2875   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2876 #endif\n",
      "2877 #if CUDART_VERSION >= 2020\n",
      "2878   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2879   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2880   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2881   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2882   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2883 #endif\n",
      "2884 #if CUDART_VERSION >= 3000\n",
      "2885   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2886 #endif\n",
      "2887 #if CUDART_VERSION >= 3010\n",
      "2888   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2889 #endif\n",
      "2890 #if CUDART_VERSION >= 3020\n",
      "2891   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2892 #endif\n",
      "2893 \n",
      "2894   return dict;\n",
      "2895 }\n",
      "2896 \n",
      "2897 /*\n",
      "2898  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2899  */\n",
      "2900 PyObject *\n",
      "2901 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2902 {\n",
      "2903     size_t free = 0, total = 0;\n",
      "2904     if(g_gpu_context_active == 0){\n",
      "2905         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2906         return NULL;\n",
      "2907     }\n",
      "2908 \n",
      "2909     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2910     if (err != cudaSuccess){\n",
      "2911         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2912         // Currently this returns the same thing as err, but if in future\n",
      "2913         // it returns something else I still don't see why we should ignore\n",
      "2914         // it.  All we want to do here is reset the flag.\n",
      "2915         cudaGetLastError();\n",
      "2916         PyErr_Format(PyExc_RuntimeError,\n",
      "2917                      \"Error while getting memory info about the gpu: %s\",\n",
      "2918                      cudaGetErrorString(err));\n",
      "2919         return NULL;\n",
      "2920     }\n",
      "2921     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));\n",
      "2922 }\n",
      "2923 \n",
      "2924 /*\n",
      "2925  * Synchronize with all the gpu device stream.\n",
      "2926  */\n",
      "2927 PyObject *\n",
      "2928 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2929 {\n",
      "2930     CNDA_BEGIN_ALLOW_THREADS\n",
      "2931     cudaThreadSynchronize();\n",
      "2932     CNDA_END_ALLOW_THREADS\n",
      "2933     Py_INCREF(Py_None);\n",
      "2934     return Py_None;\n",
      "2935 }\n",
      "2936 \n",
      "2937 /*\n",
      "2938  * Exist and return true if we link with cublas v2.\n",
      "2939  */\n",
      "2940 PyObject *\n",
      "2941 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2942 {\n",
      "2943     Py_INCREF(Py_True);\n",
      "2944     return Py_True;\n",
      "2945 }\n",
      "2946 \n",
      "2947 PyObject *\n",
      "2948 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)\n",
      "2949 {\n",
      "2950     void * rval = NULL;\n",
      "2951     cudaError_t err;\n",
      "2952     int num_gpus = 0;\n",
      "2953 \n",
      "2954     err = cudaGetDeviceCount(&num_gpus);\n",
      "2955     if (cudaSuccess != err){\n",
      "2956         printf(\"ERR!\\\\n\");\n",
      "2957             PyErr_Format(PyExc_RuntimeError,\n",
      "2958                          \"Not able to get number of GPUs (%s).\",\n",
      "2959                          cudaGetErrorString(err));\n",
      "2960             return NULL;\n",
      "2961     }\n",
      "2962 \n",
      "2963     for (int device = 0; device < num_gpus; device++) {\n",
      "2964         cudaSetDevice(device);\n",
      "2965         err = cudaDeviceSynchronize(); // << CUDA context gets created here.\n",
      "2966         cudaGetLastError(); // reset the error state     \n",
      "2967         if (cudaSuccess == err)\n",
      "2968             break;\n",
      "2969     }\n",
      "2970         \n",
      "2971     if (cudaSuccess != err){\n",
      "2972             printf(\"ERR!\\\\n\");\n",
      "2973                 PyErr_Format(PyExc_RuntimeError,\n",
      "2974                              \"Not able to select available GPU from %d cards (%s).\",\n",
      "2975                              num_gpus, cudaGetErrorString(err));\n",
      "2976                 return NULL;\n",
      "2977     }\n",
      "2978 \n",
      "2979     Py_INCREF(Py_None);\n",
      "2980     return Py_None;\n",
      "2981 }\n",
      "2982 \n",
      "2983 #if COMPUTE_GPU_MEM_USED\n",
      "2984 /*\n",
      "2985  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2986  */\n",
      "2987 PyObject *\n",
      "2988 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2989 {\n",
      "2990     PyObject* a = PyLong_FromLong(_allocated_size);\n",
      "2991     PyObject* b = PyLong_FromLong(_max_allocated_size);\n",
      "2992 \n",
      "2993     PyObject* tuple = PyTuple_New(2);\n",
      "2994     PyTuple_SetItem(tuple, 0, a);\n",
      "2995     PyTuple_SetItem(tuple, 1, b);\n",
      "2996     return tuple;\n",
      "2997 }\n",
      "2998 #endif\n",
      "2999 \n",
      "3000 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "3001     {\"shape\",\n",
      "3002         (getter)CudaNdarray_get_shape,\n",
      "3003         (setter)CudaNdarray_set_shape,\n",
      "3004         \"shape of this ndarray (tuple)\",\n",
      "3005         NULL},\n",
      "3006     {\"_strides\",\n",
      "3007         (getter)CudaNdarray_get_strides,\n",
      "3008         (setter)CudaNdarray_set_strides,\n",
      "3009         \"data pointer strides (in elements)\",\n",
      "3010         NULL},\n",
      "3011     {\"strides\",\n",
      "3012         (getter)CudaNdarray_get_strides,\n",
      "3013         (setter)CudaNdarray_set_strides,\n",
      "3014         \"data pointer strides (in elements)\",\n",
      "3015         NULL},\n",
      "3016     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "3017     {\"gpudata\",\n",
      "3018         (getter)CudaNdarray_get_dev_data,\n",
      "3019         NULL,\n",
      "3020         \"device data pointer\",\n",
      "3021         NULL},\n",
      "3022     {\"_dev_data\",\n",
      "3023         (getter)CudaNdarray_get_dev_data,\n",
      "3024         (setter)CudaNdarray_set_dev_data,\n",
      "3025         \"device data pointer\",\n",
      "3026         NULL},\n",
      "3027     {\"dtype\",\n",
      "3028         (getter)CudaNdarray_get_dtype,\n",
      "3029         NULL,\n",
      "3030         \"The dtype of the element. Now always float32\",\n",
      "3031         NULL},\n",
      "3032     {\"size\",\n",
      "3033         (getter)CudaNdarray_SIZE_Object,\n",
      "3034         NULL,\n",
      "3035         \"The number of elements in this object.\",\n",
      "3036         NULL},\n",
      "3037     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "3038     {\"mem_size\",\n",
      "3039         (getter)CudaNdarray_SIZE_Object,\n",
      "3040         NULL,\n",
      "3041         \"The number of elements in this object.\",\n",
      "3042         NULL},\n",
      "3043     {\"ndim\",\n",
      "3044         (getter)CudaNdarray_get_ndim,\n",
      "3045         NULL,\n",
      "3046         \"The number of dimensions in this object.\",\n",
      "3047         NULL},\n",
      "3048     {\"base\",\n",
      "3049         (getter)CudaNdarray_get_base,\n",
      "3050         NULL,\n",
      "3051         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "3052         NULL},\n",
      "3053 \n",
      "3054     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3055 };\n",
      "3056 \n",
      "3057 PyObject *CudaNdarray_repr(PyObject *self)\n",
      "3058 {\n",
      "3059     CudaNdarray *object = (CudaNdarray *)self;\n",
      "3060     PyObject * np_object = CudaNdarray_CreateArrayObj(object);\n",
      "3061     PyObject * str = PyObject_Str((PyObject *) np_object);\n",
      "3062     char * cstr = PyString_AsString(str);\n",
      "3063     PyObject * out = PyString_FromFormat(\"%s%s%s\",\n",
      "3064                         \"CudaNdarray(\",\n",
      "3065                         cstr,\n",
      "3066                         \")\");\n",
      "3067     Py_DECREF(str);\n",
      "3068     Py_DECREF(np_object);\n",
      "3069     #if PY_MAJOR_VERSION >= 3\n",
      "3070     // In Python 3 PyString_FromFormat return a Bytes object\n",
      "3071     PyObject* out2 = PyObject_Str(out);\n",
      "3072     Py_DECREF(out);\n",
      "3073     return out2;\n",
      "3074     #endif\n",
      "3075     return out;\n",
      "3076 }\n",
      "3077 \n",
      "3078 static PyTypeObject CudaNdarrayType =\n",
      "3079 {\n",
      "3080 #if PY_MAJOR_VERSION >= 3\n",
      "3081     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3082 #else\n",
      "3083     PyObject_HEAD_INIT(NULL)\n",
      "3084     0,                         /*ob_size*/\n",
      "3085 #endif\n",
      "3086     \"CudaNdarray\",             /*tp_name*/\n",
      "3087     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3088     0,                         /*tp_itemsize*/\n",
      "3089     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3090     0,                         /*tp_print*/\n",
      "3091     0,                         /*tp_getattr*/\n",
      "3092     0,                         /*tp_setattr*/\n",
      "3093     0,                         /*tp_compare*/\n",
      "3094     CudaNdarray_repr,          /*tp_repr*/\n",
      "3095     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3096     0,                         /*tp_as_sequence*/\n",
      "3097     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3098     0,                         /*tp_hash */\n",
      "3099     0,                         /*tp_call*/\n",
      "3100     0,                         /*tp_str*/\n",
      "3101     0,                         /*tp_getattro*/\n",
      "3102     0,                         /*tp_setattro*/\n",
      "3103     0,                         /*tp_as_buffer*/\n",
      "3104 #if PY_MAJOR_VERSION >= 3\n",
      "3105     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3106     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3107 #else\n",
      "3108     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3109 #endif\n",
      "3110     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3111     0,                         /* tp_traverse */\n",
      "3112     0,                         /* tp_clear */\n",
      "3113     0,                         /* tp_richcompare */\n",
      "3114     0,                         /* tp_weaklistoffset */\n",
      "3115     0,                         /* tp_iter */\n",
      "3116     0,                         /* tp_iternext */\n",
      "3117     CudaNdarray_methods,       /* tp_methods */\n",
      "3118     CudaNdarray_members,       /* tp_members */\n",
      "3119     CudaNdarray_getset,        /* tp_getset */\n",
      "3120     0,                         /* tp_base */\n",
      "3121     0,                         /* tp_dict */\n",
      "3122     0,                         /* tp_descr_get */\n",
      "3123     0,                         /* tp_descr_set */\n",
      "3124     0,                         /* tp_dictoffset */\n",
      "3125     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3126     0,                         /* tp_alloc */\n",
      "3127     CudaNdarray_new,           /* tp_new */\n",
      "3128 };\n",
      "3129 \n",
      "3130 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3131 {\n",
      "3132     dst[0] = sizeof(float*);\n",
      "3133     dst[1] = sizeof(int);\n",
      "3134 }\n",
      "3135 \n",
      "3136 PyObject *\n",
      "3137 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3138 {\n",
      "3139     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3140     if(gpu_data == NULL){\n",
      "3141         return NULL;\n",
      "3142     }\n",
      "3143     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3144 \n",
      "3145     cudaError_t cudaErr = cudaGetLastError();\n",
      "3146     if (cudaSuccess != cudaErr){\n",
      "3147 \n",
      "3148         device_free(gpu_data);\n",
      "3149         return PyErr_Format(PyExc_RuntimeError,\n",
      "3150                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3151                             cudaGetErrorString(cudaErr));\n",
      "3152     }\n",
      "3153 \n",
      "3154     // Transfer the result to cpu\n",
      "3155     int gpu_sizes[] = {-1,-1};\n",
      "3156     cublasStatus_t err;\n",
      "3157     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3158     device_free(gpu_data);\n",
      "3159 \n",
      "3160     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3161         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3162         return NULL;\n",
      "3163     }\n",
      "3164     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3165                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3166 }\n",
      "3167 \n",
      "3168 static int cublas_init();\n",
      "3169 static void cublas_shutdown();\n",
      "3170 // Initialize the gpu.\n",
      "3171 // Takes two optional parameters, the device number and if we should use cnmem.\n",
      "3172 // If the device number is provided, it sets that device to be the active device.\n",
      "3173 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3174 // it does not set an active device.\n",
      "3175 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3176 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.\n",
      "3177 PyObject *\n",
      "3178 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3179 {\n",
      "3180     int card_nb = 0;\n",
      "3181     int card_number_provided = 1;\n",
      "3182     float cnmem = 0; // Theano flag lib.cnmem\n",
      "3183     // if we're given something wildly invalid, this will throw a TypeError\n",
      "3184     if(!PyArg_ParseTuple(args, \"|if\", &card_nb, &cnmem))\n",
      "3185         return NULL;\n",
      "3186     if(cnmem)\n",
      "3187         g_use_cnmem = true;\n",
      "3188 \n",
      "3189     if(PyTuple_Size(args) == 0) {\n",
      "3190         card_number_provided = 0;\n",
      "3191         card_nb = 0;\n",
      "3192     }\n",
      "3193 \n",
      "3194     int deviceCount;\n",
      "3195     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3196     if(cudaSuccess != err) {\n",
      "3197         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3198                             \"Unable to get the number of gpus available: %s\",\n",
      "3199                             cudaGetErrorString(cudaGetLastError()));\n",
      "3200     }\n",
      "3201 \n",
      "3202     // as soon as the first successful call to a cuda* function is made, a\n",
      "3203     // gpu context has been created\n",
      "3204     g_gpu_context_active = 1;\n",
      "3205 \n",
      "3206     if(deviceCount <= 0) {\n",
      "3207         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3208                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3209     }\n",
      "3210     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3211         return PyErr_Format(PyExc_ValueError,\n",
      "3212                             \"Bad device number %d. Only %d devices available.\",\n",
      "3213                             card_nb,\n",
      "3214                             deviceCount);\n",
      "3215     }\n",
      "3216 \n",
      "3217     cudaDeviceProp deviceProp;\n",
      "3218     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3219     if(cudaSuccess != err) {\n",
      "3220         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3221                             \"Unable to get properties of gpu %i: %s\",\n",
      "3222                             card_nb,\n",
      "3223                             cudaGetErrorString(cudaGetLastError()));\n",
      "3224     }\n",
      "3225 \n",
      "3226     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3227         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3228                             \"There is no device that supports CUDA\");\n",
      "3229     }\n",
      "3230 \n",
      "3231     if(card_number_provided) {\n",
      "3232         err = cudaSetDevice(card_nb);\n",
      "3233         if(cudaSuccess != err) {\n",
      "3234             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3235                                 \"Unable to set device %i: %s\",\n",
      "3236                                 card_nb,\n",
      "3237                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3238         }\n",
      "3239         if (cublas_init() == -1)\n",
      "3240             return NULL;\n",
      "3241     }\n",
      "3242     if(card_number_provided && g_use_cnmem) {\n",
      "3243         size_t mem = 0;\n",
      "3244         if (cnmem > 1)\n",
      "3245             mem = cnmem * 1024 * 1024;\n",
      "3246         else{\n",
      "3247             // Clip to 95% to let memory for the driver.\n",
      "3248             // 98% didn't worked in some cases.\n",
      "3249             if (cnmem > .95){\n",
      "3250                 cnmem = .95;\n",
      "3251             }\n",
      "3252             size_t free = 0, total = 0;\n",
      "3253             cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "3254             if (err != cudaSuccess){\n",
      "3255                 // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "3256                 // Currently this returns the same thing as err, but if in future\n",
      "3257                 // it returns something else I still don't see why we should ignore\n",
      "3258                 // it.  All we want to do here is reset the flag.\n",
      "3259                 cudaGetLastError();\n",
      "3260                 PyErr_Format(PyExc_RuntimeError,\n",
      "3261                              \"Error while getting memory info about the gpu: %s\",\n",
      "3262                              cudaGetErrorString(err));\n",
      "3263                 return NULL;\n",
      "3264             }\n",
      "3265             mem = total * cnmem;\n",
      "3266         }\n",
      "3267         if(initCnmem(card_number_provided, card_nb, mem) == -1){\n",
      "3268             return NULL;\n",
      "3269         }\n",
      "3270     }\n",
      "3271 \n",
      "3272     Py_INCREF(Py_None);\n",
      "3273     return Py_None;\n",
      "3274 }\n",
      "3275 \n",
      "3276 PyObject *\n",
      "3277 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3278     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3279     // really necessary.\n",
      "3280     int currentDevice;\n",
      "3281     cudaGetDevice(&currentDevice);\n",
      "3282     return PyInt_FromLong(currentDevice);\n",
      "3283 }\n",
      "3284 \n",
      "3285 PyObject *\n",
      "3286 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3287     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3288     // really necessary.\n",
      "3289     int currentDevice;\n",
      "3290     cudaGetDevice(&currentDevice);\n",
      "3291 \n",
      "3292     cudaDeviceProp deviceProp;\n",
      "3293     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3294     return PyString_FromString(deviceProp.name);\n",
      "3295 }\n",
      "3296 \n",
      "3297 PyObject *\n",
      "3298 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3299     // Don't handle errors here\n",
      "3300     cublas_shutdown();\n",
      "3301     g_gpu_context_active = 0; // context has now been closed down\n",
      "3302     if(g_use_cnmem) {\n",
      "3303         cnmemStatus_t status = cnmemFinalize();\n",
      "3304         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "3305             fprintf(stderr, \"CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\\n\",\n",
      "3306                     cnmemGetErrorString(status));\n",
      "3307             if(status == CNMEM_STATUS_CUDA_ERROR) {\n",
      "3308                 fprintf(stderr, \"  Cuda-Reason=%s\\n\",\n",
      "3309                         cudaGetErrorString(cudaGetLastError()));\n",
      "3310             }\n",
      "3311         }\n",
      "3312     }\n",
      "3313     cudaThreadExit();\n",
      "3314 \n",
      "3315     Py_INCREF(Py_None);\n",
      "3316     return Py_None;\n",
      "3317 }\n",
      "3318 \n",
      "3319 /*\n",
      "3320  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3321  */\n",
      "3322 PyObject *\n",
      "3323 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3324 {\n",
      "3325     int verbose = 0;\n",
      "3326     PyObject *gpu_ptr = NULL;\n",
      "3327     PyObject *shapes = NULL;\n",
      "3328     PyObject *strides = NULL;\n",
      "3329     PyObject *base = NULL;\n",
      "3330     PyObject *rval = NULL;\n",
      "3331 \n",
      "3332     //args should consist of 3 python objects\n",
      "3333     //The first is the gpu ptr\n",
      "3334     //The second if the shape\n",
      "3335     //The third if the strides\n",
      "3336     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3337         return NULL;\n",
      "3338 \n",
      "3339     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3340     if (!PyLong_Check(gpu_ptr))\n",
      "3341     {\n",
      "3342         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3343         return NULL;\n",
      "3344     }\n",
      "3345 \n",
      "3346     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3347     if (nd < 0)\n",
      "3348     {\n",
      "3349         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3350         return NULL;\n",
      "3351     }\n",
      "3352     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3353     if (nd_stride < 0)\n",
      "3354     {\n",
      "3355         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3356         return NULL;\n",
      "3357     }\n",
      "3358 \n",
      "3359     if (nd != nd_stride)\n",
      "3360     {\n",
      "3361         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3362         return NULL;\n",
      "3363     }\n",
      "3364 \n",
      "3365     rval = CudaNdarray_New();\n",
      "3366 \n",
      "3367     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3368     {\n",
      "3369         //CudaNdarray_set_nd set the error msg\n",
      "3370         return NULL;\n",
      "3371     }\n",
      "3372     // set gpu pointeur\n",
      "3373     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3374     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3375     {\n",
      "3376         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3377         return NULL;\n",
      "3378 \n",
      "3379     }\n",
      "3380 \n",
      "3381     // Set dims and strides\n",
      "3382     for (int i = nd-1; i >= 0; --i)\n",
      "3383     {\n",
      "3384         PyObject * idx = PyLong_FromLong(i);\n",
      "3385         if (idx == NULL)\n",
      "3386         {\n",
      "3387             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3388             return NULL;\n",
      "3389         }\n",
      "3390         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3391         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3392         if (!PyInt_Check(dim_))\n",
      "3393         {\n",
      "3394             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3395             return NULL;\n",
      "3396         }\n",
      "3397         if (!PyInt_Check(strd_))\n",
      "3398         {\n",
      "3399             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3400             return NULL;\n",
      "3401         }\n",
      "3402         int dim = PyInt_AsLong(dim_);\n",
      "3403         int strd = PyInt_AsLong(strd_);\n",
      "3404         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3405         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3406         Py_DECREF(idx);\n",
      "3407         Py_DECREF(dim_);\n",
      "3408         Py_DECREF(strd_);\n",
      "3409     }\n",
      "3410     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3411     return rval;\n",
      "3412 }\n",
      "3413 \n",
      "3414 PyObject *\n",
      "3415 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3416 {\n",
      "3417     PyObject *l=NULL;\n",
      "3418     PyObject *r=NULL;\n",
      "3419     PyObject * rval = NULL;\n",
      "3420 \n",
      "3421     //args should consist of two python objects (\"OO\")\n",
      "3422     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3423         return NULL;\n",
      "3424 \n",
      "3425     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3426     {\n",
      "3427         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
      "3428         goto CudaNdarray_dot_fail;\n",
      "3429     }\n",
      "3430     if (((CudaNdarray*)l)->nd != 2)\n",
      "3431     {\n",
      "3432         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3433         goto CudaNdarray_dot_fail;\n",
      "3434     }\n",
      "3435     if (((CudaNdarray*)r)->nd != 2)\n",
      "3436     {\n",
      "3437         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3438         goto CudaNdarray_dot_fail;\n",
      "3439     }\n",
      "3440     rval = CudaNdarray_New();\n",
      "3441     if (!rval)\n",
      "3442     {\n",
      "3443         goto CudaNdarray_dot_fail;\n",
      "3444     }\n",
      "3445     int dims[2];\n",
      "3446     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3447     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3448     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3449     {\n",
      "3450         goto CudaNdarray_dot_fail;\n",
      "3451     }\n",
      "3452     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3453     {\n",
      "3454         goto CudaNdarray_dot_fail;\n",
      "3455     }\n",
      "3456 \n",
      "3457     return rval;\n",
      "3458 \n",
      "3459     CudaNdarray_dot_fail:\n",
      "3460     Py_XDECREF(rval);\n",
      "3461     return NULL;\n",
      "3462 }\n",
      "3463 \n",
      "3464 static PyObject *\n",
      "3465 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3466 {\n",
      "3467     /*\n",
      "3468      * TODO: DOC what this function should do in the various cases of\n",
      "3469      * What is 'strict' supposed to mean in the context of this function?\n",
      "3470      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3471      *\n",
      "3472      */\n",
      "3473     PyObject *py_data=NULL;\n",
      "3474     PyArrayObject * data = NULL;\n",
      "3475     int strict = 0;\n",
      "3476     PyObject * broadcastable=NULL;\n",
      "3477     PyObject * storage=NULL;\n",
      "3478     CudaNdarray * rval=NULL;\n",
      "3479 \n",
      "3480     //Python object references which are provided to the caller are borrowed references\n",
      "3481     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3482 \n",
      "3483     if (!PyTuple_Check(broadcastable)){\n",
      "3484         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3485         return NULL;\n",
      "3486     }\n",
      "3487     Py_INCREF(py_data);\n",
      "3488     Py_INCREF(broadcastable);\n",
      "3489 \n",
      "3490     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3491 \n",
      "3492     if (strict || CudaNdarray_Check(py_data))\n",
      "3493     {\n",
      "3494         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3495         if (!CudaNdarray_Check(py_data))\n",
      "3496         {\n",
      "3497             Py_DECREF(py_data);\n",
      "3498             Py_DECREF(broadcastable);\n",
      "3499             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3500             return NULL;\n",
      "3501         }\n",
      "3502         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3503         {\n",
      "3504             Py_DECREF(py_data);\n",
      "3505             Py_DECREF(broadcastable);\n",
      "3506             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3507             return NULL;\n",
      "3508         }\n",
      "3509         for (int i = 0; i < cnda->nd; ++i)\n",
      "3510         {\n",
      "3511             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3512             {\n",
      "3513                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3514                 Py_DECREF(py_data);\n",
      "3515                 Py_DECREF(broadcastable);\n",
      "3516                 return NULL;\n",
      "3517             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3518                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3519                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3520                 Py_DECREF(py_data);\n",
      "3521                 Py_DECREF(broadcastable);\n",
      "3522                 return NULL;\n",
      "3523             }\n",
      "3524         }\n",
      "3525         Py_DECREF(broadcastable);\n",
      "3526         return py_data;\n",
      "3527     }\n",
      "3528     else\n",
      "3529     {\n",
      "3530         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3531         if (!data)\n",
      "3532         {\n",
      "3533             //err message already defined\n",
      "3534             Py_DECREF(py_data);\n",
      "3535             Py_DECREF(broadcastable);\n",
      "3536             return NULL;\n",
      "3537         }\n",
      "3538         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3539         {\n",
      "3540             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3541             {\n",
      "3542                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3543                 Py_DECREF(data);\n",
      "3544                 Py_DECREF(py_data);\n",
      "3545                 Py_DECREF(broadcastable);\n",
      "3546                 return NULL;\n",
      "3547             }\n",
      "3548         }\n",
      "3549         if (storage && CudaNdarray_Check(storage))\n",
      "3550         {\n",
      "3551             rval = (CudaNdarray*) storage;\n",
      "3552             Py_INCREF(rval);\n",
      "3553         }\n",
      "3554         else\n",
      "3555         {\n",
      "3556             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3557         }\n",
      "3558         if (rval)\n",
      "3559         {\n",
      "3560             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3561             {\n",
      "3562                 Py_DECREF(rval);\n",
      "3563                 rval = NULL;\n",
      "3564             }\n",
      "3565         }\n",
      "3566         Py_DECREF(data);\n",
      "3567         Py_DECREF(py_data);\n",
      "3568         Py_DECREF(broadcastable);\n",
      "3569         return (PyObject*)rval;\n",
      "3570     }\n",
      "3571 }\n",
      "3572 \n",
      "3573 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3574 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3575 static PyMethodDef module_methods[] = {\n",
      "3576     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3577     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3578     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3579     {\"select_a_gpu\", CudaNdarray_select_a_gpu, METH_NOARGS, \"Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU.\"},\n",
      "3580     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3581     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3582     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3583     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3584     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3585 #if COMPUTE_GPU_MEM_USED\n",
      "3586     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3587 #endif\n",
      "3588     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3589     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3590     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3591     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3592     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3593     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3594      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3595     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3596 };\n",
      "3597 \n",
      "3598 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3599 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3600 \n",
      "3601 #if PY_MAJOR_VERSION == 3\n",
      "3602 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3603 {\n",
      "3604     PyModuleDef_HEAD_INIT,\n",
      "3605     CNDA_MOD_NAME,\n",
      "3606     CNDA_DOCSTRING,\n",
      "3607     -1,     /* size of per-interpreter state of the module,\n",
      "3608                or -1 if the module keeps state in global variables. */\n",
      "3609     module_methods\n",
      "3610 };\n",
      "3611 \n",
      "3612 PyMODINIT_FUNC\n",
      "3613 PyInit_cuda_ndarray(void)\n",
      "3614 #else\n",
      "3615 PyMODINIT_FUNC\n",
      "3616 initcuda_ndarray(void)\n",
      "3617 #endif\n",
      "3618 {\n",
      "3619     import_array();\n",
      "3620 \n",
      "3621     PyObject* m;\n",
      "3622 \n",
      "3623     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3624 #if PY_MAJOR_VERSION == 3\n",
      "3625         return NULL;\n",
      "3626 #else\n",
      "3627         return;\n",
      "3628 #endif\n",
      "3629     }\n",
      "3630 \n",
      "3631 #if PY_MAJOR_VERSION == 3\n",
      "3632     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3633 #else\n",
      "3634     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3635 #endif\n",
      "3636 \n",
      "3637     if (m == NULL) {\n",
      "3638 #if PY_MAJOR_VERSION == 3\n",
      "3639         return NULL;\n",
      "3640 #else\n",
      "3641         return;\n",
      "3642 #endif\n",
      "3643     }\n",
      "3644 \n",
      "3645     Py_INCREF(&CudaNdarrayType);\n",
      "3646     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3647 #if COMPUTE_GPU_MEM_USED\n",
      "3648     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3649         _alloc_size_table[i].ptr=NULL;\n",
      "3650         _alloc_size_table[i].size=0;\n",
      "3651     }\n",
      "3652 #endif\n",
      "3653     //    cublasInit();\n",
      "3654     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3655     //{\n",
      "3656         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3657     //}\n",
      "3658     if (0) //TODO: is this necessary?\n",
      "3659     {\n",
      "3660         int deviceId = 0; // TODO: what number goes here?\n",
      "3661         cudaSetDevice(deviceId);\n",
      "3662         cudaError_t err = cudaGetLastError();\n",
      "3663         if( cudaSuccess != err)\n",
      "3664         {\n",
      "3665             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3666         }\n",
      "3667     }\n",
      "3668 \n",
      "3669 #if PY_MAJOR_VERSION == 3\n",
      "3670     return m;\n",
      "3671 #endif\n",
      "3672 }\n",
      "3673 \n",
      "3674 \n",
      "3675 //////////////////////////////////////\n",
      "3676 //\n",
      "3677 // C API FOR CudaNdarray\n",
      "3678 //\n",
      "3679 //////////////////////////////////////\n",
      "3680 \n",
      "3681 int\n",
      "3682 CudaNdarray_Check(const PyObject * ob)\n",
      "3683 {\n",
      "3684     //TODO: doesn't work with inheritance\n",
      "3685     return CudaNdarray_CheckExact(ob);\n",
      "3686 }\n",
      "3687 int\n",
      "3688 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3689 {\n",
      "3690     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3691 }\n",
      "3692 \n",
      "3693 PyObject *\n",
      "3694 CudaNdarray_New(int nd)\n",
      "3695 {\n",
      "3696     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3697     if (self == NULL)\n",
      "3698     {\n",
      "3699         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3700         return NULL;\n",
      "3701     }\n",
      "3702     CudaNdarray_null_init(self);\n",
      "3703 \n",
      "3704     if (nd == 0)\n",
      "3705     {\n",
      "3706         self->nd = 0;\n",
      "3707     }\n",
      "3708     else if (nd > 0)\n",
      "3709     {\n",
      "3710         if (CudaNdarray_set_nd(self, nd))\n",
      "3711         {\n",
      "3712             Py_DECREF(self);\n",
      "3713             return NULL;\n",
      "3714         }\n",
      "3715     }\n",
      "3716     ++_outstanding_mallocs[1];\n",
      "3717     return (PyObject *)self;\n",
      "3718 }\n",
      "3719 \n",
      "3720 \n",
      "3721 \n",
      "3722 //////////////////////////////\n",
      "3723 //\n",
      "3724 // Published helper functions\n",
      "3725 //\n",
      "3726 //////////////////////////////\n",
      "3727 \n",
      "3728 static int\n",
      "3729 cublas_init()\n",
      "3730 {\n",
      "3731     cublasStatus_t err;\n",
      "3732     err = cublasCreate(&handle);\n",
      "3733     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3734     {\n",
      "3735         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3736             PyErr_SetString(PyExc_RuntimeError,\n",
      "3737                             \"cublasCreate() returned this error \"\n",
      "3738                             \"'the CUDA Runtime initialization failed'\");\n",
      "3739         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3740             PyErr_SetString(PyExc_RuntimeError,\n",
      "3741                             \"cublasCreate() returned this error \"\n",
      "3742                             \"'the resources could not be allocated'\");\n",
      "3743         else\n",
      "3744             PyErr_SetString(PyExc_RuntimeError,\n",
      "3745                             \"unknow error during returned by cublasCreate()\");\n",
      "3746         return -1;\n",
      "3747     }\n",
      "3748     // Set the default stream as the one to execute on (default)\n",
      "3749     cublasSetStream(handle, NULL);\n",
      "3750     // Pointer to scalars are on the host (also default)\n",
      "3751     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3752 #if CUDA_VERSION >= 5000\n",
      "3753     // atomics can be used in kernels to speed up operations (not default)\n",
      "3754     // This may lead to a slight variance from run to run in some operations\n",
      "3755     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3756 #endif\n",
      "3757     return 0;\n",
      "3758 }\n",
      "3759 \n",
      "3760 static void\n",
      "3761 cublas_shutdown()\n",
      "3762 {\n",
      "3763     if (handle != NULL)\n",
      "3764         cublasDestroy(handle);\n",
      "3765     // No point in handling any errors here\n",
      "3766     handle = NULL;\n",
      "3767 }\n",
      "3768 \n",
      "3769 int\n",
      "3770 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3771 {\n",
      "3772     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3773                                            PyArray_DIMS(obj));\n",
      "3774     if (err) {\n",
      "3775         return err;\n",
      "3776     }\n",
      "3777 \n",
      "3778     int typenum = PyArray_TYPE(obj);\n",
      "3779     if (typenum != REAL_TYPENUM)\n",
      "3780     {\n",
      "3781         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3782         return -1;\n",
      "3783     }\n",
      "3784     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3785     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3786         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3787     if (!py_src) {\n",
      "3788         return -1;\n",
      "3789     }\n",
      "3790     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3791     void *py_src_data = PyArray_DATA(py_src);\n",
      "3792     cudaError_t cerr;\n",
      "3793     CNDA_BEGIN_ALLOW_THREADS;\n",
      "3794     cerr = cudaMemcpy(self->devdata, py_src_data,\n",
      "3795                       py_src_size * sizeof(real),\n",
      "3796                       cudaMemcpyHostToDevice);\n",
      "3797     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "3798     CNDA_END_ALLOW_THREADS;\n",
      "3799     if (cudaSuccess != cerr)\n",
      "3800     {\n",
      "3801         PyErr_Format(PyExc_RuntimeError,\n",
      "3802                      \"Cuda error '%s' while copying %lli data element\"\n",
      "3803                      \" to device memory. str ptr=%p. dst ptr=%p\",\n",
      "3804                      cudaGetErrorString(cerr),\n",
      "3805                      (long long)py_src_size,\n",
      "3806                      py_src_data,\n",
      "3807                      self->devdata);\n",
      "3808         Py_DECREF(py_src);\n",
      "3809         return -1;\n",
      "3810     }\n",
      "3811     Py_DECREF(py_src);\n",
      "3812     return 0;\n",
      "3813 }\n",
      "3814 \n",
      "3815 PyObject *\n",
      "3816 CudaNdarray_new_nd(int nd)\n",
      "3817 {\n",
      "3818     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3819     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3820     {\n",
      "3821         Py_XDECREF(rval);\n",
      "3822         rval = NULL;\n",
      "3823     }\n",
      "3824     return (PyObject *) rval;\n",
      "3825 }\n",
      "3826 \n",
      "3827 \n",
      "3828 /**\n",
      "3829  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3830  */\n",
      "3831 \n",
      "3832 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3833 {\n",
      "3834     if (self->data_allocated)\n",
      "3835     {\n",
      "3836         assert(self->devdata);\n",
      "3837         if (device_free(self->devdata))\n",
      "3838         {\n",
      "3839             self->devdata = NULL;\n",
      "3840             self->data_allocated = 0;\n",
      "3841             return -1;\n",
      "3842         }\n",
      "3843     }\n",
      "3844     // Get the original base object (base.base.base...)\n",
      "3845     PyObject * orig_base = base;\n",
      "3846     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3847     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3848     {\n",
      "3849         // base_base is itself a view\n",
      "3850         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3851     }\n",
      "3852     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3853     if (self->base != orig_base)\n",
      "3854     {\n",
      "3855         Py_XDECREF(self->base);\n",
      "3856         self->base = orig_base;\n",
      "3857         Py_XINCREF(self->base);\n",
      "3858     }\n",
      "3859     self->data_allocated = 0;\n",
      "3860     self->devdata = data;\n",
      "3861     return 0;\n",
      "3862 }\n",
      "3863 \n",
      "3864 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3865 {\n",
      "3866     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3867     {\n",
      "3868         y[i*sy] = x[i*sx];\n",
      "3869     }\n",
      "3870 }\n",
      "3871 \n",
      "3872 // N1 through N4 are the size of y\n",
      "3873 static __global__ void k_copy_4d(const int N1,\n",
      "3874         const int N2, const int N3, const int N4,\n",
      "3875         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3876         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3877         const int sy3, const int sy4)\n",
      "3878 {\n",
      "3879     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3880     int bx = blockIdx.x;\n",
      "3881     int by = blockIdx.y;\n",
      "3882 \n",
      "3883     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3884     {\n",
      "3885         for (int j = by; j < N2; j += gridDim.y)\n",
      "3886         {\n",
      "3887             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3888             {\n",
      "3889                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3890                 {\n",
      "3891                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3892                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3893                 }\n",
      "3894             }\n",
      "3895         }\n",
      "3896     }\n",
      "3897 }\n",
      "3898 \n",
      "3899 //copy from other into self\n",
      "3900 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3901                                     const CudaNdarray * other,\n",
      "3902                                     bool unbroadcast)\n",
      "3903 {\n",
      "3904     int verbose = 0;\n",
      "3905     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3906 \n",
      "3907     //standard elemwise size checks\n",
      "3908     if (self->nd == -1)\n",
      "3909     {\n",
      "3910         PyErr_SetString(PyExc_TypeError,\n",
      "3911                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3912         return -1;\n",
      "3913     }\n",
      "3914     CudaNdarray * new_other = NULL;\n",
      "3915 \n",
      "3916     if (self->nd < other->nd)\n",
      "3917     {\n",
      "3918         PyErr_Format(PyExc_NotImplementedError,\n",
      "3919             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3920             \"destination needs to be >= the number of dimensions of the \"\n",
      "3921             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3922         return -1;\n",
      "3923     }\n",
      "3924     else if (self->nd != other->nd)\n",
      "3925     {\n",
      "3926         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3927         int added_dims = self->nd - other->nd;\n",
      "3928         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3929         for(int i = 0; i < added_dims; i++)\n",
      "3930             pattern[i] = -1;\n",
      "3931         for(int i = 0; i < other->nd; i++)\n",
      "3932             pattern[i + added_dims] = i;\n",
      "3933         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3934         other = new_other;\n",
      "3935     }\n",
      "3936     assert(self->nd == other->nd);\n",
      "3937     //standard elemwise dim checks (also compute total size)\n",
      "3938     unsigned int size = 1;\n",
      "3939     unsigned int size_source = 1;\n",
      "3940     for (int i = 0; i< self->nd; ++i)\n",
      "3941     {\n",
      "3942         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3943             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3944         {\n",
      "3945           PyErr_Format(PyExc_ValueError,\n",
      "3946                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3947                        \" need same dimensions for dim %d,\"\n",
      "3948                        \" destination=%d, source=%d\",\n",
      "3949                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3950                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3951           Py_XDECREF(new_other);\n",
      "3952           return -1;\n",
      "3953         }\n",
      "3954         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3955         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3956     }\n",
      "3957     if (0 == size)\n",
      "3958     {\n",
      "3959         Py_XDECREF(new_other);\n",
      "3960         return 0; //nothing to copy, we're done.\n",
      "3961     }\n",
      "3962     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3963         CudaNdarray_is_c_contiguous(other) &&\n",
      "3964         size == size_source)\n",
      "3965     {\n",
      "3966         if (verbose)\n",
      "3967             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3968 \n",
      "3969         cublasStatus_t err;\n",
      "3970         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3971                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3972         CNDA_THREAD_SYNC;\n",
      "3973         Py_XDECREF(new_other);\n",
      "3974         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3975         {\n",
      "3976             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3977             return -1;\n",
      "3978         }\n",
      "3979         return 0;\n",
      "3980     }\n",
      "3981     //TODO: rewrite these copy operations to be more efficient\n",
      "3982     //      See, for example the transpose example in the cuda_sdk.\n",
      "3983     switch (self->nd)\n",
      "3984     {\n",
      "3985         case 0: // scalar\n",
      "3986             {\n",
      "3987                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3988                 assert(0);\n",
      "3989             }; break;\n",
      "3990         case 1: // vector\n",
      "3991             {\n",
      "3992                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3993                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3994                 unsigned int n_blocks = std::min(size,\n",
      "3995                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3996                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3997                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3998                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3999                                             CudaNdarray_DEV_DATA(other),\n",
      "4000                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4001                                             CudaNdarray_DEV_DATA(self),\n",
      "4002                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "4003                 CNDA_THREAD_SYNC;\n",
      "4004                 cudaError_t err = cudaGetLastError();\n",
      "4005                 if( cudaSuccess != err)\n",
      "4006                 {\n",
      "4007                     PyErr_Format(PyExc_RuntimeError,\n",
      "4008                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4009                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "4010                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "4011                     Py_XDECREF(new_other);\n",
      "4012                     return -1;\n",
      "4013                 }\n",
      "4014             }; break;\n",
      "4015         case 4: // 4-tensor\n",
      "4016             {\n",
      "4017                 if (verbose)\n",
      "4018                 {\n",
      "4019                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "4020                     {\n",
      "4021                         Py_XDECREF(new_other);\n",
      "4022                         return -1;\n",
      "4023                     }\n",
      "4024                 }\n",
      "4025 \n",
      "4026                 // The blocks implement the looping over the first two axes so\n",
      "4027                 // this needs to be (N1, N2)\n",
      "4028                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "4029                                         NUM_VECTOR_OP_BLOCKS),\n",
      "4030                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "4031                                         NUM_VECTOR_OP_BLOCKS));\n",
      "4032                 // For the threads, just make as many as possible\n",
      "4033                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "4034                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "4035                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "4036                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "4037 \n",
      "4038                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "4039                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "4040 \n",
      "4041                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "4042                                             // size of y\n",
      "4043                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "4044                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "4045                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "4046                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "4047                                             CudaNdarray_DEV_DATA(other), // x\n",
      "4048                                             // x strides\n",
      "4049                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4050                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "4051                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "4052                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "4053                                             CudaNdarray_DEV_DATA(self), // y\n",
      "4054                                             // y strides\n",
      "4055                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "4056                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "4057                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "4058                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "4059                                             );\n",
      "4060                 CNDA_THREAD_SYNC;\n",
      "4061                 cudaError_t err = cudaGetLastError();\n",
      "4062                 if( cudaSuccess != err)\n",
      "4063                 {\n",
      "4064                     PyErr_Format(PyExc_RuntimeError,\n",
      "4065                                  \"Cuda error: %s: %s.\",\n",
      "4066                                  \"k_copy_4d\",\n",
      "4067                                  cudaGetErrorString(err));\n",
      "4068                     Py_XDECREF(new_other);\n",
      "4069                     return -1;\n",
      "4070                 }\n",
      "4071             }; break;\n",
      "4072         default:\n",
      "4073             {\n",
      "4074                 cudaError_t err = cudaGetLastError();\n",
      "4075                 if(cudaSuccess != err){\n",
      "4076                     PyErr_Format(PyExc_RuntimeError,\n",
      "4077                                  \"Unexpected Cuda error: %s: %s\\n\",\n",
      "4078                                  \"CudaNdarray_CopyFromCudaNdarray\",\n",
      "4079                                  cudaGetErrorString(err));\n",
      "4080                     Py_XDECREF(new_other);\n",
      "4081                     return -1;\n",
      "4082                 }\n",
      "4083 \n",
      "4084                 if (verbose)\n",
      "4085                     fprintf(stderr,\n",
      "4086                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "4087                             unbroadcast);\n",
      "4088                 // call worker routine\n",
      "4089                 unsigned int threads_per_block = std::min(size,\n",
      "4090                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4091                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n",
      "4092                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "4093                 const CudaNdarray * cuda_dims = other;\n",
      "4094                 if(unbroadcast)\n",
      "4095                     cuda_dims = self;\n",
      "4096                 //copy from other into self\n",
      "4097                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "4098                         size,\n",
      "4099                         (unsigned int)other->nd,\n",
      "4100                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "4101                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "4102                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "4103                         CudaNdarray_DEV_DATA(self),\n",
      "4104                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "4105                 CNDA_THREAD_SYNC;\n",
      "4106                 err = cudaGetLastError();\n",
      "4107                 if(verbose>1)\n",
      "4108                     fprintf(stderr,\n",
      "4109                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "4110                             \" n_threads_per_block=%i)\\n\",\n",
      "4111                             n_blocks, threads_per_block);\n",
      "4112                 if( cudaSuccess != err)\n",
      "4113                 {\n",
      "4114                     //fprint_CudaNdarray(stderr, self);\n",
      "4115                     //fprint_CudaNdarray(stderr, other);\n",
      "4116                     PyErr_Format(PyExc_RuntimeError,\n",
      "4117                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4118                                  \" n_threads_per_block=%i)\\n\",\n",
      "4119                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "4120                                  cudaGetErrorString(err), n_blocks,\n",
      "4121                                  threads_per_block);\n",
      "4122                     Py_XDECREF(new_other);\n",
      "4123                     return -1;\n",
      "4124                 }\n",
      "4125             }\n",
      "4126     };\n",
      "4127     Py_XDECREF(new_other);\n",
      "4128     return 0;\n",
      "4129 }\n",
      "4130 \n",
      "4131 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4132 {\n",
      "4133     if (A->nd != 2)\n",
      "4134     {\n",
      "4135         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4136         return -1;\n",
      "4137     }\n",
      "4138     if (B->nd != 2)\n",
      "4139     {\n",
      "4140         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4141         return -1;\n",
      "4142     }\n",
      "4143     if (C->nd != 2)\n",
      "4144     {\n",
      "4145         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4146         return -1;\n",
      "4147     }\n",
      "4148 \n",
      "4149     // We must allow dimensions to be zeros.\n",
      "4150     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4151             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4152             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4153     {\n",
      "4154         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4155                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4156                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4157                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4158                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4159                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4160                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4161         return -1;\n",
      "4162     }\n",
      "4163 \n",
      "4164     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4165     // dimensions, we can make a copy.\n",
      "4166     CudaNdarray * A_new = NULL;\n",
      "4167     CudaNdarray * B_new = NULL;\n",
      "4168     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4169          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4170          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4171          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4172         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4173         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4174     {\n",
      "4175         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4176         if (!A_new)\n",
      "4177             return -1;\n",
      "4178         A = A_new;\n",
      "4179     }\n",
      "4180 \n",
      "4181     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4182          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4183          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4184          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4185         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4186         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4187     {\n",
      "4188         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4189         if (!B_new)\n",
      "4190         {\n",
      "4191             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4192             Py_XDECREF(A_new);\n",
      "4193             return -1;\n",
      "4194         }\n",
      "4195         B = B_new;\n",
      "4196     }\n",
      "4197 \n",
      "4198     // If matrix C has non-unit size and non-unit stride in both\n",
      "4199     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4200     // C either, because the calling code will expect the result to be\n",
      "4201     // in the original C container.\n",
      "4202     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4203          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4204          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4205          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4206         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4207         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4208     {\n",
      "4209         PyErr_Format(PyExc_AssertionError,\n",
      "4210                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4211                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4212                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4213                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4214                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4215         Py_XDECREF(A_new);\n",
      "4216         Py_XDECREF(B_new);\n",
      "4217         return -1;\n",
      "4218     }\n",
      "4219 \n",
      "4220     // the unit integer is divided logically into three fields of 4 bits\n",
      "4221     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4222     // the next higher 4 bits encode the B variable (or y)\n",
      "4223     // the next higher 4 bits encode the C variable (or x)\n",
      "4224     //\n",
      "4225     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4226     //                                                 1 for unit stride from row to row (Col major)\n",
      "4227 \n",
      "4228     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4229     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4230     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4231     // consider it a 'unit' stride too.\n",
      "4232     int unit = 0;\n",
      "4233     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4234         unit |= (0x0 << 8);\n",
      "4235     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4236         unit |= (0x1 << 8);\n",
      "4237     } else {\n",
      "4238         unit |= (0x2 << 8);\n",
      "4239     }\n",
      "4240     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4241         unit |= (0x0 << 4);\n",
      "4242     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4243         unit |= (0x1 << 4);\n",
      "4244     } else {\n",
      "4245         unit |= (0x2 << 4);\n",
      "4246     }\n",
      "4247     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4248         unit |= (0x0 << 0);\n",
      "4249     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4250         unit |= (0x1 << 0);\n",
      "4251     } else {\n",
      "4252         unit |= (0x2 << 0);\n",
      "4253     }\n",
      "4254 \n",
      "4255     /* create appropriate strides for malformed matrices that are row or column\n",
      "4256      * vectors\n",
      "4257      */\n",
      "4258     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4259     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4260     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4261     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4262     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4263     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4264 \n",
      "4265     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4266     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4267     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4268     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4269     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4270     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4271     // There should be no negative stride at that point\n",
      "4272 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4273     if (sx == 0){sx = 1;}\\\n",
      "4274     if (sy == 0){sy = 1;}\\\n",
      "4275     if (sz == 0){sz = 1;}\\\n",
      "4276     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4277         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4278     } else { \\\n",
      "4279         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4280         Py_XDECREF(A_new);\\\n",
      "4281         Py_XDECREF(B_new);\\\n",
      "4282         return -1; \\\n",
      "4283     }\n",
      "4284 \n",
      "4285     cublasStatus_t err;\n",
      "4286     switch(unit)\n",
      "4287     {\n",
      "4288         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4289         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4290         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4291         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4292         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4293         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4294         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4295         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4296         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4297                  return -1;\n",
      "4298     };\n",
      "4299     CNDA_THREAD_SYNC;\n",
      "4300     Py_XDECREF(A_new);\n",
      "4301     Py_XDECREF(B_new);\n",
      "4302 \n",
      "4303     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4304     {\n",
      "4305         PyErr_Format(PyExc_RuntimeError,\n",
      "4306                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4307                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4308                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4309                      err,  cublasGetErrorString(err),\n",
      "4310                      unit, N,\n",
      "4311                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4312                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4313                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4314                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4315 \n",
      "4316         return -1;\n",
      "4317     }\n",
      "4318     return 0;\n",
      "4319 }\n",
      "4320 \n",
      "4321 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4322 {\n",
      "4323     /**\n",
      "4324     * C <- alpha A B + beta C\n",
      "4325     *    A : matrix\n",
      "4326     *    B, C: vector\n",
      "4327     *    alpha, beta: scalars\n",
      "4328     */\n",
      "4329     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4330     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4331     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4332 \n",
      "4333     // We must allow dimensions to be zeros.\n",
      "4334     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4335             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4336     {\n",
      "4337         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4338                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4339                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4340                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4341                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4342         return -1;\n",
      "4343     }\n",
      "4344 \n",
      "4345     // If matrix A has non-unit size and non-unit stride in both\n",
      "4346     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4347     // make a copy.\n",
      "4348     CudaNdarray * A_new = NULL;\n",
      "4349     CudaNdarray * B_new = NULL;\n",
      "4350     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4351          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4352          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4353          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4354         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4355         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4356     {\n",
      "4357         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4358         if (!A_new)\n",
      "4359             return -1;\n",
      "4360         A = A_new;\n",
      "4361     }\n",
      "4362 \n",
      "4363     // If vector B as a negative stride, we also have to make a copy.\n",
      "4364     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4365     {\n",
      "4366         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4367         if (!B_new)\n",
      "4368         {\n",
      "4369             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4370             // do anything\n",
      "4371             Py_XDECREF(A_new);\n",
      "4372             return -1;\n",
      "4373         }\n",
      "4374         B = B_new;\n",
      "4375     }\n",
      "4376 \n",
      "4377     // cudablas does not handle negative strides as expected\n",
      "4378     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4379         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4380     {\n",
      "4381         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4382                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4383                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4384         Py_XDECREF(A_new);\n",
      "4385         Py_XDECREF(B_new);\n",
      "4386         return -1;\n",
      "4387     }\n",
      "4388 \n",
      "4389     /* create appropriate strides for malformed matrices that are row or column\n",
      "4390      * vectors\n",
      "4391      */\n",
      "4392     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4393     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4394     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4395     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4396 \n",
      "4397     if (sa_0 == 0)\n",
      "4398         sa_0 = 1;\n",
      "4399     if (sa_1 == 0)\n",
      "4400         sa_1 = 1;\n",
      "4401 \n",
      "4402     // This is important because we can end up not calling Sgemv at all\n",
      "4403     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4404     if (CudaNdarray_SIZE(C)) {\n",
      "4405         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4406             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4407                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4408         {\n",
      "4409             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4410                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4411                     &alpha,\n",
      "4412                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4413                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4414                     &beta,\n",
      "4415                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4416         }\n",
      "4417         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4418                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4419                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4420         {\n",
      "4421             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4422                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4423                     &alpha,\n",
      "4424                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4425                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4426                     &beta,\n",
      "4427                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4428         }\n",
      "4429         else\n",
      "4430         {\n",
      "4431             PyErr_Format(PyExc_AssertionError,\n",
      "4432                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4433                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4434                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4435                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4436                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4437                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4438                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4439                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4440                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4441                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4442             Py_XDECREF(A_new);\n",
      "4443             Py_XDECREF(B_new);\n",
      "4444             return -1;\n",
      "4445         }\n",
      "4446     }\n",
      "4447 \n",
      "4448     CNDA_THREAD_SYNC;\n",
      "4449     Py_XDECREF(A_new);\n",
      "4450     Py_XDECREF(B_new);\n",
      "4451 \n",
      "4452     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4453     {\n",
      "4454         PyErr_Format(PyExc_RuntimeError,\n",
      "4455                      \"cublasSgemv failed (%i)\",\n",
      "4456                      err);\n",
      "4457         return -1;\n",
      "4458     }\n",
      "4459     return 0;\n",
      "4460 }\n",
      "4461 \n",
      "4462 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4463     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4464     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4465     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4466 \n",
      "4467     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4468         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4469         PyErr_Format(PyExc_ValueError,\n",
      "4470                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4471                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4472                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4473                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4474                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4475         return -1;\n",
      "4476     }\n",
      "4477 \n",
      "4478     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4479     CudaNdarray * x_new = NULL;\n",
      "4480     if(x_strides == 0){\n",
      "4481         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4482             PyErr_Format(PyExc_RuntimeError,\n",
      "4483                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4484                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4485                          \" that has more than 1 element!\");\n",
      "4486             return -1;\n",
      "4487         }\n",
      "4488         x_strides = 1;\n",
      "4489     } else if(x_strides < 0){\n",
      "4490         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4491         x = x_new;\n",
      "4492         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4493     }\n",
      "4494 \n",
      "4495     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4496     CudaNdarray * y_new = NULL;\n",
      "4497     if(y_strides == 0){\n",
      "4498         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4499             PyErr_Format(PyExc_RuntimeError,\n",
      "4500                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4501                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4502                          \" that has more than 1 elements!\");\n",
      "4503             Py_XDECREF(x_new);\n",
      "4504             return -1;\n",
      "4505         }\n",
      "4506         y_strides = 1;\n",
      "4507     } else if(y_strides < 0){\n",
      "4508         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4509         y = y_new;\n",
      "4510         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4511     }\n",
      "4512 \n",
      "4513     // Create appropriate strides if A is a row or column vector\n",
      "4514     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4515                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4516     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4517                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4518 \n",
      "4519     // This is important because we can end up not calling Sger at all\n",
      "4520     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4521     if(CudaNdarray_SIZE(A)){\n",
      "4522         // If A is in col-major\n",
      "4523         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4524             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4525                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4526         {\n",
      "4527             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4528                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4529                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4530                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4531         }\n",
      "4532         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4533         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4534                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4535                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4536         {\n",
      "4537             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4538                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4539                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4540                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4541         }\n",
      "4542         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4543         else\n",
      "4544         {\n",
      "4545             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4546                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4547             Py_XDECREF(x_new);\n",
      "4548             Py_XDECREF(y_new);\n",
      "4549             return -1;\n",
      "4550         }\n",
      "4551     }\n",
      "4552     CNDA_THREAD_SYNC;\n",
      "4553     Py_XDECREF(x_new);\n",
      "4554     Py_XDECREF(y_new);\n",
      "4555 \n",
      "4556     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4557     {\n",
      "4558         PyErr_Format(PyExc_RuntimeError,\n",
      "4559                      \"cublasSger failed (%i)\",\n",
      "4560                      err);\n",
      "4561         return -1;\n",
      "4562     }\n",
      "4563 \n",
      "4564     return 0;\n",
      "4565 }\n",
      "4566 \n",
      "4567 /**\n",
      "4568  *\n",
      "4569  * Precondition:\n",
      "4570  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4571  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4572  *\n",
      "4573  *  TODO: templatize this function to support other reductions.\n",
      "4574  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4575  */\n",
      "4576 \n",
      "4577 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4578         const unsigned int nd,\n",
      "4579         const int * dims_a,\n",
      "4580         const int * log2_dims_a,\n",
      "4581         const int * a_str,\n",
      "4582         const float * a_data,\n",
      "4583         const int * z_str,\n",
      "4584         float * z_data)\n",
      "4585 {\n",
      "4586     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4587     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4588 \n",
      "4589     //structure data contains the strides and dimensions of both a and z\n",
      "4590     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4591     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4592     // a_str[0], ... a_str[nd-1],\n",
      "4593     // z_str[0], ... z_str[nd-1]\n",
      "4594     extern __shared__ int structure_data[];\n",
      "4595     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4596     {\n",
      "4597         structure_data[i+0*nd] = dims_a[i];\n",
      "4598         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4599         structure_data[i+2*nd] = a_str[i];\n",
      "4600         structure_data[i+3*nd] = z_str[i];\n",
      "4601     }\n",
      "4602     dims_a = structure_data;\n",
      "4603     log2_dims_a = structure_data + nd;\n",
      "4604     a_str = structure_data + 2*nd;\n",
      "4605     z_str = structure_data + 3*nd;\n",
      "4606 \n",
      "4607     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4608 \n",
      "4609     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4610     {\n",
      "4611         unsigned int ii = i;\n",
      "4612         const float * a_data_i = a_data;\n",
      "4613         float * z_data_i = z_data;\n",
      "4614         unsigned int n_reduce_elements = 1;\n",
      "4615         unsigned int n_reduce_dims = 0;\n",
      "4616         unsigned int reduce_dim0 = nd-1;\n",
      "4617 \n",
      "4618 \n",
      "4619         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4620         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4621         for (unsigned int d = 0; d < nd; ++d)\n",
      "4622         {\n",
      "4623             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4624             {\n",
      "4625                 n_reduce_elements *= dims_a[d];\n",
      "4626                 n_reduce_dims += 1;\n",
      "4627                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4628             }\n",
      "4629             else //'d' is not a dimension that we are reducing over\n",
      "4630             {\n",
      "4631                 unsigned int pos_d;\n",
      "4632                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4633                 {\n",
      "4634                     // this branch is not preferred,\n",
      "4635                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4636                     pos_d = (ii % dims_a[d]);\n",
      "4637                     ii = (ii / dims_a[d]);\n",
      "4638                 }\n",
      "4639                 else\n",
      "4640                 {\n",
      "4641                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4642                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4643                 }\n",
      "4644                 a_data_i += pos_d * a_str[d];\n",
      "4645                 z_data_i += pos_d * z_str[d];\n",
      "4646             }\n",
      "4647         }\n",
      "4648         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4649         // do a similar loop\n",
      "4650 \n",
      "4651         float sum = 0.0f;\n",
      "4652         switch(n_reduce_dims)\n",
      "4653         {\n",
      "4654             case 0:\n",
      "4655                 {\n",
      "4656                     sum = a_data_i[0];\n",
      "4657                 }\n",
      "4658                 break;\n",
      "4659             case 1:\n",
      "4660                 {\n",
      "4661                     const int stride = a_str[reduce_dim0];\n",
      "4662                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4663                     while (a_data_i != a_data_i_max)\n",
      "4664                     {\n",
      "4665                         sum += a_data_i[0];\n",
      "4666                         a_data_i += stride;\n",
      "4667                     }\n",
      "4668                 }\n",
      "4669                 break;\n",
      "4670             case 2:\n",
      "4671                 {\n",
      "4672                     int rd = reduce_dim0+1;\n",
      "4673                     for (; rd < nd; ++rd)\n",
      "4674                     {\n",
      "4675                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4676                             break;\n",
      "4677                     }\n",
      "4678                     const int stride0 = a_str[reduce_dim0];\n",
      "4679                     const int stride1 = a_str[rd];\n",
      "4680                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4681                     {\n",
      "4682                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4683                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4684                         while (a_data_ri != a_data_ri_max)\n",
      "4685                         {\n",
      "4686                             sum += a_data_ri[0];\n",
      "4687                             a_data_ri += stride0;\n",
      "4688                         }\n",
      "4689                     }\n",
      "4690                 };\n",
      "4691                 break;\n",
      "4692             default:\n",
      "4693                 {\n",
      "4694                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4695                     {\n",
      "4696                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4697                         unsigned int reduce_ii = reduce_i;\n",
      "4698                         const float * a_data_ri = a_data_i;\n",
      "4699 \n",
      "4700                         //This loop finds the element in the a slice to add.\n",
      "4701                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4702                         {\n",
      "4703                             unsigned int pos_d;\n",
      "4704                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4705                             {\n",
      "4706                                 if (log2_dims_a[rd]==-1)\n",
      "4707                                 {\n",
      "4708                                     // this branch is not preferred,\n",
      "4709                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4710                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4711                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4712                                 }\n",
      "4713                                 else\n",
      "4714                                 {\n",
      "4715                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4716                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4717                                 }\n",
      "4718                                 a_data_ri += pos_d * a_str[rd];\n",
      "4719                             }\n",
      "4720                         }\n",
      "4721                         sum += a_data_ri[0];\n",
      "4722                     }\n",
      "4723                 }\n",
      "4724         }\n",
      "4725         z_data_i[0] = sum;\n",
      "4726     }\n",
      "4727 }\n",
      "4728 \n",
      "4729 static __global__ void kernel_reduce_sum_1011(\n",
      "4730         const unsigned int d0,\n",
      "4731         const unsigned int d1,\n",
      "4732         const unsigned int d2,\n",
      "4733         const unsigned int d3,\n",
      "4734         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4735         float * Z, const int sZ0)\n",
      "4736 {\n",
      "4737     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4738     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4739     extern __shared__ float buf[];\n",
      "4740     float mysum = 0.0f;\n",
      "4741 \n",
      "4742     if (warpSize != 32)\n",
      "4743     {\n",
      "4744         return;  //TODO: set error code\n",
      "4745     }\n",
      "4746 \n",
      "4747     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4748     {\n",
      "4749         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4750         mysum += Ai;\n",
      "4751     }\n",
      "4752     buf[threadNum] = mysum;\n",
      "4753     __syncthreads();\n",
      "4754 \n",
      "4755     // rest of function is handled by one warp\n",
      "4756     if (threadNum < warpSize)\n",
      "4757     {\n",
      "4758         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4759         {\n",
      "4760             mysum += buf[i];\n",
      "4761         }\n",
      "4762         buf[threadNum] = mysum;\n",
      "4763         if (threadNum < 16)\n",
      "4764         {\n",
      "4765             //reduce so that threadNum 0 has the sum of everything\n",
      "4766             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4767             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4768             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4769             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4770             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4771             if (threadNum == 0)\n",
      "4772             {\n",
      "4773                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4774             }\n",
      "4775         }\n",
      "4776     }\n",
      "4777 }\n",
      "4778 /**\n",
      "4779  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4780  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4781  */\n",
      "4782 int\n",
      "4783 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4784 {\n",
      "4785     int verbose = 0;\n",
      "4786     //check input rank\n",
      "4787     if (self->nd != A->nd)\n",
      "4788     {\n",
      "4789         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4790         return -1;\n",
      "4791     }\n",
      "4792     for (int i = 0; i < self->nd; ++i)\n",
      "4793     {\n",
      "4794         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4795         {\n",
      "4796             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4797                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4798             return -1;\n",
      "4799         }\n",
      "4800     }\n",
      "4801 \n",
      "4802     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4803     if (verbose)\n",
      "4804     {\n",
      "4805         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4806         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4807         fprint_CudaNdarray(stderr, A);\n",
      "4808         fprint_CudaNdarray(stderr, self);\n",
      "4809     }\n",
      "4810     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4811             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4812             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4813             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4814        )\n",
      "4815     {\n",
      "4816         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4817         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4818         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4819         n_threads.z -= 1;\n",
      "4820         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4821         if (n_threads.z)\n",
      "4822         {\n",
      "4823             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4824             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4825             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4826                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4827                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4828                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4829                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4830                     CudaNdarray_DEV_DATA(A),\n",
      "4831                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4832                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4833                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4834                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4835                     CudaNdarray_DEV_DATA(self),\n",
      "4836                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4837             CNDA_THREAD_SYNC;\n",
      "4838             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4839             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4840         }\n",
      "4841     }\n",
      "4842 \n",
      "4843     int n_threads_per_block = std::min(n_summations,\n",
      "4844             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4845     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4846             NUM_VECTOR_OP_BLOCKS);\n",
      "4847     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4848 \n",
      "4849     if (verbose)\n",
      "4850     {\n",
      "4851         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4852     }\n",
      "4853     assert (self->nd > 0);\n",
      "4854     assert (self->nd == A->nd);\n",
      "4855     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4856             n_summations,\n",
      "4857             self->nd,\n",
      "4858             CudaNdarray_DEV_DIMS(A),\n",
      "4859             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4860             CudaNdarray_DEV_STRIDES(A),\n",
      "4861             CudaNdarray_DEV_DATA(A),\n",
      "4862             CudaNdarray_DEV_STRIDES(self),\n",
      "4863             CudaNdarray_DEV_DATA(self));\n",
      "4864     CNDA_THREAD_SYNC;\n",
      "4865     cudaError_t err = cudaGetLastError();\n",
      "4866     if (cudaSuccess != err)\n",
      "4867     {\n",
      "4868         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4869         return -1;\n",
      "4870     }\n",
      "4871     return 0;\n",
      "4872 }\n",
      "4873 int\n",
      "4874 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4875 {\n",
      "4876     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4877     return -1;\n",
      "4878 }\n",
      "4879 int\n",
      "4880 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4881 {\n",
      "4882     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4883     return -1;\n",
      "4884 }\n",
      "4885 int\n",
      "4886 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4887 {\n",
      "4888     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4889     return -1;\n",
      "4890 }\n",
      "4891 \n",
      "4892 \n",
      "4893 /**\n",
      "4894  *\n",
      "4895  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4896  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4897  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4898  *\n",
      "4899  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4900  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4901  */\n",
      "4902 int\n",
      "4903 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4904 {\n",
      "4905     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4906     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4907     int * newstrides = newdims + len;\n",
      "4908     int * dims_taken = newstrides + len;\n",
      "4909     if (!newdims)\n",
      "4910     {\n",
      "4911         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4912         return -1;\n",
      "4913     }\n",
      "4914     for (int i = 0; i < self->nd; ++i)\n",
      "4915     {\n",
      "4916         dims_taken[i] = 0;\n",
      "4917     }\n",
      "4918     for (int i = 0; i < len; ++i)\n",
      "4919     {\n",
      "4920         if (pattern[i] < 0)\n",
      "4921         {\n",
      "4922             newdims[i] = 1;\n",
      "4923             newstrides[i] = 0;\n",
      "4924         }\n",
      "4925         else if(dims_taken[pattern[i]])\n",
      "4926         {\n",
      "4927             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4928                          pattern[i]);\n",
      "4929             free(newdims);\n",
      "4930             return -1;\n",
      "4931         }\n",
      "4932         else if (pattern[i]>= self->nd)\n",
      "4933         {\n",
      "4934             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4935                          pattern[i], self->nd);\n",
      "4936             free(newdims);\n",
      "4937             return -1;\n",
      "4938         }\n",
      "4939         else\n",
      "4940         {\n",
      "4941             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4942             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4943             dims_taken[pattern[i]] = 1;\n",
      "4944         }\n",
      "4945     }\n",
      "4946     //Check if we dropped not broadcastable dims\n",
      "4947     for (int i = 0; i < self->nd; ++i)\n",
      "4948     {\n",
      "4949         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4950         {\n",
      "4951             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4952             free(newdims);\n",
      "4953             return -1;\n",
      "4954         }\n",
      "4955     }\n",
      "4956     //swap this structure in for the one in self, and sync to the card\n",
      "4957     if (CudaNdarray_set_nd(self, len))\n",
      "4958     {\n",
      "4959         free(newdims);\n",
      "4960         return -1;\n",
      "4961     }\n",
      "4962     for (int i = 0; i < len; ++i)\n",
      "4963     {\n",
      "4964         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4965         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4966     }\n",
      "4967     if (cnda_copy_structure_to_device(self))\n",
      "4968     {\n",
      "4969         free(newdims);\n",
      "4970         return -1;\n",
      "4971     }\n",
      "4972     free(newdims);\n",
      "4973     return 0;\n",
      "4974 }\n",
      "4975 \n",
      "4976 \n",
      "4977 \n",
      "4978 /**\n",
      "4979  *\n",
      "4980  *  This is the function that bind to python.\n",
      "4981  *  See CudaNdarray_dimshuffle to call from C.\n",
      "4982  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "4983  */\n",
      "4984 PyObject *\n",
      "4985 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "4986 {\n",
      "4987     PyObject * self = NULL;\n",
      "4988     PyObject * pattern_object = NULL;\n",
      "4989     int * pattern = NULL;\n",
      "4990     PyObject * rval = NULL;\n",
      "4991     int success = -1;\n",
      "4992     //const int * dims = NULL;\n",
      "4993 \n",
      "4994     //args should consist of two python objects (\"OO\")\n",
      "4995     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "4996         return NULL;\n",
      "4997 \n",
      "4998     if (!CudaNdarray_Check(self) )\n",
      "4999     {\n",
      "5000         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "5001         return NULL;\n",
      "5002     }\n",
      "5003 \n",
      "5004     //parse pattern_object into int * pattern\n",
      "5005 \n",
      "5006     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "5007 \n",
      "5008     if (pattern_dim < 0)\n",
      "5009     {\n",
      "5010         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "5011         return NULL;\n",
      "5012     }\n",
      "5013 \n",
      "5014     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "5015 \n",
      "5016     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "5017     {\n",
      "5018         PyObject * idx = PyLong_FromLong(i);\n",
      "5019 \n",
      "5020         if (idx == NULL)\n",
      "5021         {\n",
      "5022             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "5023             goto CudaNdarray_dimshuffle_fail;\n",
      "5024         }\n",
      "5025 \n",
      "5026         long elem_value = 0;\n",
      "5027 \n",
      "5028         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "5029 \n",
      "5030         if (elem == NULL)\n",
      "5031         {\n",
      "5032             Py_XDECREF( elem);\n",
      "5033             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5034             goto CudaNdarray_dimshuffle_fail;\n",
      "5035         }\n",
      "5036 \n",
      "5037         elem_value = PyInt_AsLong(elem);\n",
      "5038 \n",
      "5039         if (elem_value == -1 && PyErr_Occurred() )\n",
      "5040         {\n",
      "5041             Py_XDECREF(elem);\n",
      "5042             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5043             goto CudaNdarray_dimshuffle_fail;\n",
      "5044         }\n",
      "5045 \n",
      "5046         pattern[i] = elem_value;\n",
      "5047 \n",
      "5048         Py_XDECREF( elem );\n",
      "5049         Py_XDECREF( idx );\n",
      "5050     }\n",
      "5051 \n",
      "5052     //allocate rval\n",
      "5053     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "5054 \n",
      "5055     if (rval == NULL)\n",
      "5056     {\n",
      "5057         //CudaNdarray_New should have set the exception string\n",
      "5058         goto CudaNdarray_dimshuffle_fail;\n",
      "5059     }\n",
      "5060 \n",
      "5061 \n",
      "5062     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "5063     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "5064     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "5065     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "5066 \n",
      "5067     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "5068 \n",
      "5069     if (success != 0)\n",
      "5070     {\n",
      "5071         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "5072         goto CudaNdarray_dimshuffle_fail;\n",
      "5073     }\n",
      "5074 \n",
      "5075     free(pattern);\n",
      "5076 \n",
      "5077     return rval;\n",
      "5078 \n",
      "5079     CudaNdarray_dimshuffle_fail:\n",
      "5080 \n",
      "5081     if (pattern != NULL)\n",
      "5082         free(pattern);\n",
      "5083 \n",
      "5084     Py_XDECREF(rval);\n",
      "5085     return NULL;\n",
      "5086 }\n",
      "5087 \n",
      "5088 \n",
      "5089 int\n",
      "5090 cnda_structure_size(int nd)\n",
      "5091 {\n",
      "5092     // dim0, dim1, ...\n",
      "5093     // str0, str1, ...\n",
      "5094     // log2(dim0), log2(dim1), ...\n",
      "5095     return nd + nd + nd;\n",
      "5096 }\n",
      "5097 \n",
      "5098 const int *\n",
      "5099 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "5100 {\n",
      "5101     return self->host_structure;\n",
      "5102 }\n",
      "5103 \n",
      "5104 const int *\n",
      "5105 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "5106 {\n",
      "5107     return self->host_structure + self->nd;\n",
      "5108 }\n",
      "5109 const int *\n",
      "5110 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "5111 {\n",
      "5112     return self->host_structure + 2*self->nd;\n",
      "5113 }\n",
      "5114 \n",
      "5115 int\n",
      "5116 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "5117 {\n",
      "5118     int verbose = 0;\n",
      "5119 \n",
      "5120     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "5121     {\n",
      "5122         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "5123         return 0;\n",
      "5124     }\n",
      "5125 \n",
      "5126     if (cnda1->nd != cnda2->nd)\n",
      "5127     {\n",
      "5128         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "5129         return 0;\n",
      "5130     }\n",
      "5131 \n",
      "5132     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "5133     {\n",
      "5134         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5135         {\n",
      "5136             if(verbose)\n",
      "5137                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5138             return 0;\n",
      "5139         }\n",
      "5140     }\n",
      "5141 \n",
      "5142     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5143     {\n",
      "5144         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5145         return 0;\n",
      "5146     }\n",
      "5147     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5148     {\n",
      "5149         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5150         return 0;\n",
      "5151     }\n",
      "5152     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5153     {\n",
      "5154         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5155         // no need to check devdata if data is not allocated\n",
      "5156         return 0;\n",
      "5157     }\n",
      "5158 \n",
      "5159     return 1;\n",
      "5160 }\n",
      "5161 \n",
      "5162 \n",
      "5163 int\n",
      "5164 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5165 {\n",
      "5166     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5167 }\n",
      "5168 \n",
      "5169 int\n",
      "5170 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5171 {\n",
      "5172     //If the device structure do not exists, create it.\n",
      "5173     //We allocate it here as we do not need it often.\n",
      "5174     //In fact, we need it so infrequently that we expect\n",
      "5175     //that most object won't need it. Not allocating it\n",
      "5176     //save a significant when creating object.\n",
      "5177     //This speed up a benchmark by 8% with the gc.\n",
      "5178     if (!self->dev_structure)\n",
      "5179     {\n",
      "5180         int struct_size = cnda_structure_size(self->nd);\n",
      "5181         if (struct_size)\n",
      "5182         {\n",
      "5183             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5184             if (NULL == self->dev_structure)\n",
      "5185             {\n",
      "5186                 return -1;\n",
      "5187             }\n",
      "5188         }\n",
      "5189     }\n",
      "5190     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5191                         sizeof(int),\n",
      "5192                         self->host_structure,\n",
      "5193                         1,\n",
      "5194                         self->dev_structure,\n",
      "5195                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5196     {\n",
      "5197         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5198         return -1;\n",
      "5199     }\n",
      "5200     self->dev_structure_fresh = 1;\n",
      "5201     return 0;\n",
      "5202 }\n",
      "5203 \n",
      "5204 const int *\n",
      "5205 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5206 {\n",
      "5207     if (!self->dev_structure_fresh)\n",
      "5208     {\n",
      "5209         if (cnda_copy_structure_to_device(self))\n",
      "5210             return NULL;\n",
      "5211     }\n",
      "5212     return self->dev_structure;\n",
      "5213 }\n",
      "5214 const int *\n",
      "5215 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5216 {\n",
      "5217     if (!self->dev_structure_fresh)\n",
      "5218     {\n",
      "5219         if (cnda_copy_structure_to_device(self))\n",
      "5220             return NULL;\n",
      "5221     }\n",
      "5222     return self->dev_structure + self->nd;\n",
      "5223 }\n",
      "5224 const int *\n",
      "5225 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5226 {\n",
      "5227     if (!self->dev_structure_fresh)\n",
      "5228     {\n",
      "5229         if (cnda_copy_structure_to_device(self))\n",
      "5230             return NULL;\n",
      "5231     }\n",
      "5232     return self->dev_structure + 2*self->nd;\n",
      "5233 }\n",
      "5234 float *\n",
      "5235 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5236 {\n",
      "5237     return self->devdata;\n",
      "5238 }\n",
      "5239 \n",
      "5240 /**\n",
      "5241  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5242  */\n",
      "5243 size_t\n",
      "5244 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5245 {\n",
      "5246     if (self->nd == -1) return 0;\n",
      "5247     size_t size = 1;\n",
      "5248     for (int i = 0; i < self->nd; ++i)\n",
      "5249     {\n",
      "5250         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5251     }\n",
      "5252     return size;\n",
      "5253 }\n",
      "5254 \n",
      "5255 PyObject *\n",
      "5256 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5257 {\n",
      "5258     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5259 }\n",
      "5260 \n",
      "5261 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5262 {\n",
      "5263     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5264 }\n",
      "5265 \n",
      "5266 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5267 {\n",
      "5268     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5269 }\n",
      "5270 \n",
      "5271 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5272 {\n",
      "5273     cudaError_t err = cudaGetLastError();\n",
      "5274     if( cudaSuccess != err)\n",
      "5275     {\n",
      "5276         PyErr_Format(PyExc_RuntimeError,\n",
      "5277                      \"Cuda error: %s: %s.\",\n",
      "5278                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5279                      cudaGetErrorString(err));\n",
      "5280         return -1;\n",
      "5281     }\n",
      "5282     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5283             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5284     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5285     for (int i = 0; i < self->nd; ++i)\n",
      "5286     {\n",
      "5287         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5288     }\n",
      "5289     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5290     for (int i = 0; i < self->nd; ++i)\n",
      "5291     {\n",
      "5292         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5293     }\n",
      "5294 \n",
      "5295     if (self->dev_structure)\n",
      "5296     {\n",
      "5297         int data=0;\n",
      "5298         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5299         for (int i = 0; i < self->nd; ++i)\n",
      "5300         {\n",
      "5301             cublasGetVector(1, sizeof(int),\n",
      "5302                             self->dev_structure+i, 1,\n",
      "5303                             &data, 1);\n",
      "5304             fprintf(fd, \"%i\\t\", data);\n",
      "5305         }\n",
      "5306         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5307         for (int i = 0; i < self->nd; ++i)\n",
      "5308         {\n",
      "5309             cublasGetVector(1, sizeof(int),\n",
      "5310                             self->dev_structure + self->nd+i, 1,\n",
      "5311                             &data, 1);\n",
      "5312             fprintf(fd, \"%i \\t\", data);\n",
      "5313         }\n",
      "5314         fprintf(fd, \"\\n\");\n",
      "5315     }\n",
      "5316     else\n",
      "5317     {\n",
      "5318         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5319     }\n",
      "5320 \n",
      "5321     err = cudaGetLastError();\n",
      "5322     if( cudaSuccess != err)\n",
      "5323     {\n",
      "5324         PyErr_Format(PyExc_RuntimeError,\n",
      "5325                      \"Cuda error: %s: %s.\",\n",
      "5326                      \"fprint_CudaNdarray\",\n",
      "5327                      cudaGetErrorString(err));\n",
      "5328         return -1;\n",
      "5329     }\n",
      "5330     return 0;\n",
      "5331 }\n",
      "5332 \n",
      "5333 \n",
      "5334 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5335                             const int * dims, int fortran)\n",
      "5336 {\n",
      "5337     bool allocated = false;\n",
      "5338     if (*arr == NULL)\n",
      "5339     {\n",
      "5340         // This allocates the metadata but not the data\n",
      "5341         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5342         if (*arr == NULL)\n",
      "5343             return -1;\n",
      "5344         allocated = true;\n",
      "5345     }\n",
      "5346 \n",
      "5347     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5348     {\n",
      "5349         if (allocated)\n",
      "5350         {\n",
      "5351             Py_DECREF(*arr);\n",
      "5352             *arr = NULL;\n",
      "5353         }\n",
      "5354         return -1;\n",
      "5355     }\n",
      "5356     return 0;\n",
      "5357 }\n",
      "5358 \n",
      "5359 \n",
      "5360 /*\n",
      "5361   Local Variables:\n",
      "5362   mode:c++\n",
      "5363   c-basic-offset:4\n",
      "5364   c-file-style:\"stroustrup\"\n",
      "5365   indent-tabs-mode:nil\n",
      "5366   fill-column:79\n",
      "5367   End:\n",
      "5368 */\n",
      "5369 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5370 \n",
      "===============================\n",
      "In file included from /vol/cuda/8.0.27-RC1/bin/..//include/cuda_runtime.h:78:0,\n",
      "                 from <command-line>:0:\n",
      "/vol/cuda/8.0.27-RC1/bin/..//include/host_config.h:115:2: error: #error -- unsupported GNU version! gcc versions later than 5.3 are not supported!\n",
      " #error -- unsupported GNU version! gcc versions later than 5.3 are not supported!\n",
      "  ^\n",
      "In file included from mod.cu:10:0:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:17:0: warning: \"PyString_Check\" redefined\n",
      " #define PyString_Check PyUnicode_Check\n",
      " ^\n",
      "In file included from /vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:10:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:63:0: note: this is the location of the previous definition\n",
      " #define PyString_Check PyBytes_Check\n",
      " ^\n",
      "In file included from mod.cu:10:0:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:18:0: warning: \"PyString_FromString\" redefined\n",
      " #define PyString_FromString PyUnicode_FromString\n",
      " ^\n",
      "In file included from /vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:10:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:65:0: note: this is the location of the previous definition\n",
      " #define PyString_FromString PyBytes_FromString\n",
      " ^\n",
      "In file included from mod.cu:10:0:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:19:0: warning: \"PyString_AsString\" redefined\n",
      " #define PyString_AsString PyUnicode_AsUTF8\n",
      " ^\n",
      "In file included from /vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:10:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:72:0: note: this is the location of the previous definition\n",
      " #define PyString_AsString PyBytes_AsString\n",
      " ^\n",
      "In file included from mod.cu:10:0:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:20:0: warning: \"PyString_FromStringAndSize\" redefined\n",
      " #define PyString_FromStringAndSize PyUnicode_FromStringAndSize\n",
      " ^\n",
      "In file included from /vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:10:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:66:0: note: this is the location of the previous definition\n",
      " #define PyString_FromStringAndSize PyBytes_FromStringAndSize\n",
      " ^\n",
      "In file included from mod.cu:10:0:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:21:0: warning: \"PyString_Size\" redefined\n",
      " #define PyString_Size PyUnicode_GET_SIZE\n",
      " ^\n",
      "In file included from /vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,\n",
      "                 from mod.cu:10:\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:74:0: note: this is the location of the previous definition\n",
      " #define PyString_Size PyBytes_Size\n",
      " ^\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -m64 -Xcompiler -DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden -Xlinker -rpath,/homes/yz4009/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray -I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda -I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include -I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/include/python3.5m -I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/gof -L/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib -o /homes/yz4009/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray/cuda_ndarray.so mod.cu -lcublas -lpython3.5m -lcudart')\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: cuda unavailable)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['nvcc', '-shared', '-O3', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden', '-Xlinker', '-rpath,/homes/yz4009/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray', '-I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/sandbox/cuda', '-I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/numpy/core/include', '-I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/include/python3.5m', '-I/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/theano/gof', '-L/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib', '-o', '/homes/yz4009/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray/cuda_ndarray.so', 'mod.cu', '-lcublas', '-lpython3.5m', '-lcudart']\n"
     ]
    }
   ],
   "source": [
    "import menpo.io as mio\n",
    "from menpo.transform import AlignmentSimilarity, Translation\n",
    "from menpo.shape import PointCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.html.widgets import interact\n",
    "from menpowidgets import visualize_images, visualize_fitting_result\n",
    "from menpofit.transform import DifferentiableThinPlateSplines as tps\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from menpo.image import Image\n",
    "import os\n",
    "import itertools\n",
    "from shutil import copytree\n",
    "from menpowidgets import plot_ced\n",
    "from menpofit.error import euclidean_distance_normalised_error\n",
    "from menpofit.visualize import plot_cumulative_error_distribution\n",
    "from menpofit.fitter import noisy_shape_from_shape, noisy_target_alignment_transform\n",
    "from menpo.visualize import print_dynamic\n",
    "from dAAMs import dAAMs\n",
    "from menpofit.aam import HolisticAAM, PatchAAM\n",
    "from menpo.visualize import print_dynamic\n",
    "from menpofit.sdm import RegularizedSDM\n",
    "from menpofit.aam.base import compute_reference_shape\n",
    "from menpo.feature import igo, hog, no_op, double_igo as digo, dsift, fast_dsift, hellinger_vector_128_dsift\n",
    "from dAAMs.tools import group_from_labels, sift_svs_shape, hog_svs_shape, distance_transform_shape\n",
    "shog = lambda x: hog(x, cell_size=2, block_size=2)\n",
    "# from menpo.landmark import labeller, svs_face_68, ear_55\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_levels = 3\n",
    "n_shape = [5,8,13]\n",
    "scales = (0.25, 0.5, 1.0)\n",
    "n_appearance = 0.9\n",
    "max_iters = 50\n",
    "diagonal=200\n",
    "f = dsift\n",
    "patch_shape=[(9, 9),(13, 13),(17, 17)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path_to_db = '/vol/atlas/homes/yz4009/databases/ear/600EITW/train'\n",
    "training_images = []\n",
    "# load landmarked images\n",
    "for j,i in enumerate(mio.import_images(path_to_db + '/train*', verbose=True)):\n",
    "    # convert it to greyscale if needed\n",
    "    if i.n_channels == 3:\n",
    "        i = i.as_greyscale(mode='luminosity')\n",
    "    i = i.crop_to_landmarks_proportion(0.1)\n",
    "#     i.landmarks['LABEL'] = i.landmarks['PTS']\n",
    "#     i.landmarks['LABEL']['outbound'] = range(20) \n",
    "#     i.landmarks['LABEL']['inbound'] = range(20, 35)\n",
    "#     i.landmarks['LABEL']['innerbound'] = range(35, 50)\n",
    "#     i.landmarks['LABEL']['line5p'] = range(50, 55)\n",
    "    training_images.append(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_images[0].view_widget()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dsift_paam = PatchAAM(training_images, group='PTS', holistic_features=dsift, scales=scales, patch_shape=patch_shape, verbose=True,diagonal=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dsift_paam.view_aam_widget()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from menpofit.aam import LucasKanadeAAMFitter\n",
    "\n",
    "# define Lucas-Kanade based AAM fitter\n",
    "aam_fitter = LucasKanadeAAMFitter(dsift_paam, n_shape=n_shape, n_appearance=n_appearance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bb_from_shape(shape, boundary=10):\n",
    "    h,w = shape\n",
    "    return PointCloud(np.array([[boundary,boundary],[h-boundary,boundary],[boundary,w-boundary],[h-boundary,w-boundary]])).bounding_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk(seq, num):\n",
    "  avg = len(seq) / float(num)\n",
    "  out = []\n",
    "  last = 0.0\n",
    "\n",
    "  while last < len(seq):\n",
    "    out.append(seq[int(last):int(last + avg)])\n",
    "    last += avg\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_patch_feature(patches):\n",
    "    img_shape = patches.shape[2:]\n",
    "    return np.array([cnn_feature(Image(pimg)) for pimg in patches.reshape((-1,)+img_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_match(img, crp):\n",
    "    _, h, w = img.pixels.shape\n",
    "    _, ch, cw = crp.pixels.shape\n",
    "    translations = []\n",
    "    for ih in range(h-ch):\n",
    "        for iw in range(w-cw):\n",
    "            if (img.pixels[:,ih:ih+ch,iw:iw+cw] == crp.pixels).all():\n",
    "                return ih,iw\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from menpo.model import PCAModel, PCAVectorModel\n",
    "def pca_mapping(datas, ratio=0.90):\n",
    "    pca = PCAVectorModel(datas)\n",
    "    pca.n_active_components = np.argwhere(pca.eigenvalues_cumulative_ratio() > ratio)[0][0]\n",
    "    return [pca.project(vec) for vec in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_align(img):\n",
    "    warp_transform = DifferentiablePiecewiseAffine(reference_frame.landmarks['source'].lms,\n",
    "                               img.landmarks['PTS'].lms)\n",
    "    \n",
    "    warped_i = img.warp_to_mask(reference_frame.mask, warp_transform,\n",
    "                                  warp_landmarks=False)\n",
    "    # attach reference frame landmarks to images\n",
    "    warped_i.landmarks['PTS'] = reference_frame.landmarks['source']\n",
    "    \n",
    "    \n",
    "    return warped_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG Ear DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_path = Path('/homes/yz4009/wd/databases/ear/VGGEers-Recognition')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "images = []\n",
    "for directory in db_path.iterdir():\n",
    "    print directory\n",
    "    for img in mio.import_images(directory):\n",
    "        if img.n_channels == 3:\n",
    "            img = img.as_greyscale(mode='luminosity')\n",
    "        \n",
    "#         img = img.crop_to_landmarks_proportion(0.2)\n",
    "#         print img.path\n",
    "#         img = img.rescale_to_diagonal(250)\n",
    "        images.append(img.mirror())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "images_crop = []\n",
    "for directory in db_path.iterdir():\n",
    "    print directory\n",
    "    for img in mio.import_images(directory):\n",
    "        if img.n_channels == 3:\n",
    "            img = img.as_greyscale(mode='luminosity')\n",
    "        \n",
    "        img = img.crop_to_landmarks_proportion(0.5)\n",
    "        images_crop.append(img.mirror())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from menpo.transform import Translation\n",
    "def recover_landmarks(img, crp, fr, trans):\n",
    "    fr_img = fr.image\n",
    "    fr_img.landmarks['PTS'] = fr.final_shape\n",
    "    fr_img = fr_img.resize(crp.shape)\n",
    "    cimg = img.copy()\n",
    "    cimg.landmarks['PTS'] = Translation(trans).apply(fr_img.landmarks['PTS'].lms)\n",
    "    \n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# mio.export_pickle(data, '/homes/yz4009/wd/PickleModel/EarRecognition/VGGEAR-crop.pkl', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from menpofit.builder import build_reference_frame\n",
    "from menpofit.transform import DifferentiablePiecewiseAffine\n",
    "from menpo.transform import Translation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dsift_paam = mio.import_pickle('/homes/yz4009/wd/PickleModel/aam-ear-dsift-3-level.pkl', encoding='latin1')\n",
    "mean_shape = dsift_paam.shape_models[-1].model.mean()\n",
    "reference_frame = build_reference_frame(mean_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reference_frame = build_reference_frame(PointCloud(np.concatenate([img.landmarks['PTS'].lms.points for img in images])))\n",
    "reference_frame = reference_frame.as_unmasked()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wimages = [img.warp_to_shape(reference_frame.shape, Translation(-reference_frame.centre()+img.centre())) for img in images]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def n_fold_generate(data, n_fold=4):\n",
    "    it = itertools.groupby(data, lambda x: x[0])\n",
    "    folded_data = [[] for i in range(n_fold)]\n",
    "\n",
    "    for grp in it:\n",
    "        for j,d in enumerate(chunk(list(grp[1]), n_fold)):\n",
    "            folded_data[j].append(d)\n",
    "            \n",
    "    fdata = [reduce(lambda x,y: x+y, f) for f in folded_data]\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def cross_validation(fdata):\n",
    "    accuracy = 0\n",
    "    for j,test_set in enumerate(fdata):\n",
    "        print_dynamic('processing {}/{} fold'.format(j+1, len(fdata)))\n",
    "        \n",
    "        train_set = reduce(lambda x,y: x+y, fdata[:j] + fdata[j+1:])\n",
    "        \n",
    "        train_X = map(lambda x:x[-1], train_set)\n",
    "        train_y = map(lambda x:x[0], train_set)\n",
    "        test_X = map(lambda x:x[-1], test_set)\n",
    "        test_y = map(lambda x:x[0], test_set)\n",
    "        \n",
    "        lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "        pred_y = lda.fit(train_X, train_y).predict(test_X)\n",
    "        \n",
    "        acc = np.sum(test_y == pred_y) / float(len(pred_y))\n",
    "        \n",
    "        print '{}/{} fold acc: {}'.format(j+1, len(fdata), acc)\n",
    "        \n",
    "        accuracy += acc\n",
    "        \n",
    "        del lda, train_set, train_X, train_y, test_X, test_y, pred_y\n",
    "        \n",
    "        \n",
    "    return accuracy / float(len(fdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dAAMs.tools import cnn_feature\n",
    "from menpo.image import Image"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "str_labels = [i.path.parent.name for i in images]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "labels = [int(np.argwhere(np.array(list(set(str_labels))) == l).squeeze()) for l in str_labels]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# mio.export_pickle(labels,'/homes/yz4009/wd/PickleModel/EarRecognition/VGG-Labels.pkl')\n",
    "labels = mio.import_pickle('/homes/yz4009/wd/PickleModel/EarRecognition/VGG-Labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    mio.export_pickle(np.concatenate([hellinger_vector_128_dsift(Image(p.squeeze())).ravel() for p in i.extract_patches_around_landmarks()]), '{}/{}+128dsift.pkl'.format(i.path.parent, i.path.stem), overwrite=True)\n",
    "\n",
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/aligned/')):\n",
    "    mio.export_pickle(np.concatenate([hellinger_vector_128_dsift(Image(p.squeeze())).ravel() for p in i.extract_patches_around_landmarks()]), '{}/{}+128dsift.pkl'.format(i.path.parent, i.path.stem), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 1, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellinger_vector_128_dsift(Image(img.extract_patches_around_landmarks()[0].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepf = cnn_feature(img, layer_name='cov5_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    mio.export_pickle(cnn_feature(i, layer_name='cov5_1').ravel(), '{}/{}+deep.pkl'.format(i.path.parent, i.path.stem), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/aligned/')):\n",
    "    mio.export_pickle(cnn_feature(i, layer_name='cov5_1').ravel(), '{}/{}+deep.pkl'.format(i.path.parent, i.path.stem), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    mio.export_pickle(igo(i).pixels.ravel(), '{}/{}+igo.pkl'.format(i.path.parent, i.path.stem), overwrite=True)\n",
    "    \n",
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    mio.export_pickle(dsift(i).pixels.ravel(), '{}/{}+dsift.pkl'.format(i.path.parent, i.path.stem), overwrite=True)\n",
    "\n",
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    mio.export_pickle(hog(i).pixels.ravel(), '{}/{}+hog.pkl'.format(i.path.parent, i.path.stem), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/bound/')):\n",
    "    feature = []\n",
    "    for s,ps in zip(scales,patch_shape):\n",
    "        timg = i.rescale(s)\n",
    "        feature.append(np.concatenate([hellinger_vector_128_dsift(Image(p.squeeze())).ravel() for p in timg.extract_patches_around_landmarks()]))\n",
    "        feature.append(timg.landmarks['PTS'].lms.points.ravel())\n",
    "    features = np.concatenate(feature)\n",
    "    mio.export_pickle(features, '{}/{}+PEP128.pkl'.format(i.path.parent, i.path.stem), overwrite=True)\n",
    "    \n",
    "for j,i in enumerate(mio.import_images('/homes/yz4009/wd/databases/ear/EarVerification/VGGEAR/aligned/')):\n",
    "    feature = []\n",
    "    for s,ps in zip(scales,patch_shape):\n",
    "        timg = i.rescale(s)\n",
    "        feature.append(np.concatenate([hellinger_vector_128_dsift(Image(p.squeeze())).ravel() for p in timg.extract_patches_around_landmarks()]))\n",
    "        feature.append(timg.landmarks['PTS'].lms.points.ravel())\n",
    "    features = np.concatenate(feature)\n",
    "    mio.export_pickle(features, '{}/{}+PEP128.pkl'.format(i.path.parent, i.path.stem), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3348\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /homes/yz4009/wd/databases/ear/EarVerification/WPUTEDB/bound/*PEP.pkl | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = mio.import_pickle('/homes/yz4009/wd/PickleModel/EarRecognition/VGGEAR-bound-dsift.pkl',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, images = list(zip(*data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_datas = []\n",
    "for j,i in enumerate(images):\n",
    "#     img_datas.append(dsift(i).pixels.ravel())\n",
    "#     img_datas.append(dsift(img_align(i)).pixels.ravel())\n",
    "    img = i.resize((225,225))\n",
    "    feature = []\n",
    "    for s,ps in zip(scales,patch_shape):\n",
    "        timg = img.rescale(s)\n",
    "        feature.append(dsift(timg).extract_patches_around_landmarks(group='PTS', patch_shape=ps).ravel())\n",
    "        feature.append(timg.landmarks['PTS'].lms.points.ravel())\n",
    "    features = np.concatenate(feature)\n",
    "    img_datas.append(features)\n",
    "#     print_dynamic(j)\n",
    "#     img_datas.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_datas = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_datas = pca_mapping(img_datas, ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(labels, pca_datas))\n",
    "mio.export_pickle(data, '/homes/yz4009/wd/PickleModel/EarRecognition/VGGEAR-bound-dsift-pca.pkl', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_datas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354896,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "img_datas = []\n",
    "for directory in db_path.iterdir():\n",
    "    print directory\n",
    "    for f in mio.import_pickles(str(directory) + '/*_cnn_patch.pkl'):\n",
    "        img_datas.append(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = zip(labels, img_datas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mio.export_pickle(data, '/homes/yz4009/wd/PickleModel/EarRecognition/LDA-VGG-Data-cnn-patch.pkl', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in wimages:\n",
    "    img_datas.append(dsift(i).as_vector())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = zip(labels, img_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bounding box test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_data = mio.import_pickle('/homes/yz4009/wd/PickleModel/EarRecognition/VGGEAR-crop.pkl', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_images = list(zip(*recov_data))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounded_images = []\n",
    "for img in recov_images:\n",
    "    img = img.rescale_landmarks_to_diagonal_range((200,200), group='PTS')\n",
    "    \n",
    "    pc_bound = PointCloud(np.array([[0,0],[0,199],[199,0],[199,199]]))\n",
    "    lms_bounds = img.landmarks['PTS'].lms.bounding_box()\n",
    "    trans = Translation(lms_bounds.centre() - pc_bound.centre())\n",
    "    bounded_images.append(img.warp_to_shape((200,200), trans))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(labels, bounded_images)\n",
    "mio.export_pickle(data, '/homes/yz4009/wd/PickleModel/EarRecognition/VGGEAR-bound.pkl', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### no ear test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datas = []\n",
    "shape_h = 0\n",
    "shape_w = 0\n",
    "\n",
    "for fr in frs:\n",
    "    i = fr.image\n",
    "    i.landmarks['PTS'] = fr.final_shape\n",
    "    img_datas.append(i)\n",
    "    h,w = i.shape\n",
    "    \n",
    "    if h > shape_h:\n",
    "        shape_h = h\n",
    "        \n",
    "    if w > shape_w:\n",
    "        shape_w = w\n",
    "    \n",
    "# data = zip(labels, img_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 178)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_h, shape_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = Image.init_blank((shape_h, shape_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from menpo.transform import Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomask_data = []\n",
    "for img in img_datas:\n",
    "    wimg = img.warp_to_shape((shape_h, shape_w), Translation(cimg.centre() - bb.centre()))\n",
    "    nomask_data.append(dsift(wimg).pixels.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2058, 2058)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nomask_data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<menpo.visualize.viewmatplotlib.MatplotlibImageViewer2d at 0x7f62798de4d0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAHMCAYAAAAJY2mgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW7du3R7QOjh369at2wPa/wM33KNLLlqzGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62798de3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wimg.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = zip(labels, nomask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mio.export_pickle(data, '/homes/yz4009/wd/PickleModel/EarRecognition/LDA-VGG-Data-bbsift.pkl', overwrite=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = mio.import_pickle('/homes/yz4009/wd/PickleModel/EarRecognition/LDA-VGG-Data-onlymask.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cordor setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from workerbee import JobSet\n",
    "import menpo.io as mio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobname = 'cnn_patch_features'\n",
    "# db_path = Path('/homes/yz4009/wd/databases/ear/VGGEers-Recognition')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     (2016-09-12 13:27:58): Found Postgresql version 9.2.10 - Using JSON as the data type for the input_data field.\n",
      "WARNING  (2016-09-12 13:27:58): NO UNIQUE constraint enforced - input_data field is of type JSON\n",
      "INFO     (2016-09-12 13:27:58): Created table for jobset 'cnn_patch_features'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "js = JobSet(jobname,host='db.doc.ic.ac.uk',user='yz4009',password='1330871Pp',dbname='yz4009')\n",
    "js.setup_jobset(ignore_existing_jobset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_path = '/vol/atlas/homes/yz4009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vol/atlas/homes/yz4009/databases/ear/VGGEers-Recognition/Matt_Lauria/00000010_fr.pkl'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(frs[1].path).replace('/homes/yz4009/wd',wd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:08): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:09): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:10): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:11): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:12): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:13): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:14): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n",
      "INFO     (2016-09-12 13:28:15): Submitted 1 jobs\n"
     ]
    }
   ],
   "source": [
    "for fr in frs:\n",
    "    i = fr.image\n",
    "    i.landmarks['PTS'] = fr.final_shape\n",
    "#     img_datas.append(cnn_feature(img_align(i)).ravel())\n",
    "#     img_datas.append(cnn_patch_feature(i.rescale_to_diagonal(200).extract_patches_around_landmarks(group='PTS', patch_shape=(17,17))).ravel())\n",
    "    js.add_jobs([{'path':str(fr.path).replace('/homes/yz4009/wd',wd_path)}])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dAAMs.tools import denseCNN, cnn_feature\n",
    "from pathlib import Path\n",
    "import menpo.io as mio\n",
    "from menpo.image import Image\n",
    "import numpy as np\n",
    "from menpofit.aam import LucasKanadeAAMFitter\n",
    "\n",
    "\n",
    "def cnn_patch_feature(patches):\n",
    "    img_shape = patches.shape[2:]\n",
    "    return np.array([cnn_feature(Image(pimg)) for pimg in patches.reshape((-1,)+img_shape)])\n",
    "\n",
    "\n",
    "def func(args):\n",
    "    \n",
    "    \n",
    "    fr_path = args['path']\n",
    "    p = Path(fr_path)\n",
    "    fr = mio.import_pickle(fr_path)\n",
    "    \n",
    "    img = fr.image\n",
    "    img.landmarks['PTS'] = fr.final_shape\n",
    "    \n",
    "    feature = cnn_patch_feature(img.rescale_to_diagonal(200).extract_patches_around_landmarks(group='PTS', patch_shape=(17,17))).ravel()\n",
    "    \n",
    "    mio.export_pickle(feature, '{}/{}_cnn_patch.pkl'.format(p.parent, p.stem), overwrite=True)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 37s, sys: 1min 43s, total: 16min 20s\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "func({'path':'/vol/atlas/homes/yz4009/databases/ear/VGGEers-Recognition/Matt_Lauria/00000010_fr.pkl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Feature test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dAAMs.tensorflowear import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2058 files.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "test_provider = EarDB(batch_size=64,db_name='VGGEAR', shape=(225,225), num_classes=500,root='/homes/yz4009/wd/PickleModel/EarRecognition/',is_training=False)\n",
    "images, labels = test_provider.get('labels')\n",
    "\n",
    "tf.image_summary('images', images)\n",
    "\n",
    "predictions, layers = network(images, is_training=False, output_classes=500)\n",
    "\n",
    "predictions = tf.to_int32(tf.argmax(predictions, 1))\n",
    "labels = tf.to_int32(tf.argmax(labels, 1))\n",
    "\n",
    "tf.scalar_summary('accuracy', slim.metrics.accuracy(predictions, labels))\n",
    "\n",
    "num_batches = 859 // batch_size\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# These are streaming metrics which compute the \"running\" metric,\n",
    "# e.g running accuracy\n",
    "metrics_to_values, metrics_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    \"streaming_accuracy\": slim.metrics.streaming_accuracy(predictions, labels),\n",
    "})\n",
    "\n",
    "# Define the streaming summaries to write:\n",
    "for metric_name, metric_value in metrics_to_values.items():\n",
    "    tf.scalar_summary(metric_name, metric_value)\n",
    "\n",
    "global_step = slim.get_or_create_global_step()\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.999, global_step)\n",
    "variables_to_restore = variable_averages.variables_to_restore(slim.get_model_variables())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Evaluate every 30 seconds\n",
    "slim.evaluation.evaluation_loop(\n",
    "    '',\n",
    "    'ckpt/' + log_dir  + '_train/',\n",
    "    'ckpt/' + log_dir  + '_eval/',\n",
    "    num_evals=num_batches,\n",
    "    eval_op=list(metrics_to_updates.values()),\n",
    "    summary_op=tf.merge_all_summaries(),\n",
    "    variables_to_restore=variables_to_restore,\n",
    "    eval_interval_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.training import saver as tf_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_layer = layers['resnet_v1_50/conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(64), Dimension(113), Dimension(113), Dimension(64)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = slim.get_or_create_global_step()\n",
    "\n",
    "saver = tf_saver.Saver(slim.get_variables_to_restore())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/homes/yz4009/wd/gitdev/DenseHumanPose/dAAMs/ckpt/wputedb_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = tf_saver.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch:0' shape=(64, 225, 225, 3) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3cd4061565de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \"\"\"\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3688\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3690\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Validate and process feed_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    517\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \"\"\"\n\u001b[0;32m--> 519\u001b[0;31m     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\u001b[0m\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"Use `if t is not None:` instead of `if t:` to test if a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                     \u001b[0;34m\"tensor is defined, and use the logical TensorFlow ops \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use the logical TensorFlow ops to test the value of a tensor."
     ]
    }
   ],
   "source": [
    "output_layer.eval(feed_dict=[],session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': <tf.Tensor 'resnet_v1_50/predictions/Reshape_1:0' shape=(64, 1, 1, 500) dtype=float32>,\n",
       " 'resnet_v1_50/block1': <tf.Tensor 'resnet_v1_50/block1/unit_3/bottleneck_v1/Relu:0' shape=(64, 29, 29, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_1/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/Relu:0' shape=(64, 57, 57, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/Relu:0' shape=(64, 57, 57, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv2/Relu:0' shape=(64, 57, 57, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 57, 57, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut': <tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/shortcut/BatchNorm/batchnorm/add_1:0' shape=(64, 57, 57, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_2/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block1/unit_2/bottleneck_v1/Relu:0' shape=(64, 57, 57, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv1/Relu:0' shape=(64, 57, 57, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv2/Relu:0' shape=(64, 57, 57, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block1/unit_2/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 57, 57, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv1/Relu:0' shape=(64, 57, 57, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv2/Relu:0' shape=(64, 29, 29, 64) dtype=float32>,\n",
       " 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block1/unit_3/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 29, 29, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block2': <tf.Tensor 'resnet_v1_50/block2/unit_4/bottleneck_v1/Relu:0' shape=(64, 15, 15, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_1/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block2/unit_1/bottleneck_v1/Relu:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv1/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv2/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block2/unit_1/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut': <tf.Tensor 'resnet_v1_50/block2/unit_1/bottleneck_v1/shortcut/BatchNorm/batchnorm/add_1:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_2/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block2/unit_2/bottleneck_v1/Relu:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv1/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv2/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block2/unit_2/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_3/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block2/unit_3/bottleneck_v1/Relu:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv1/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv2/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block2/unit_3/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 29, 29, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv1/Relu:0' shape=(64, 29, 29, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 128) dtype=float32>,\n",
       " 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block2/unit_4/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block3': <tf.Tensor 'resnet_v1_50/block3/unit_6/bottleneck_v1/Relu:0' shape=(64, 8, 8, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_1/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block3/unit_1/bottleneck_v1/Relu:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_1/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut': <tf.Tensor 'resnet_v1_50/block3/unit_1/bottleneck_v1/shortcut/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_2/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block3/unit_2/bottleneck_v1/Relu:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_2/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_3/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block3/unit_3/bottleneck_v1/Relu:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_3/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_4/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block3/unit_4/bottleneck_v1/Relu:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_4/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_5/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block3/unit_5/bottleneck_v1/Relu:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv2/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_5/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 15, 15, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv1/Relu:0' shape=(64, 15, 15, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv2/Relu:0' shape=(64, 8, 8, 256) dtype=float32>,\n",
       " 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block3/unit_6/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 8, 8, 1024) dtype=float32>,\n",
       " 'resnet_v1_50/block4': <tf.Tensor 'resnet_v1_50/block4/unit_3/bottleneck_v1/Relu:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_1/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block4/unit_1/bottleneck_v1/Relu:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv1/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv2/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block4/unit_1/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut': <tf.Tensor 'resnet_v1_50/block4/unit_1/bottleneck_v1/shortcut/BatchNorm/batchnorm/add_1:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_2/bottleneck_v1': <tf.Tensor 'resnet_v1_50/block4/unit_2/bottleneck_v1/Relu:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv1/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv2/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block4/unit_2/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1': <tf.Tensor 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv1/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2': <tf.Tensor 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv2/Relu:0' shape=(64, 8, 8, 512) dtype=float32>,\n",
       " 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3': <tf.Tensor 'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3/BatchNorm/batchnorm/add_1:0' shape=(64, 8, 8, 2048) dtype=float32>,\n",
       " 'resnet_v1_50/conv1': <tf.Tensor 'resnet_v1_50/conv1/Relu:0' shape=(64, 113, 113, 64) dtype=float32>,\n",
       " 'resnet_v1_50/logits': <tf.Tensor 'resnet_v1_50/logits/BiasAdd:0' shape=(64, 1, 1, 500) dtype=float32>}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ToInt32:0' shape=(64,) dtype=int32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'resnet_v1_50/conv1/Relu:0' shape=(64, 113, 113, 64) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (gitdev)",
   "language": "python",
   "name": "gitdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
