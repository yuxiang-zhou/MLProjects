{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ear Record Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n",
      "/vol/atlas/homes/yz4009/gitdev/menpofit/menpofit/math/fft_utils.py:20: RuntimeWarning: pyfftw is known to be buggy on your system, numpy.fft will be used instead. Consequently, all algorithms using ffts will be running at a slower speed.\n",
      "  RuntimeWarning)\n",
      "/vol/atlas/homes/yz4009/miniconda/envs/gitdev/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('/homes/yz4009/wd/gitdev/TFNet/')\n",
    "\n",
    "import numpy as np\n",
    "import menpo.io as mio\n",
    "import scipy.io as sio\n",
    "from io import BytesIO\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import scipy\n",
    "import utils\n",
    "import os\n",
    "from menpo.image import Image\n",
    "from menpo.visualize import print_dynamic, print_progress\n",
    "\n",
    "from dAAMs.lineerror import interpolate\n",
    "from dAAMs.tools import loadmatToDict, multi_channel_svs\n",
    "from scipy.spatial.distance import pdist\n",
    "from pathlib import Path\n",
    "\n",
    "from menpo.shape import PointCloud, PointUndirectedGraph\n",
    "from menpo.transform import Translation\n",
    "from menpofit.transform import DifferentiableAlignmentSimilarity\n",
    "\n",
    "from menpowidgets import visualize_images, visualize_pointclouds\n",
    "from IPython.html.widgets import interact\n",
    "from IPython.html.widgets import Button\n",
    "from IPython.display import display, clear_output\n",
    "from dAAMs.svs import SVS, MultiSVS\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import data_provider\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def get_jpg_string(im):\n",
    "    # Gets the serialized jpg from a menpo `Image`.\n",
    "    fp = BytesIO()\n",
    "    mio.export_image(im, fp, extension='jpg')\n",
    "    fp.seek(0)\n",
    "    return fp.read()\n",
    "\n",
    "def _int_feauture(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feauture(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feauture(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ear Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UERC_train.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/gitdev/menpo/menpo/image/boolean.py:553: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  boolean_image = BooleanImage(warped.pixels.reshape(template_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% (166/166) - done.                                   \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/tfrecords')\n",
    "load_path = Path('/vol/atlas/databases/ear/UERC/UERC 2017 Dataset/Train Dataset')\n",
    "record_name = '%s.tfrecords'%'UERC_train'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    id_no = 1\n",
    "    for identity_path in print_progress(list(load_path.glob('*'))):\n",
    "        if identity_path.is_dir():\n",
    "            images = mio.import_images(identity_path)\n",
    "            for img in images:\n",
    "                cimgs = utils.crop_image(img, img.centre(), img.diagonal()/350, [256,256], base=384)[0]\n",
    "                img_height = 256\n",
    "                img_width = 256\n",
    "                id_no = int(identity_path.stem)\n",
    "\n",
    "                yield cimgs, img_height, img_width, id_no\n",
    "                \n",
    "            id_no += 1\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    writer = tf.python_io.TFRecordWriter(str(store_path/record_name))\n",
    "    \n",
    "    for img_all, img_height, img_width,  id_no in iterator:\n",
    "\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "              # Features contains a map of string to Feature proto objects\n",
    "                feature={\n",
    "                    # images\n",
    "                    'image': _bytes_feauture(get_jpg_string(img_all)),\n",
    "                    'height': _int_feauture(img_height),\n",
    "                    'width': _int_feauture(img_width),\n",
    "                    'id_no': _int_feauture(id_no)\n",
    "                }))\n",
    "\n",
    "        # use the proto object to serialize the example to a string\n",
    "        serialized = example.SerializeToString()\n",
    "        # write the serialized object to disk\n",
    "        writer.write(serialized)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UERC_train.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/gitdev/menpo/menpo/image/boolean.py:553: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  boolean_image = BooleanImage(warped.pixels.reshape(template_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% (166/166) - done.                                   \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/UERC_160')\n",
    "load_path = Path('/vol/atlas/databases/ear/UERC/UERC 2017 Dataset/Train Dataset')\n",
    "record_name = '%s.tfrecords'%'UERC_train'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    id_no = 1\n",
    "    for identity_path in print_progress(list(load_path.glob('*'))):\n",
    "        if identity_path.is_dir():\n",
    "            images = mio.import_images(identity_path)\n",
    "            for img_id,img in enumerate(images):\n",
    "                cimgs = utils.crop_image(img, img.centre(), img.diagonal()/350, [160,160], base=384)[0]\n",
    "                img_height = 160\n",
    "                img_width = 160\n",
    "                id_no = int(identity_path.stem)\n",
    "\n",
    "                yield cimgs, img_height, img_width, id_no, img_id\n",
    "                \n",
    "            id_no += 1\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    \n",
    "    for img_all, img_height, img_width,  id_no, img_id in iterator:\n",
    "        d_path = str(store_path/str(id_no))\n",
    "        if not os.path.exists(d_path):\n",
    "            os.mkdir(d_path)\n",
    "        mio.export_image(img_all, store_path/str(id_no)/('%04d.png'%img_id))\n",
    "        \n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UERC_test.tfrecords\n",
      "[                    ] 1% (58/3542) - 00:03:19 remaining                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/gitdev/menpo/menpo/image/boolean.py:553: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  boolean_image = BooleanImage(warped.pixels.reshape(template_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% (3542/3542) - done.                                 \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/UERC_160_generate')\n",
    "load_path = Path('/vol/atlas/databases/ear/UERC/UERC 2017 Dataset/Test Dataset')\n",
    "record_name = '%s.tfrecords'%'UERC_test'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    for identity_path in print_progress(list(load_path.glob('*'))):\n",
    "        if identity_path.is_dir():\n",
    "            images = mio.import_images(identity_path)\n",
    "            for img_id,img in enumerate(images):\n",
    "                cimgs = utils.crop_image(img, img.centre(), img.diagonal()/350, [160,160], base=384)[0]\n",
    "                img_height = 160\n",
    "                img_width = 160\n",
    "                id_no = identity_path.stem\n",
    "                image_name = img.path.name\n",
    "\n",
    "                yield cimgs, img_height, img_width, id_no, image_name\n",
    "            \n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    \n",
    "    for img_all, img_height, img_width,  id_no, image_name in iterator:\n",
    "        d_path = str(store_path/str(id_no))\n",
    "        if not os.path.exists(d_path):\n",
    "            os.mkdir(d_path)\n",
    "        mio.export_image(img_all, store_path/str(id_no)/image_name)\n",
    "        \n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vgg ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    ] 4% (10/232) - 00:00:55 remaining                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/atlas/homes/yz4009/gitdev/menpo/menpo/image/boolean.py:553: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  boolean_image = BooleanImage(warped.pixels.reshape(template_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================] 100% (232/232) - done.                                   \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/VGGEAR_160')\n",
    "load_path = Path('/homes/yz4009/wd/databases/ear/VGGEers-Recognition')\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    id_no = 167\n",
    "    for identity_path in print_progress(list(load_path.glob('*'))):\n",
    "        if identity_path.is_dir():\n",
    "            images = mio.import_images(identity_path)\n",
    "            for img_id,img in enumerate(images):\n",
    "                img = img.crop_to_landmarks_proportion(0.1)\n",
    "                \n",
    "                if img.n_channels == 1:\n",
    "                    img = Image(np.stack([img.pixels.squeeze() for _ in range(3)]))\n",
    "                    \n",
    "                cimgs = utils.crop_image(img, img.centre(), img.diagonal()/350, [160,160], base=384)[0]\n",
    "                img_height = 160\n",
    "                img_width = 160\n",
    "\n",
    "                yield cimgs, img_height, img_width, id_no, img_id\n",
    "                \n",
    "            id_no += 1\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    \n",
    "    for img_all, img_height, img_width,  id_no, img_id in iterator:\n",
    "        d_path = str(store_path/str(id_no))\n",
    "        if not os.path.exists(d_path):\n",
    "            os.mkdir(d_path)\n",
    "        mio.export_image(img_all, store_path/str(id_no)/('%d_%04d.png'%(id_no,img_id)))\n",
    "        \n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Face Record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASIA_182.tfrecords\n",
      "[====================] 100% (10580/10580) - done.                               \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/tfrecords')\n",
    "load_path = Path('/vol/atlas/homes/jiankang/code/facenet/data/CASIA_182_multi/')\n",
    "record_name = '%s.tfrecords'%'CASIA_182'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    image_id = 1\n",
    "    for tpath in print_progress(list(load_path.glob('*'))):\n",
    "        \n",
    "        if tpath.is_dir():\n",
    "        \n",
    "            img_height = 182\n",
    "            img_width = 182\n",
    "\n",
    "            img_all = np.stack([img.pixels_with_channels_at_back() for img in mio.import_images(tpath)])\n",
    "            \n",
    "            if len(img_all) < 16:\n",
    "                img_all = img_all[np.random.choice(len(img_all),16)]\n",
    "\n",
    "            n_img = np.min([len(img_all), 354])\n",
    "\n",
    "            img_all = img_all.reshape(-1,img_height,img_width,3)\n",
    "            img_all = img_all[:n_img].reshape(-1,img_width,3)\n",
    "\n",
    "            yield Image.init_from_channels_at_back(img_all), img_height, img_width, n_img, image_id\n",
    "\n",
    "            image_id += 1\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    writer = tf.python_io.TFRecordWriter(str(store_path/record_name))\n",
    "    \n",
    "    for img_all, img_height, img_width, n_img, id_no in iterator:\n",
    "        try:\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                  # Features contains a map of string to Feature proto objects\n",
    "                    feature={\n",
    "                        # images\n",
    "                        'image': _bytes_feauture(get_jpg_string(img_all)),\n",
    "                        'height': _int_feauture(img_height),\n",
    "                        'width': _int_feauture(img_width),\n",
    "                        'n_image': _int_feauture(n_img),\n",
    "                        'id_no': _int_feauture(id_no)\n",
    "                    }))\n",
    "\n",
    "            # use the proto object to serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "            # write the serialized object to disk\n",
    "            writer.write(serialized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASIA.tfrecords\n",
      "[====================] 100% (10575/10575) - done.                               \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/tfrecords')\n",
    "load_path = Path('/vol/atlas/homes/jiankang/data/recognition/data/CASIA_112/')\n",
    "record_name = '%s.tfrecords'%'CASIA'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    for timg in print_progress(mio.import_images(load_path)):\n",
    "        img_height = 112\n",
    "        img_width = 112\n",
    "        id_no = int(timg.path.stem)\n",
    "        \n",
    "        nh,nw = np.array(timg.shape) // 112\n",
    "        \n",
    "        n_img = np.min([nh*nw, 36*16])\n",
    "        img_all = timg.pixels.reshape(3,nh,112,nw,112).transpose(1,3,2,4,0).reshape(-1,112,112,3)\n",
    "        img_all = img_all[:n_img].reshape(-1,112,3)\n",
    "        \n",
    "        yield Image.init_from_channels_at_back(img_all), img_height, img_width, n_img, id_no\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    writer = tf.python_io.TFRecordWriter(str(store_path/record_name))\n",
    "    \n",
    "    for img_all, img_height, img_width, n_img, id_no in iterator:\n",
    "        try:\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                  # Features contains a map of string to Feature proto objects\n",
    "                    feature={\n",
    "                        # images\n",
    "                        'image': _bytes_feauture(get_jpg_string(img_all)),\n",
    "                        'height': _int_feauture(img_height),\n",
    "                        'width': _int_feauture(img_width),\n",
    "                        'n_image': _int_feauture(n_img),\n",
    "                        'id_no': _int_feauture(id_no)\n",
    "                    }))\n",
    "\n",
    "            # use the proto object to serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "            # write the serialized object to disk\n",
    "            writer.write(serialized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/tfrecords')\n",
    "load_path = Path('/vol/atlas/homes/jiankang/code/facenet/data/lfw_160/')\n",
    "record_name = '%s.tfrecords'%'LFW_160'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "\n",
    "    image_id = 1\n",
    "    for tpath in print_progress(list(load_path.glob('*'))):\n",
    "        \n",
    "        if tpath.is_dir():\n",
    "        \n",
    "            img_height = 182\n",
    "            img_width = 182\n",
    "\n",
    "            img_all = np.stack([img.pixels_with_channels_at_back() for img in mio.import_images(tpath)])\n",
    "\n",
    "            n_img = np.min([len(img_all), 354])\n",
    "\n",
    "            img_all = img_all.reshape(-1,img_height,img_width,3)\n",
    "            img_all = img_all[:n_img].reshape(-1,img_width,3)\n",
    "\n",
    "            yield Image.init_from_channels_at_back(img_all), img_height, img_width, n_img, image_id\n",
    "\n",
    "            image_id += 1\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    writer = tf.python_io.TFRecordWriter(str(store_path/record_name))\n",
    "    \n",
    "    for img_all, img_height, img_width, n_img, id_no in iterator:\n",
    "        try:\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                  # Features contains a map of string to Feature proto objects\n",
    "                    feature={\n",
    "                        # images\n",
    "                        'image': _bytes_feauture(get_jpg_string(img_all)),\n",
    "                        'height': _int_feauture(img_height),\n",
    "                        'width': _int_feauture(img_width),\n",
    "                        'n_image': _int_feauture(n_img),\n",
    "                        'id_no': _int_feauture(id_no)\n",
    "                    }))\n",
    "\n",
    "            # use the proto object to serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "            # write the serialized object to disk\n",
    "            writer.write(serialized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFW.tfrecords\n",
      "[====================] 100% (10/10) - done.                                     \n"
     ]
    }
   ],
   "source": [
    "store_path = Path('/homes/yz4009/wd/databases/tfrecords')\n",
    "load_path = Path('/vol/atlas/homes/jiankang/data/recognition/data/lfw_112/')\n",
    "record_name = '%s.tfrecords'%'LFW'\n",
    "\n",
    "print(record_name)\n",
    "\n",
    "def data_iterator():\n",
    "    database_path = load_path\n",
    "    \n",
    "    img_height = 112\n",
    "    img_width = 112\n",
    "    n_img=2\n",
    "    \n",
    "    with open('/homes/yz4009/Desktop/lfw_pairs.txt') as f:\n",
    "        pairs = f.readlines()\n",
    "        n_fold, n_pairs = map(int, pairs[0].strip().split('\\t'))\n",
    "        pairs = pairs[1:]\n",
    "        for fold in print_progress(range(n_fold)):\n",
    "            for p in range(n_pairs):\n",
    "                name,id1,id2=pairs[fold*n_pairs*2+p].strip().split('\\t')\n",
    "                img1 = mio.import_image(database_path/name/('%s_%04d.jpg'%(name, int(id1))))\n",
    "                img2 = mio.import_image(database_path/name/('%s_%04d.jpg'%(name, int(id2))))\n",
    "                img_all = Image(np.concatenate([img1.pixels, img2.pixels], axis=1))\n",
    "                \n",
    "                yield img_all, img_height, img_width, n_img, 1\n",
    "\n",
    "            for p in range(n_pairs, n_pairs*2):\n",
    "                name1,id1, name2,id2=pairs[fold*n_pairs*2+p].strip().split('\\t')\n",
    "                img1 = mio.import_image(database_path/name1/('%s_%04d.jpg'%(name1, int(id1))))\n",
    "                img2 = mio.import_image(database_path/name2/('%s_%04d.jpg'%(name2, int(id2))))\n",
    "                img_all = Image(np.concatenate([img1.pixels, img2.pixels], axis=1))\n",
    "                \n",
    "                yield img_all, img_height, img_width, n_img, 0\n",
    "    \n",
    "\n",
    "def generate(iterator,\n",
    "             store_path=store_path,\n",
    "             record_name=record_name,\n",
    "             base=384):\n",
    "\n",
    "    store_path = Path(store_path)\n",
    "    writer = tf.python_io.TFRecordWriter(str(store_path/record_name))\n",
    "    \n",
    "    for img_all, img_height, img_width, n_img, id_no in iterator:\n",
    "        try:\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                  # Features contains a map of string to Feature proto objects\n",
    "                    feature={\n",
    "                        # images\n",
    "                        'image': _bytes_feauture(get_jpg_string(img_all)),\n",
    "                        'height': _int_feauture(img_height),\n",
    "                        'width': _int_feauture(img_width),\n",
    "                        'n_image': _int_feauture(n_img),\n",
    "                        'id_no': _int_feauture(id_no)\n",
    "                    }))\n",
    "\n",
    "            # use the proto object to serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "            # write the serialized object to disk\n",
    "            writer.write(serialized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "generate(data_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gitdev]",
   "language": "python",
   "name": "conda-env-gitdev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
