{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import menpo.io as mio\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, name, root, batch_size=1):\n",
    "        self.name = name\n",
    "        self.root = Path(root)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_keys(self, path='images'):\n",
    "        path = self.root / path\n",
    "        keys = [x.stem for x in path.glob('*')]\n",
    "        print('Found {} files.'.format(len(keys)))\n",
    "\n",
    "        if len(keys) == 0:\n",
    "            raise RuntimeError('No images found in {}'.format(path))\n",
    "        return tf.constant(keys, tf.string)\n",
    "\n",
    "class EarWPUTEDB(Dataset):\n",
    "    def __init__(self, batch_size=1, db_name='WPUTEDB-train'):\n",
    "        self.name = 'EarWPUTEDB'\n",
    "        self.batch_size = batch_size\n",
    "        self.root = Path('/homes/yz4009/wd/PickleModel/EarRecognition/')\n",
    "        self.dataset = mio.import_pickle(str(self.root / '{}.pkl'.format(db_name)))\n",
    "        self.num_classes = 500\n",
    "        self.shape = (250, 190)\n",
    "\n",
    "    def get_keys(self, path='images'):\n",
    "        path = self.root / path\n",
    "        keys = map(str, np.arange(len(self.dataset)))\n",
    "        print('Found {} files.'.format(len(keys)))\n",
    "\n",
    "        if len(keys) == 0:\n",
    "            raise RuntimeError('No images found in {}'.format(path))\n",
    "        return tf.constant(keys, tf.string)\n",
    "\n",
    "    def get_images(self, key, shape=None):\n",
    "        def wrapper(index):\n",
    "            return self.dataset[int(index)][1].resize(self.shape).pixels.reshape(self.shape + (1,)).astype(np.float32)\n",
    "\n",
    "        image = tf.py_func(wrapper, [key],\n",
    "                                   [tf.float32])[0]\n",
    "        \n",
    "        image.set_shape(self.shape + (1,))\n",
    "        return image\n",
    "\n",
    "    def get_labels(self, key, shape=None):\n",
    "        def wrapper(index):\n",
    "            return self.dataset[int(index)][0].astype(np.int32)\n",
    "\n",
    "        label = tf.py_func(wrapper, [key],\n",
    "                                   [tf.int32])[0]\n",
    "\n",
    "        label = tf.one_hot(label, self.num_classes, dtype=tf.int32)\n",
    "        label.set_shape([500,])\n",
    "        return label, None\n",
    "\n",
    "    def get(self, *names):\n",
    "        producer = tf.train.string_input_producer(self.get_keys(),\n",
    "                                                  shuffle=True)\n",
    "        key = producer.dequeue()\n",
    "        images = self.get_images(key)\n",
    "\n",
    "        image_shape = tf.shape(images)\n",
    "        tensors = [images]\n",
    "\n",
    "        for name in names:\n",
    "            fun = getattr(self, 'get_' + name.split('/')[0])\n",
    "            use_mask = (\n",
    "                len(name.split('/')) > 1) and name.split('/')[1] == 'mask'\n",
    "\n",
    "            label, mask = fun(key, shape=image_shape)\n",
    "            tensors.append(label)\n",
    "\n",
    "        return tf.train.shuffle_batch(tensors,\n",
    "                              self.batch_size,\n",
    "                              capacity=2000, min_after_dequeue=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.slim import nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_float('initial_learning_rate', 0.001,\n",
    "                          '''Initial learning rate.''')\n",
    "tf.app.flags.DEFINE_float('num_epochs_per_decay', 5.0,\n",
    "                          '''Epochs after which learning rate decays.''')\n",
    "tf.app.flags.DEFINE_float('learning_rate_decay_factor', 0.97,\n",
    "                          '''Learning rate decay factor.''')\n",
    "tf.app.flags.DEFINE_integer('batch_size', batch_size, '''The batch size to use.''')\n",
    "tf.app.flags.DEFINE_integer('num_preprocess_threads', 4,\n",
    "                            '''How many preprocess threads to use.''')\n",
    "tf.app.flags.DEFINE_string('train_dir', 'ckpt/ear_train',\n",
    "                           '''Directory where to write event logs '''\n",
    "                           '''and checkpoint.''')\n",
    "# tf.app.flags.DEFINE_string('pretrained_model_checkpoint_path', '/vol/atlas/homes/gt108/Projects/ibugface/pretrained_models/resnet_v1_50.ckpt',\n",
    "#                            '''If specified, restore this pretrained model '''\n",
    "#                            '''before beginning any training.''')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 100000,\n",
    "                            '''Number of batches to run.''')\n",
    "tf.app.flags.DEFINE_string('train_device', '/gpu:3',\n",
    "                           '''Device to train with.''')\n",
    "tf.app.flags.DEFINE_string('dataset_path', '', 'Dataset directory')\n",
    "# The decay to use for the moving average.\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network(inputs, scale=1, output_classes=500):\n",
    "    net, _ = nets.resnet_v1.resnet_v1_50(inputs)\n",
    "    net = slim.layers.fully_connected(slim.flatten(net), output_classes, activation_fn=None, scope='logits')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 859 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_provider = EarWPUTEDB(batch_size=batch_size, db_name='WPUTEDB-test')\n",
    "\n",
    "images, labels = test_provider.get('labels')\n",
    "\n",
    "predictions = network(images)\n",
    "\n",
    "predictions = tf.to_int32(tf.argmax(predictions, 1))\n",
    "labels = tf.to_int32(tf.argmax(labels, 1))\n",
    "\n",
    "tf.scalar_summary('accuracy', slim.metrics.accuracy(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 859 // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are streaming metrics which compute the \"running\" metric,\n",
    "# e.g running accuracy\n",
    "metrics_to_values, metrics_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    \"streaming_accuracy\": slim.metrics.streaming_accuracy(predictions, labels),\n",
    "})\n",
    "\n",
    "# Define the streaming summaries to write:\n",
    "for metric_name, metric_value in metrics_to_values.iteritems():\n",
    "    tf.scalar_summary(metric_name, metric_value)\n",
    "\n",
    "# Evaluate every 30 seconds\n",
    "slim.evaluation.evaluation_loop(\n",
    "    '',\n",
    "    'ckpt/ear_train',\n",
    "    'ckpt/ear_eval',\n",
    "    num_evals=batch_size,\n",
    "    eval_op=metrics_to_updates.values(),\n",
    "    summary_op=tf.merge_all_summaries(),\n",
    "    eval_interval_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
